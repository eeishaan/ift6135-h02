
########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2839.2592430114746	speed (wps):4976.940401240569
step: 51	loss: 12456.148660182953	speed (wps):5159.507804408196
step: 92	loss: 21742.001128196716	speed (wps):5183.993816780398
step: 133	loss: 30621.293823719025	speed (wps):5193.317264560654
step: 174	loss: 39134.01853322983	speed (wps):5198.839315867525
step: 215	loss: 47629.13449525833	speed (wps):5201.464958575069
step: 256	loss: 56019.88470077515	speed (wps):5203.496649378175
step: 297	loss: 64266.37461185455	speed (wps):5205.136163987863
step: 338	loss: 72411.162378788	speed (wps):5206.062640473867
step: 379	loss: 80535.02017736435	speed (wps):5206.8186138701185
epoch: 0	train ppl: 410.46836714175	val ppl: 442.145025041507	best val: 442.145025041507	time (s) spent in epoch: 181.7755057811737

EPOCH 1 ------------------
step: 10	loss: 2169.9723625183105	speed (wps):5128.843373125659
step: 51	loss: 10094.012537002563	speed (wps):5193.446950120355
step: 92	loss: 17964.7349858284	speed (wps):5203.007726104705
step: 133	loss: 25758.71075153351	speed (wps):5206.689552634451
step: 174	loss: 33360.99336147308	speed (wps):5208.098950078298
step: 215	loss: 41080.74312925339	speed (wps):5208.714617748475
step: 256	loss: 48815.91544628143	speed (wps):5209.2790619186635
step: 297	loss: 56480.140645504	speed (wps):5209.898351055526
step: 338	loss: 64107.59464263916	speed (wps):5209.967773343803
step: 379	loss: 71768.96542072296	speed (wps):5210.136758736902
epoch: 1	train ppl: 218.60583715316992	val ppl: 339.8549207225993	best val: 339.8549207225993	time (s) spent in epoch: 181.62781810760498

EPOCH 2 ------------------
step: 10	loss: 2068.9103198051453	speed (wps):5134.039763458209
step: 51	loss: 9601.81654214859	speed (wps):5193.862642621714
step: 92	loss: 17128.7277507782	speed (wps):5201.131104602125
step: 133	loss: 24606.34170293808	speed (wps):5204.818507503229
step: 174	loss: 31893.832411766052	speed (wps):5205.573039099174
step: 215	loss: 39327.15709924698	speed (wps):5207.173746454953
step: 256	loss: 46799.14754152298	speed (wps):5207.836092971474
step: 297	loss: 54211.225798130035	speed (wps):5208.405368861672
step: 338	loss: 61589.625997543335	speed (wps):5208.952994550217
step: 379	loss: 69014.71388101578	speed (wps):5209.432978466199
epoch: 2	train ppl: 178.42562201154706	val ppl: 288.25605787584016	best val: 288.25605787584016	time (s) spent in epoch: 181.65545010566711

EPOCH 3 ------------------
step: 10	loss: 2012.9084372520447	speed (wps):5123.27192521469
step: 51	loss: 9334.166040420532	speed (wps):5193.741279372964
step: 92	loss: 16670.356245040894	speed (wps):5202.5676243059115
step: 133	loss: 23963.711891174316	speed (wps):5205.307051117547
step: 174	loss: 31062.845635414124	speed (wps):5207.447124946726
step: 215	loss: 38314.93399858475	speed (wps):5208.662880169323
step: 256	loss: 45605.21909713745	speed (wps):5209.033778663235
step: 297	loss: 52851.21431350708	speed (wps):5209.654065799601
step: 338	loss: 60072.143869400024	speed (wps):5209.8902887392815
step: 379	loss: 67339.77130651474	speed (wps):5209.969816659821
epoch: 3	train ppl: 157.54029030815698	val ppl: 260.5007845117435	best val: 260.5007845117435	time (s) spent in epoch: 181.65235018730164

EPOCH 4 ------------------
step: 10	loss: 1966.4340257644653	speed (wps):5124.948975472879
step: 51	loss: 9141.075789928436	speed (wps):5194.579397759394
step: 92	loss: 16332.0645236969	speed (wps):5201.097203328121
step: 133	loss: 23488.341965675354	speed (wps):5203.568019627438
step: 174	loss: 30452.225034236908	speed (wps):5205.6485080056045
step: 215	loss: 37567.401185035706	speed (wps):5206.9241194867145
step: 256	loss: 44734.24942970276	speed (wps):5207.534306405046
step: 297	loss: 51850.071806907654	speed (wps):5208.742611632072
step: 338	loss: 58941.29323720932	speed (wps):5209.696357229806
step: 379	loss: 66093.84791374207	speed (wps):5210.398586699322
epoch: 4	train ppl: 143.54046008834126	val ppl: 242.09472010052812	best val: 242.09472010052812	time (s) spent in epoch: 181.619775056839

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2837.1338605880737	speed (wps):3625.181414375548
step: 51	loss: 12461.361963748932	speed (wps):3739.5990899567596
step: 92	loss: 21790.99595785141	speed (wps):3754.365868447199
step: 133	loss: 30700.927855968475	speed (wps):3759.4404793601593
step: 174	loss: 39262.56878852844	speed (wps):3762.777942008777
step: 215	loss: 47818.407735824585	speed (wps):3765.6431722422103
step: 256	loss: 56267.27702856064	speed (wps):3768.858837015761
step: 297	loss: 64578.254680633545	speed (wps):3771.9187701950627
step: 338	loss: 72788.27116012573	speed (wps):3775.704693582359
step: 379	loss: 80979.14858341217	speed (wps):3779.1578374874834
epoch: 0	train ppl: 424.69568936464884	val ppl: 409.41099569530655	best val: 409.41099569530655	time (s) spent in epoch: 250.18134760856628

EPOCH 1 ------------------
step: 10	loss: 2183.966474533081	speed (wps):3761.753218501462
step: 51	loss: 10161.461544036865	speed (wps):3798.181840020694
step: 92	loss: 18091.066615581512	speed (wps):3802.0290835287806
step: 133	loss: 25944.517557621002	speed (wps):3803.841150581904
step: 174	loss: 33616.55922412872	speed (wps):3805.223533863538
step: 215	loss: 41394.488043785095	speed (wps):3806.0120100931485
step: 256	loss: 49186.15479469299	speed (wps):3806.4567205116655
step: 297	loss: 56912.488925457	speed (wps):3806.5153771134655
step: 338	loss: 64593.772258758545	speed (wps):3806.7135964981817
step: 379	loss: 72310.62521457672	speed (wps):3807.062253658367
epoch: 1	train ppl: 227.63747908531053	val ppl: 294.6606044165734	best val: 294.6606044165734	time (s) spent in epoch: 248.46840524673462

EPOCH 2 ------------------
step: 10	loss: 2085.7966542243958	speed (wps):3763.3589282758853
step: 51	loss: 9667.583050727844	speed (wps):3797.9611869422765
step: 92	loss: 17252.458503246307	speed (wps):3802.43926958692
step: 133	loss: 24793.37019920349	speed (wps):3804.3664975263355
step: 174	loss: 32147.294187545776	speed (wps):3805.23559911341
step: 215	loss: 39645.468373298645	speed (wps):3805.97274417119
step: 256	loss: 47178.74000787735	speed (wps):3806.5066408552802
step: 297	loss: 54670.55952548981	speed (wps):3806.887850128201
step: 338	loss: 62119.43615436554	speed (wps):3806.9830004182904
step: 379	loss: 69615.2143239975	speed (wps):3807.49037955863
epoch: 2	train ppl: 186.75809655610036	val ppl: 254.7995877218822	best val: 254.7995877218822	time (s) spent in epoch: 248.43797516822815

EPOCH 3 ------------------
step: 10	loss: 2029.3945455551147	speed (wps):3764.1117082633837
step: 51	loss: 9424.474048614502	speed (wps):3801.5169598056423
step: 92	loss: 16825.026590824127	speed (wps):3805.414528618905
step: 133	loss: 24185.88124513626	speed (wps):3806.472858926705
step: 174	loss: 31356.90922498703	speed (wps):3807.151033398984
step: 215	loss: 38686.75873041153	speed (wps):3807.4817205057766
step: 256	loss: 46057.18855381012	speed (wps):3807.5244936065847
step: 297	loss: 53395.33821582794	speed (wps):3807.688304238367
step: 338	loss: 60695.97430229187	speed (wps):3807.6733649986354
step: 379	loss: 68050.95465660095	speed (wps):3807.9703294094425
epoch: 3	train ppl: 166.25294110797452	val ppl: 242.37218692720063	best val: 242.37218692720063	time (s) spent in epoch: 248.41591262817383

EPOCH 4 ------------------
step: 10	loss: 1990.6644535064697	speed (wps):3766.208248755711
step: 51	loss: 9261.75971031189	speed (wps):3799.6176410632147
step: 92	loss: 16541.17209672928	speed (wps):3803.7956263865126
step: 133	loss: 23780.242204666138	speed (wps):3805.7590399483024
step: 174	loss: 30833.49020242691	speed (wps):3806.5678970911117
step: 215	loss: 38038.43836784363	speed (wps):3807.084352107618
step: 256	loss: 45288.74988794327	speed (wps):3807.519053944241
step: 297	loss: 52515.24754524231	speed (wps):3807.776153249344
step: 338	loss: 59720.59169769287	speed (wps):3807.706782254702
step: 379	loss: 66967.01778173447	speed (wps):3807.8023212179737
epoch: 4	train ppl: 153.41362569881582	val ppl: 231.83287800962603	best val: 231.83287800962603	time (s) spent in epoch: 248.41733503341675

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2919.4890832901	speed (wps):7233.508568332925
step: 30	loss: 7586.995761394501	speed (wps):7489.481989517154
step: 50	loss: 12226.631207466125	speed (wps):7546.817100696681
step: 70	loss: 16848.84430885315	speed (wps):7572.6940676514305
step: 90	loss: 21398.407850265503	speed (wps):7587.177096253618
step: 110	loss: 25828.07315349579	speed (wps):7596.9270501579485
step: 130	loss: 30165.622630119324	speed (wps):7603.588342760499
step: 150	loss: 34425.29188632965	speed (wps):7608.23793594865
step: 170	loss: 38611.93450450897	speed (wps):7612.0766939848245
step: 190	loss: 42779.74827051163	speed (wps):7614.749506056162
epoch: 0	train ppl: 578.5773262418214	val ppl: 443.2377670957711	best val: 443.2377670957711	time (s) spent in epoch: 124.42735004425049

EPOCH 1 ------------------
step: 10	loss: 2271.842918395996	speed (wps):7537.903025134646
step: 30	loss: 6348.000247478485	speed (wps):7602.943034743863
step: 50	loss: 10375.332744121552	speed (wps):7616.346227490276
step: 70	loss: 14374.51150894165	speed (wps):7622.230751571447
step: 90	loss: 18347.75314807892	speed (wps):7625.223543502107
step: 110	loss: 22288.393573760986	speed (wps):7626.874186061982
step: 130	loss: 26202.658562660217	speed (wps):7628.22717796339
step: 150	loss: 30087.100706100464	speed (wps):7629.522647303885
step: 170	loss: 33933.88249158859	speed (wps):7630.392162162567
step: 190	loss: 37785.36727190018	speed (wps):7631.036527664632
epoch: 1	train ppl: 281.63498579061155	val ppl: 301.2403411610259	best val: 301.2403411610259	time (s) spent in epoch: 124.01774382591248

EPOCH 2 ------------------
step: 10	loss: 2131.353054046631	speed (wps):7535.042155454552
step: 30	loss: 5969.767825603485	speed (wps):7600.659634061027
step: 50	loss: 9776.058897972107	speed (wps):7614.891194371674
step: 70	loss: 13569.533972740173	speed (wps):7620.249268045819
step: 90	loss: 17351.627762317657	speed (wps):7624.061814967153
step: 110	loss: 21114.59018945694	speed (wps):7626.460392174787
step: 130	loss: 24864.35915708542	speed (wps):7627.8918429621435
step: 150	loss: 28595.831077098846	speed (wps):7628.601699343737
step: 170	loss: 32296.532566547394	speed (wps):7629.274752853373
step: 190	loss: 36005.981702804565	speed (wps):7630.05665474027
epoch: 2	train ppl: 217.00808188457924	val ppl: 264.02428892270865	best val: 264.02428892270865	time (s) spent in epoch: 124.02669787406921

EPOCH 3 ------------------
step: 10	loss: 2062.9145216941833	speed (wps):7541.154262833771
step: 30	loss: 5779.141516685486	speed (wps):7603.643299761586
step: 50	loss: 9467.364616394043	speed (wps):7615.900193060093
step: 70	loss: 13156.092467308044	speed (wps):7622.275736890171
step: 90	loss: 16834.478175640106	speed (wps):7625.759241017266
step: 110	loss: 20497.638201713562	speed (wps):7628.063668211012
step: 130	loss: 24152.669854164124	speed (wps):7629.751283659735
step: 150	loss: 27792.1160697937	speed (wps):7631.551441095817
step: 170	loss: 31400.885949134827	speed (wps):7632.870921908547
step: 190	loss: 35023.02437543869	speed (wps):7633.793896922048
epoch: 3	train ppl: 187.76230593918626	val ppl: 242.2344244644091	best val: 242.2344244644091	time (s) spent in epoch: 123.96128153800964

EPOCH 4 ------------------
step: 10	loss: 2015.330855846405	speed (wps):7553.663609038334
step: 30	loss: 5654.9080991744995	speed (wps):7612.416661341287
step: 50	loss: 9263.181772232056	speed (wps):7624.601687017113
step: 70	loss: 12875.521743297577	speed (wps):7631.126737003622
step: 90	loss: 16483.423628807068	speed (wps):7636.693821418466
step: 110	loss: 20076.571254730225	speed (wps):7643.030833975241
step: 130	loss: 23666.725554466248	speed (wps):7650.269166245572
step: 150	loss: 27233.592104911804	speed (wps):7656.379473905006
step: 170	loss: 30771.80547952652	speed (wps):7661.1209985266405
step: 190	loss: 34322.42683172226	speed (wps):7664.509812957946
epoch: 4	train ppl: 169.20925249333436	val ppl: 226.5669146951146	best val: 226.5669146951146	time (s) spent in epoch: 123.44352436065674

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2809.2396306991577	speed (wps):4225.839239573711
step: 30	loss: 7501.9419622421265	speed (wps):4274.7483117052225
step: 50	loss: 12161.527831554413	speed (wps):4285.67598887115
step: 70	loss: 16738.682675361633	speed (wps):4290.261523080026
step: 90	loss: 21160.10326385498	speed (wps):4293.357318009583
step: 110	loss: 25454.521963596344	speed (wps):4295.379864748337
step: 130	loss: 29666.190922260284	speed (wps):4296.976800395945
step: 150	loss: 33805.82320213318	speed (wps):4298.757392858668
step: 170	loss: 37866.872391700745	speed (wps):4301.0420892841785
step: 190	loss: 41915.666761398315	speed (wps):4303.40476058925

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=256_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=256_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2854.1316962242126	speed (wps):7357.644885003483
step: 51	loss: 12472.021553516388	speed (wps):7743.13365143704
step: 92	loss: 21823.548789024353	speed (wps):7789.250870391816
step: 133	loss: 30861.626427173615	speed (wps):7808.311006094417
step: 174	loss: 39579.98092651367	speed (wps):7818.336984547106
step: 215	loss: 48304.50639486313	speed (wps):7823.758025341707
step: 256	loss: 56918.945014476776	speed (wps):7830.002676903287
step: 297	loss: 65410.05491018295	speed (wps):7834.074067076747
step: 338	loss: 73806.93703174591	speed (wps):7835.934241529371
step: 379	loss: 82187.3082613945	speed (wps):7837.911013120185
epoch: 0	train ppl: 466.83206384706557	val ppl: 436.1192036651116	best val: 436.1192036651116	time (s) spent in epoch: 120.58447647094727

EPOCH 1 ------------------
step: 10	loss: 2236.5905594825745	speed (wps):7620.838800957821
step: 51	loss: 10434.902715682983	speed (wps):7802.9791417249735
step: 92	loss: 18580.918715000153	speed (wps):7832.673964006436
step: 133	loss: 26662.467551231384	speed (wps):7849.102932821306
step: 174	loss: 34573.80256175995	speed (wps):7866.291125497699
step: 215	loss: 42604.93290424347	speed (wps):7881.831638308383
step: 256	loss: 50636.49388074875	speed (wps):7892.715439840807
step: 297	loss: 58614.4849729538	speed (wps):7897.193983563858
step: 338	loss: 66539.53191280365	speed (wps):7898.486622160728
step: 379	loss: 74499.03579950333	speed (wps):7902.436549041765
epoch: 1	train ppl: 268.6197612462828	val ppl: 315.7967764617189	best val: 315.7967764617189	time (s) spent in epoch: 119.55344486236572

EPOCH 2 ------------------
step: 10	loss: 2152.287495136261	speed (wps):7722.896803014202
step: 51	loss: 10014.398214817047	speed (wps):7893.855968792229
step: 92	loss: 17862.783987522125	speed (wps):7911.7684643286675
step: 133	loss: 25661.776740550995	speed (wps):7918.646064437041
step: 174	loss: 33307.12256908417	speed (wps):7923.014021364481
step: 215	loss: 41088.88044834137	speed (wps):7926.388418036375
step: 256	loss: 48894.88601207733	speed (wps):7928.2794366618955
step: 297	loss: 56656.33942127228	speed (wps):7930.212425452424
step: 338	loss: 64378.75734090805	speed (wps):7931.23270625143
step: 379	loss: 72143.20522785187	speed (wps):7931.644632688583
epoch: 2	train ppl: 225.92016131103628	val ppl: 280.5932431576389	best val: 280.5932431576389	time (s) spent in epoch: 119.13929224014282

EPOCH 3 ------------------
step: 10	loss: 2106.999158859253	speed (wps):7730.1029888541125
step: 51	loss: 9797.53564119339	speed (wps):7887.621597095368
step: 92	loss: 17488.44945192337	speed (wps):7913.072457644434
step: 133	loss: 25137.382912635803	speed (wps):7925.552773596908
step: 174	loss: 32621.64933681488	speed (wps):7932.1356112933945
step: 215	loss: 40259.30990934372	speed (wps):7935.624964471713
step: 256	loss: 47930.90410232544	speed (wps):7938.768980282538
step: 297	loss: 55564.089522361755	speed (wps):7941.643225046877
step: 338	loss: 63159.047265052795	speed (wps):7943.640088000437
step: 379	loss: 70804.89959716797	speed (wps):7944.905841716895
epoch: 3	train ppl: 204.66717266606508	val ppl: 264.65738599215865	best val: 264.65738599215865	time (s) spent in epoch: 118.9505021572113

EPOCH 4 ------------------
step: 10	loss: 2074.563252925873	speed (wps):7745.356170075251
step: 51	loss: 9659.190685749054	speed (wps):7912.064645290237
step: 92	loss: 17242.187106609344	speed (wps):7932.60153269363
step: 133	loss: 24790.025346279144	speed (wps):7940.3278699318935
step: 174	loss: 32180.858824253082	speed (wps):7944.334278479312
step: 215	loss: 39730.78633069992	speed (wps):7946.766812605707
step: 256	loss: 47304.84416246414	speed (wps):7949.160647461206
step: 297	loss: 54845.67237854004	speed (wps):7950.258619010375
step: 338	loss: 62353.614761829376	speed (wps):7950.943371616799
step: 379	loss: 69916.91413640976	speed (wps):7951.2741880177455
epoch: 4	train ppl: 191.5608989632261	val ppl: 251.46006884845843	best val: 251.46006884845843	time (s) spent in epoch: 118.85859847068787

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2825.3493785858154	speed (wps):4998.39455558716
step: 51	loss: 12477.596831321716	speed (wps):5185.83569604042
step: 92	loss: 21717.25172996521	speed (wps):5207.546204057316
step: 133	loss: 30640.370111465454	speed (wps):5215.559367994631
step: 174	loss: 39253.17618370056	speed (wps):5219.295359025886
step: 215	loss: 47899.47459459305	speed (wps):5221.972924158045
step: 256	loss: 56461.88353538513	speed (wps):5223.955773748673
step: 297	loss: 64905.39140701294	speed (wps):5225.120153032704
step: 338	loss: 73267.82924890518	speed (wps):5226.89238848822
step: 379	loss: 81619.86206293106	speed (wps):5228.178266896311
epoch: 0	train ppl: 448.52762372744183	val ppl: 612.261785277213	best val: 612.261785277213	time (s) spent in epoch: 181.052508354187

EPOCH 1 ------------------
step: 10	loss: 2227.3148131370544	speed (wps):5142.714725412351
step: 51	loss: 10424.480698108673	speed (wps):5212.397231540581
step: 92	loss: 18568.391942977905	speed (wps):5226.7274063770055
step: 133	loss: 26650.724976062775	speed (wps):5232.523383056125
step: 174	loss: 34555.261249542236	speed (wps):5238.044947114235
step: 215	loss: 42590.187957286835	speed (wps):5244.048365283394
step: 256	loss: 50630.14799118042	speed (wps):5248.0520109571835
step: 297	loss: 58612.271955013275	speed (wps):5250.953935586292
step: 338	loss: 66552.45737552643	speed (wps):5252.8545073927735
step: 379	loss: 74532.40951061249	speed (wps):5254.847990379593
epoch: 1	train ppl: 269.6256083669402	val ppl: 359.35217598834413	best val: 359.35217598834413	time (s) spent in epoch: 180.05280947685242

EPOCH 2 ------------------
step: 10	loss: 2156.2872195243835	speed (wps):5187.594173037279
step: 51	loss: 10042.178874015808	speed (wps):5256.007892739049
step: 92	loss: 17919.90091562271	speed (wps):5264.640932841996
step: 133	loss: 25748.533115386963	speed (wps):5267.613442227495
step: 174	loss: 33418.876061439514	speed (wps):5268.847048968538
step: 215	loss: 41233.352580070496	speed (wps):5269.846301493852
step: 256	loss: 49070.32896280289	speed (wps):5269.980468103755
step: 297	loss: 56861.67249202728	speed (wps):5270.713517887631
step: 338	loss: 64623.668320178986	speed (wps):5271.196245510317
step: 379	loss: 72431.12793922424	speed (wps):5271.6228469697235
epoch: 2	train ppl: 231.07657603805407	val ppl: 317.371971065015	best val: 317.371971065015	time (s) spent in epoch: 179.54093313217163

EPOCH 3 ------------------
step: 10	loss: 2116.6010642051697	speed (wps):5185.41871063025
step: 51	loss: 9857.60425567627	speed (wps):5254.654872710181
step: 92	loss: 17596.87735080719	speed (wps):5264.568387952941
step: 133	loss: 25289.764420986176	speed (wps):5267.534649774163
step: 174	loss: 32830.10812997818	speed (wps):5269.246411794113
step: 215	loss: 40514.79384422302	speed (wps):5270.316083237445
step: 256	loss: 48237.87035703659	speed (wps):5270.31900801931
step: 297	loss: 55922.639684677124	speed (wps):5270.899260942754
step: 338	loss: 63578.61374378204	speed (wps):5271.525645715352
step: 379	loss: 71284.32811021805	speed (wps):5271.9198038425375
epoch: 3	train ppl: 212.17109327775356	val ppl: 292.5628661408514	best val: 292.5628661408514	time (s) spent in epoch: 179.5273516178131

EPOCH 4 ------------------
step: 10	loss: 2086.433951854706	speed (wps):5188.479664588165
step: 51	loss: 9723.03139925003	speed (wps):5257.055556884474
step: 92	loss: 17369.562368392944	speed (wps):5264.902136377036
step: 133	loss: 24974.196968078613	speed (wps):5267.62944344443
step: 174	loss: 32419.110555648804	speed (wps):5269.945286189337
step: 215	loss: 40023.70466709137	speed (wps):5270.345360121174
step: 256	loss: 47667.41390943527	speed (wps):5270.949881964828
step: 297	loss: 55271.78959131241	speed (wps):5271.604209405763
step: 338	loss: 62841.01281642914	speed (wps):5272.172028729436
step: 379	loss: 70472.15910434723	speed (wps):5272.69954102417
epoch: 4	train ppl: 199.83960293062717	val ppl: 281.72745437593466	best val: 281.72745437593466	time (s) spent in epoch: 179.50146961212158

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2847.7905464172363	speed (wps):5355.775964379005
step: 51	loss: 12460.18428325653	speed (wps):5601.1613154194165
step: 92	loss: 21842.288904190063	speed (wps):5630.970053856171
step: 133	loss: 30921.303527355194	speed (wps):5642.375066107737
step: 174	loss: 39675.31490087509	speed (wps):5647.856235589491
step: 215	loss: 48432.12106466293	speed (wps):5653.361557703915
step: 256	loss: 57075.72203397751	speed (wps):5656.756148973308
step: 297	loss: 65595.18102645874	speed (wps):5658.746014785345
step: 338	loss: 74026.7713522911	speed (wps):5660.60648745005
step: 379	loss: 82435.56844949722	speed (wps):5663.143387715797
epoch: 0	train ppl: 475.83390728452633	val ppl: 432.02884975892056	best val: 432.02884975892056	time (s) spent in epoch: 166.65363454818726

EPOCH 1 ------------------
step: 10	loss: 2239.1255807876587	speed (wps):5614.4627589407255
step: 51	loss: 10475.171444416046	speed (wps):5710.61366342251
step: 92	loss: 18657.1502327919	speed (wps):5727.5085124475845
step: 133	loss: 26771.330206394196	speed (wps):5733.789471724869
step: 174	loss: 34717.53502368927	speed (wps):5736.789034892384
step: 215	loss: 42783.733892440796	speed (wps):5739.181272597295
step: 256	loss: 50850.786068439484	speed (wps):5741.68515794208
step: 297	loss: 58860.72241783142	speed (wps):5742.506886033086
step: 338	loss: 66819.12836551666	speed (wps):5743.845264973193
step: 379	loss: 74806.71143293381	speed (wps):5744.822761203111
epoch: 1	train ppl: 274.9735789168107	val ppl: 307.57751355492474	best val: 307.57751355492474	time (s) spent in epoch: 164.34345841407776

EPOCH 2 ------------------
step: 10	loss: 2161.646821498871	speed (wps):5639.884431856996
step: 51	loss: 10063.888566493988	speed (wps):5730.994101030516
step: 92	loss: 17957.847244739532	speed (wps):5742.005223573683
step: 133	loss: 25802.644350528717	speed (wps):5744.867722489078
step: 174	loss: 33487.122950553894	speed (wps):5747.138997014105
step: 215	loss: 41319.47641372681	speed (wps):5748.976574759264
step: 256	loss: 49171.24056816101	speed (wps):5749.787207428946
step: 297	loss: 56977.90016889572	speed (wps):5750.557625387038
step: 338	loss: 64748.51632118225	speed (wps):5751.002776283369
step: 379	loss: 72565.79672574997	speed (wps):5751.610460452979
epoch: 2	train ppl: 233.17027509995324	val ppl: 278.0494497063494	best val: 278.0494497063494	time (s) spent in epoch: 164.16578555107117

EPOCH 3 ------------------
step: 10	loss: 2115.9778356552124	speed (wps):5651.439635674819
step: 51	loss: 9861.145074367523	speed (wps):5732.439458982546
step: 92	loss: 17596.23482942581	speed (wps):5741.334272930722
step: 133	loss: 25297.489697933197	speed (wps):5745.314641769654
step: 174	loss: 32841.1487531662	speed (wps):5748.278749105096
step: 215	loss: 40533.96215677261	speed (wps):5750.103734671565
step: 256	loss: 48256.46879673004	speed (wps):5751.241979711499
step: 297	loss: 55942.31576919556	speed (wps):5751.6963368755305
step: 338	loss: 63594.44164276123	speed (wps):5752.166983898647
step: 379	loss: 71296.89156532288	speed (wps):5752.607476849274
epoch: 3	train ppl: 212.40154721591819	val ppl: 269.3638284920692	best val: 269.3638284920692	time (s) spent in epoch: 164.1269371509552

EPOCH 4 ------------------
step: 10	loss: 2087.6431250572205	speed (wps):5650.166360315172
step: 51	loss: 9735.782408714294	speed (wps):5736.301534188239
step: 92	loss: 17385.595846176147	speed (wps):5746.566128031697
step: 133	loss: 24999.20958995819	speed (wps):5750.113616742224
step: 174	loss: 32454.31428194046	speed (wps):5751.842051781032
step: 215	loss: 40064.197483062744	speed (wps):5753.095189162598
step: 256	loss: 47710.2978348732	speed (wps):5753.988418711544
step: 297	loss: 55315.53682088852	speed (wps):5754.879381523916
step: 338	loss: 62900.53777694702	speed (wps):5755.162879614525
step: 379	loss: 70531.69898509979	speed (wps):5753.262877478295
epoch: 4	train ppl: 200.62251209565528	val ppl: 260.93432257981067	best val: 260.93432257981067	time (s) spent in epoch: 164.10461568832397

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2839.0085196495056	speed (wps):3633.1992795871747
step: 51	loss: 12564.498603343964	speed (wps):3743.579187067235
step: 92	loss: 21931.694571971893	speed (wps):3755.7796414243694
step: 133	loss: 30910.52218914032	speed (wps):3762.210186766818
step: 174	loss: 39595.13806581497	speed (wps):3765.782548457722
step: 215	loss: 48306.469106674194	speed (wps):3768.047189553341
step: 256	loss: 56934.960985183716	speed (wps):3771.2258849072587
step: 297	loss: 65436.00475311279	speed (wps):3775.1077899247352
step: 338	loss: 73861.25676631927	speed (wps):3779.2357655628944
step: 379	loss: 82280.24780511856	speed (wps):3781.5498547425227
epoch: 0	train ppl: 471.05832922712636	val ppl: 793.9839102694191	best val: 793.9839102694191	time (s) spent in epoch: 250.0356810092926

EPOCH 1 ------------------
step: 10	loss: 2244.8629784584045	speed (wps):3768.4170505513553
step: 51	loss: 10524.73955154419	speed (wps):3803.1629581459392
step: 92	loss: 18742.588419914246	speed (wps):3806.5946261908725
step: 133	loss: 26904.95764732361	speed (wps):3807.7511986934505
step: 174	loss: 34896.48898601532	speed (wps):3807.984012243638
step: 215	loss: 43020.61065912247	speed (wps):3808.672600892032
step: 256	loss: 51146.20806694031	speed (wps):3809.354386654771
step: 297	loss: 59216.13958120346	speed (wps):3809.804684490243
step: 338	loss: 67240.85614204407	speed (wps):3809.7909815528383
step: 379	loss: 75308.39943408966	speed (wps):3809.552746791068
epoch: 1	train ppl: 285.90334765118246	val ppl: 449.0757949564023	best val: 449.0757949564023	time (s) spent in epoch: 248.30334496498108

EPOCH 2 ------------------
step: 10	loss: 2182.299246788025	speed (wps):3767.9625541686555
step: 51	loss: 10169.661881923676	speed (wps):3803.8430389651344
step: 92	loss: 18143.828270435333	speed (wps):3808.2751676300254
step: 133	loss: 26082.190601825714	speed (wps):3809.7356563422786
step: 174	loss: 33868.54459762573	speed (wps):3810.3396063230625
step: 215	loss: 41801.056044101715	speed (wps):3810.541594794686
step: 256	loss: 49752.58224487305	speed (wps):3810.71600201272
step: 297	loss: 57661.85499191284	speed (wps):3810.920437131068
step: 338	loss: 65541.15998029709	speed (wps):3810.61821352231
step: 379	loss: 73479.41226243973	speed (wps):3810.557650835331
epoch: 2	train ppl: 250.22710809687317	val ppl: 379.10834839458937	best val: 379.10834839458937	time (s) spent in epoch: 248.2554154396057

EPOCH 3 ------------------
step: 10	loss: 2147.210075855255	speed (wps):3767.6828763104845
step: 51	loss: 10019.428069591522	speed (wps):3803.6136432980175
step: 92	loss: 17906.803703308105	speed (wps):3807.99119048157
step: 133	loss: 25773.922998905182	speed (wps):3809.824186598976
step: 174	loss: 33473.37609052658	speed (wps):3810.5393255870304
step: 215	loss: 41317.02091693878	speed (wps):3810.709752683596
step: 256	loss: 49196.297516822815	speed (wps):3811.1198508956218
step: 297	loss: 57033.65504980087	speed (wps):3811.1895592971914
step: 338	loss: 64841.92533493042	speed (wps):3811.2790938975268
step: 379	loss: 72705.47560691833	speed (wps):3811.3892220612215
epoch: 3	train ppl: 236.20033433750456	val ppl: 345.8327571457867	best val: 345.8327571457867	time (s) spent in epoch: 248.18158888816833

EPOCH 4 ------------------
step: 10	loss: 2130.173170566559	speed (wps):3770.669028578417
step: 51	loss: 9943.786084651947	speed (wps):3804.4561403339367
step: 92	loss: 17772.416048049927	speed (wps):3807.972569964709
step: 133	loss: 25573.8853764534	speed (wps):3809.120542906523
step: 174	loss: 33219.7913479805	speed (wps):3810.372994436568
step: 215	loss: 41014.950268268585	speed (wps):3810.8531290043943
step: 256	loss: 48848.89090061188	speed (wps):3811.076468359087
step: 297	loss: 56650.61188697815	speed (wps):3811.3125595056918
step: 338	loss: 64420.00888586044	speed (wps):3811.4418503184093
step: 379	loss: 72251.5277838707	speed (wps):3811.5431907026773
epoch: 4	train ppl: 228.27846148975968	val ppl: 317.54801272525896	best val: 317.54801272525896	time (s) spent in epoch: 248.17308735847473

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2832.075550556183	speed (wps):9776.392739347451
step: 30	loss: 7519.875767230988	speed (wps):10169.34121890244
step: 50	loss: 12173.564088344574	speed (wps):10258.89289912995
step: 70	loss: 16729.476056098938	speed (wps):10298.016901779481
step: 90	loss: 21182.746589183807	speed (wps):10319.800256945204
step: 110	loss: 25540.214829444885	speed (wps):10334.424113107187
step: 130	loss: 29828.327326774597	speed (wps):10344.643084611429
step: 150	loss: 34048.3536195755	speed (wps):10352.001793699206
step: 170	loss: 38194.97920036316	speed (wps):10357.800106860457
step: 190	loss: 42329.31229352951	speed (wps):10363.026807009142
epoch: 0	train ppl: 542.262640879691	val ppl: 512.3706109849052	best val: 512.3706109849052	time (s) spent in epoch: 91.49360299110413

EPOCH 1 ------------------
step: 10	loss: 2261.592700481415	speed (wps):10183.508387568605
step: 30	loss: 6328.41730594635	speed (wps):10319.4932213374
step: 50	loss: 10357.129423618317	speed (wps):10352.441840292848
step: 70	loss: 14359.307906627655	speed (wps):10365.5981042387
step: 90	loss: 18337.38862514496	speed (wps):10371.543718248238
step: 110	loss: 22291.25608921051	speed (wps):10376.314070413333
step: 130	loss: 26219.098525047302	speed (wps):10380.749235395144
step: 150	loss: 30125.721316337585	speed (wps):10382.095319987207
step: 170	loss: 33990.85186958313	speed (wps):10383.83094347244
step: 190	loss: 37865.18465280533	speed (wps):10385.555104837378
epoch: 1	train ppl: 285.620378497694	val ppl: 348.29250632661547	best val: 348.29250632661547	time (s) spent in epoch: 91.19781136512756

EPOCH 2 ------------------
step: 10	loss: 2149.4792199134827	speed (wps):10183.707575604169
step: 30	loss: 6019.856421947479	speed (wps):10319.494866692745
step: 50	loss: 9870.875537395477	speed (wps):10347.766791641701
step: 70	loss: 13705.93721151352	speed (wps):10362.355513513476
step: 90	loss: 17531.987748146057	speed (wps):10369.315421271189
step: 110	loss: 21341.355192661285	speed (wps):10374.044900349321
step: 130	loss: 25139.163661003113	speed (wps):10377.896007716114
step: 150	loss: 28919.30064678192	speed (wps):10380.941577512078
step: 170	loss: 32669.845299720764	speed (wps):10382.711876020825
step: 190	loss: 36431.93472623825	speed (wps):10384.958616673304
epoch: 2	train ppl: 231.569130585206	val ppl: 300.3340387013068	best val: 300.3340387013068	time (s) spent in epoch: 91.2023253440857

EPOCH 3 ------------------
step: 10	loss: 2092.187604904175	speed (wps):10180.003050622756
step: 30	loss: 5865.429155826569	speed (wps):10323.042558521984
step: 50	loss: 9623.736402988434	speed (wps):10351.384660751572
step: 70	loss: 13377.693383693695	speed (wps):10366.27852080265
step: 90	loss: 17119.16554927826	speed (wps):10373.366741800452
step: 110	loss: 20846.476576328278	speed (wps):10377.637540198824
step: 130	loss: 24568.147938251495	speed (wps):10380.698891835498
step: 150	loss: 28271.186864376068	speed (wps):10382.793068508037
step: 170	loss: 31949.744606018066	speed (wps):10383.928930399568
step: 190	loss: 35640.35180568695	speed (wps):10385.779124892188
epoch: 3	train ppl: 206.03831893991236	val ppl: 273.55360822970084	best val: 273.55360822970084	time (s) spent in epoch: 91.19831919670105

EPOCH 4 ------------------
step: 10	loss: 2057.061550617218	speed (wps):10180.663909632265
step: 30	loss: 5769.232928752899	speed (wps):10323.656185854861
step: 50	loss: 9463.194561004639	speed (wps):10354.58683141983
step: 70	loss: 13160.083162784576	speed (wps):10367.119016554445
step: 90	loss: 16842.762110233307	speed (wps):10373.876502692146
step: 110	loss: 20516.04765176773	speed (wps):10379.19045973187
step: 130	loss: 24183.833017349243	speed (wps):10382.345335570213
step: 150	loss: 27832.36887693405	speed (wps):10384.098169991865
step: 170	loss: 31454.506134986877	speed (wps):10385.543833384789
step: 190	loss: 35092.35820531845	speed (wps):10387.399515791367
epoch: 4	train ppl: 189.8798784304284	val ppl: 258.00100117883136	best val: 258.00100117883136	time (s) spent in epoch: 91.18244576454163

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2821.6655492782593	speed (wps):7194.622446381788
step: 30	loss: 7499.589354991913	speed (wps):7449.695536019145
step: 50	loss: 12148.840041160583	speed (wps):7507.20890582242
step: 70	loss: 16706.53218984604	speed (wps):7533.507059105663
step: 90	loss: 21163.87664794922	speed (wps):7548.222014050981
step: 110	loss: 25537.913608551025	speed (wps):7557.924275852389
step: 130	loss: 29839.730808734894	speed (wps):7564.137783958525
step: 150	loss: 34071.034779548645	speed (wps):7569.062559701784
step: 170	loss: 38226.89379930496	speed (wps):7572.30758807871
step: 190	loss: 42371.557970047	speed (wps):7574.8680608621935
epoch: 0	train ppl: 545.9029510374721	val ppl: 455.83073375249126	best val: 455.83073375249126	time (s) spent in epoch: 125.08725309371948

EPOCH 1 ------------------
step: 10	loss: 2264.812800884247	speed (wps):7499.730722704922
step: 30	loss: 6333.807265758514	speed (wps):7563.880153211619
step: 50	loss: 10366.975309848785	speed (wps):7576.242601654308
step: 70	loss: 14369.59615945816	speed (wps):7584.402396607188
step: 90	loss: 18349.93318796158	speed (wps):7588.382973166684
step: 110	loss: 22302.21653699875	speed (wps):7591.181357140333
step: 130	loss: 26233.599541187286	speed (wps):7593.29131512862
step: 150	loss: 30139.301416873932	speed (wps):7595.7946028213055
step: 170	loss: 34008.32231283188	speed (wps):7599.331049541149
step: 190	loss: 37885.282793045044	speed (wps):7602.658286304969
epoch: 1	train ppl: 286.40837176393666	val ppl: 318.95591151527753	best val: 318.95591151527753	time (s) spent in epoch: 124.41276335716248

EPOCH 2 ------------------
step: 10	loss: 2151.755073070526	speed (wps):7567.1762741847615
step: 30	loss: 6029.650037288666	speed (wps):7626.806358023712
step: 50	loss: 9882.877814769745	speed (wps):7639.355110167741
step: 70	loss: 13726.858184337616	speed (wps):7646.214250542355
step: 90	loss: 17556.204640865326	speed (wps):7649.64343731119
step: 110	loss: 21371.05502128601	speed (wps):7651.590403737358
step: 130	loss: 25173.393065929413	speed (wps):7653.208427210975
step: 150	loss: 28955.580599308014	speed (wps):7654.560827123406
step: 170	loss: 32710.734570026398	speed (wps):7655.584522799734
step: 190	loss: 36479.72839355469	speed (wps):7656.364250051093
epoch: 2	train ppl: 233.26149625255584	val ppl: 282.35560498334655	best val: 282.35560498334655	time (s) spent in epoch: 123.61443066596985

EPOCH 3 ------------------
step: 10	loss: 2095.810101032257	speed (wps):7571.578451376632
step: 30	loss: 5877.8219628334045	speed (wps):7632.965218569383
step: 50	loss: 9636.866126060486	speed (wps):7643.725419002864
step: 70	loss: 13398.490138053894	speed (wps):7649.9068035095725
step: 90	loss: 17145.179312229156	speed (wps):7652.626723729663
step: 110	loss: 20880.732567310333	speed (wps):7654.403322399035
step: 130	loss: 24606.961209774017	speed (wps):7655.967616829962
step: 150	loss: 28313.685133457184	speed (wps):7657.190178704065
step: 170	loss: 32000.156490802765	speed (wps):7657.441266859182
step: 190	loss: 35696.77399158478	speed (wps):7657.689812313842
epoch: 3	train ppl: 207.79848752956318	val ppl: 264.06549179698527	best val: 264.06549179698527	time (s) spent in epoch: 123.59822511672974

EPOCH 4 ------------------
step: 10	loss: 2060.7564449310303	speed (wps):7562.4408059996285
step: 30	loss: 5778.4215569496155	speed (wps):7624.1814776970305
step: 50	loss: 9474.405882358551	speed (wps):7639.6196616249235
step: 70	loss: 13179.368851184845	speed (wps):7645.735476429447
step: 90	loss: 16871.06511116028	speed (wps):7648.612301844578
step: 110	loss: 20550.31993150711	speed (wps):7650.539270231462
step: 130	loss: 24226.81387424469	speed (wps):7652.00613370826
step: 150	loss: 27883.647253513336	speed (wps):7653.320669736546
step: 170	loss: 31516.030440330505	speed (wps):7654.393204357447
step: 190	loss: 35163.118653297424	speed (wps):7655.320237351949
epoch: 4	train ppl: 192.01108818388397	val ppl: 251.7487495535048	best val: 251.7487495535048	time (s) spent in epoch: 123.62762188911438

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2808.568937778473	speed (wps):4226.404606448172
step: 30	loss: 7524.131455421448	speed (wps):4270.652679377438
step: 50	loss: 12169.250304698944	speed (wps):4280.600516726687
step: 70	loss: 16681.909062862396	speed (wps):4284.869212835702
step: 90	loss: 21062.36736536026	speed (wps):4288.069247965536
step: 110	loss: 25346.451699733734	speed (wps):4290.179363070179
step: 130	loss: 29561.398136615753	speed (wps):4291.277362774781
step: 150	loss: 33723.26558828354	speed (wps):4292.94304135917
step: 170	loss: 37815.523035526276	speed (wps):4294.336524334855
step: 190	loss: 41906.669726371765	speed (wps):4296.384289169576

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=256_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=256_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=256_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=64_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2845.4904437065125	speed (wps):7320.073362472634
step: 51	loss: 12512.604308128357	speed (wps):7706.82207922815
step: 92	loss: 21977.16739177704	speed (wps):7757.736793263186
step: 133	loss: 31144.14532661438	speed (wps):7777.701136372236
step: 174	loss: 40004.799411296844	speed (wps):7787.9531714031655
step: 215	loss: 48866.44093513489	speed (wps):7793.59812865658
step: 256	loss: 57615.72312831879	speed (wps):7799.896226470345
step: 297	loss: 66248.05656671524	speed (wps):7804.691763824579
step: 338	loss: 74787.73427963257	speed (wps):7807.327484814127
step: 379	loss: 83309.23440694809	speed (wps):7809.364447540462
epoch: 0	train ppl: 508.57382909299906	val ppl: 608.3030082040299	best val: 608.3030082040299	time (s) spent in epoch: 121.04176425933838

EPOCH 1 ------------------
step: 10	loss: 2267.857964038849	speed (wps):7628.067391281884
step: 51	loss: 10626.500809192657	speed (wps):7786.769718304326
step: 92	loss: 18935.691304206848	speed (wps):7818.57258208633
step: 133	loss: 27174.24735546112	speed (wps):7835.0036993651765
step: 174	loss: 35252.254037857056	speed (wps):7848.124334337651
step: 215	loss: 43447.875010967255	speed (wps):7861.7220013963715
step: 256	loss: 51650.27037143707	speed (wps):7872.933970393005
step: 297	loss: 59804.27253484726	speed (wps):7880.741970682661
step: 338	loss: 67907.81111001968	speed (wps):7886.051153930207
step: 379	loss: 76040.78884363174	speed (wps):7890.685603802846
epoch: 1	train ppl: 301.88577280798006	val ppl: 351.9199771288449	best val: 351.9199771288449	time (s) spent in epoch: 119.72267818450928

EPOCH 2 ------------------
step: 10	loss: 2200.8094000816345	speed (wps):7727.799591580058
step: 51	loss: 10251.69760465622	speed (wps):7887.709593263493
step: 92	loss: 18288.935363292694	speed (wps):7906.936855289786
step: 133	loss: 26286.66333913803	speed (wps):7914.503953035951
step: 174	loss: 34141.02620124817	speed (wps):7917.832275562631
step: 215	loss: 42134.491028785706	speed (wps):7919.982949361724
step: 256	loss: 50145.38090467453	speed (wps):7921.906170702876
step: 297	loss: 58115.36929368973	speed (wps):7922.628300205522
step: 338	loss: 66037.98471927643	speed (wps):7923.1649433337725
step: 379	loss: 74001.84256076813	speed (wps):7924.102211269871
epoch: 2	train ppl: 259.9940353675276	val ppl: 322.5606102920038	best val: 322.5606102920038	time (s) spent in epoch: 119.2559289932251

EPOCH 3 ------------------
step: 10	loss: 2157.578136920929	speed (wps):7718.607783939195
step: 51	loss: 10064.417934417725	speed (wps):7889.8333030828035
step: 92	loss: 17966.579220294952	speed (wps):7907.715138415065
step: 133	loss: 25835.0648021698	speed (wps):7913.950668975791
step: 174	loss: 33556.16139650345	speed (wps):7918.517568813988
step: 215	loss: 41430.96823453903	speed (wps):7921.7017464364735
step: 256	loss: 49327.80623435974	speed (wps):7923.85218410983
step: 297	loss: 57191.994404792786	speed (wps):7924.6972981313675
step: 338	loss: 65023.50328922272	speed (wps):7926.303567860165
step: 379	loss: 72887.78021335602	speed (wps):7927.503439924444
epoch: 3	train ppl: 239.42968594836267	val ppl: 297.62235798376446	best val: 297.62235798376446	time (s) spent in epoch: 119.21471738815308

EPOCH 4 ------------------
step: 10	loss: 2134.704782962799	speed (wps):7732.090157547444
step: 51	loss: 9956.664934158325	speed (wps):7888.45732154361
step: 92	loss: 17779.265286922455	speed (wps):7908.194664568452
step: 133	loss: 25570.160675048828	speed (wps):7917.511278216341
step: 174	loss: 33207.98667192459	speed (wps):7921.52475042194
step: 215	loss: 41000.888431072235	speed (wps):7924.536657685935
step: 256	loss: 48819.42115068436	speed (wps):7926.927640234307
step: 297	loss: 56610.09942770004	speed (wps):7927.85571032153
step: 338	loss: 64365.65687417984	speed (wps):7928.89268968373
step: 379	loss: 72167.3059630394	speed (wps):7929.221315734812
epoch: 4	train ppl: 226.99314568150925	val ppl: 293.5046632353769	best val: 293.5046632353769	time (s) spent in epoch: 119.18882417678833

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=64_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2848.8715624809265	speed (wps):4982.873691236674
step: 51	loss: 12735.81669330597	speed (wps):5165.755826523155
step: 92	loss: 22169.259121418	speed (wps):5186.62477765373
step: 133	loss: 31236.529834270477	speed (wps):5196.617105827675
step: 174	loss: 39990.91401338577	speed (wps):5201.1301007819275
step: 215	loss: 48772.83782482147	speed (wps):5204.136308203321
step: 256	loss: 57476.32199048996	speed (wps):5205.599767843468
step: 297	loss: 66092.99006700516	speed (wps):5206.180006924238
step: 338	loss: 74637.28774547577	speed (wps):5207.470626145877
step: 379	loss: 83181.29604816437	speed (wps):5209.36490947631
epoch: 0	train ppl: 505.5955927771072	val ppl: 664.180171224884	best val: 664.180171224884	time (s) spent in epoch: 181.61197710037231

EPOCH 1 ------------------
step: 10	loss: 2285.9144854545593	speed (wps):5158.210713530272
step: 51	loss: 10711.470828056335	speed (wps):5228.97874116213
step: 92	loss: 19090.60492992401	speed (wps):5237.636517905624
step: 133	loss: 27409.644498825073	speed (wps):5241.74705459638
step: 174	loss: 35573.930525779724	speed (wps):5243.889314920436
step: 215	loss: 43882.00937986374	speed (wps):5245.224938278781
step: 256	loss: 52188.91117095947	speed (wps):5245.816590686365
step: 297	loss: 60464.03773546219	speed (wps):5246.155172819626
step: 338	loss: 68691.83409452438	speed (wps):5246.403485091852
step: 379	loss: 76961.58603668213	speed (wps):5246.556303051907
epoch: 1	train ppl: 324.2363758994953	val ppl: 404.17736696605965	best val: 404.17736696605965	time (s) spent in epoch: 180.37509083747864

EPOCH 2 ------------------
step: 10	loss: 2235.0537753105164	speed (wps):5161.424695474015
step: 51	loss: 10413.708455562592	speed (wps):5231.561941522545
step: 92	loss: 18599.325845241547	speed (wps):5240.013235846345
step: 133	loss: 26742.248950004578	speed (wps):5242.792906862567
step: 174	loss: 34748.99717569351	speed (wps):5243.878644516607
step: 215	loss: 42898.86608362198	speed (wps):5244.818297701331
step: 256	loss: 51069.68847513199	speed (wps):5245.601233091741
step: 297	loss: 59197.84473657608	speed (wps):5246.3081828385475
step: 338	loss: 67295.62879085541	speed (wps):5246.774574167112
step: 379	loss: 75432.34592914581	speed (wps):5247.226422758412
epoch: 2	train ppl: 289.86217632374166	val ppl: 355.8572473158946	best val: 355.8572473158946	time (s) spent in epoch: 180.3566596508026

EPOCH 3 ------------------
step: 10	loss: 2205.6398844718933	speed (wps):5169.451342427239
step: 51	loss: 10286.19954586029	speed (wps):5234.216838633182
step: 92	loss: 18370.37397623062	speed (wps):5240.565775260235
step: 133	loss: 26417.70700931549	speed (wps):5243.657066967317
step: 174	loss: 34326.302087306976	speed (wps):5245.697504944644
step: 215	loss: 42395.611407756805	speed (wps):5246.427375491604
step: 256	loss: 50483.76448392868	speed (wps):5248.021338859499
step: 297	loss: 58532.95006275177	speed (wps):5248.597113742111
step: 338	loss: 66547.95955896378	speed (wps):5249.110259632099
step: 379	loss: 74600.38236141205	speed (wps):5249.364687528535
epoch: 3	train ppl: 272.3443169642883	val ppl: 342.36044396168154	best val: 342.36044396168154	time (s) spent in epoch: 180.28439903259277

EPOCH 4 ------------------
step: 10	loss: 2183.507652282715	speed (wps):5163.39715990213
step: 51	loss: 10190.28199672699	speed (wps):5235.026617382056
step: 92	loss: 18208.197717666626	speed (wps):5243.07558972339
step: 133	loss: 26189.38853263855	speed (wps):5246.913162444713
step: 174	loss: 34026.89067840576	speed (wps):5248.784799213133
step: 215	loss: 42028.52169752121	speed (wps):5249.6843054436185
step: 256	loss: 50052.503361701965	speed (wps):5250.713886714372
step: 297	loss: 58052.772800922394	speed (wps):5251.24590844485
step: 338	loss: 66023.45269441605	speed (wps):5251.61813037969
step: 379	loss: 74033.99478197098	speed (wps):5251.711537745959
epoch: 4	train ppl: 261.14564068226946	val ppl: 336.7361038177327	best val: 336.7361038177327	time (s) spent in epoch: 180.20105290412903

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2868.725004196167	speed (wps):3621.958679223356
step: 51	loss: 12762.94511795044	speed (wps):3736.129094502508
step: 92	loss: 22284.968461990356	speed (wps):3749.795787659974
step: 133	loss: 31451.594569683075	speed (wps):3754.7891433059244
step: 174	loss: 40285.3170132637	speed (wps):3757.226562433944
step: 215	loss: 49154.95671749115	speed (wps):3758.7683961037656
step: 256	loss: 57958.51259946823	speed (wps):3759.2407905119667
step: 297	loss: 66674.56936597824	speed (wps):3760.028796544535
step: 338	loss: 75323.54121923447	speed (wps):3760.9695073157054
step: 379	loss: 83973.49106550217	speed (wps):3762.0682846132413
epoch: 0	train ppl: 536.957962386874	val ppl: 930.9326892375311	best val: 930.9326892375311	time (s) spent in epoch: 251.4373960494995

EPOCH 1 ------------------
step: 10	loss: 2312.7704071998596	speed (wps):3729.221092881836
step: 51	loss: 10852.250068187714	speed (wps):3754.8403597403576
step: 92	loss: 19351.85728788376	speed (wps):3759.2705214084003
step: 133	loss: 27792.443914413452	speed (wps):3762.1390101262964
step: 174	loss: 36072.25810289383	speed (wps):3764.700675773831
step: 215	loss: 44485.793125629425	speed (wps):3768.597532171205
step: 256	loss: 52894.537184238434	speed (wps):3773.855593800346
step: 297	loss: 61290.77473163605	speed (wps):3778.0639387540846
step: 338	loss: 69629.70699548721	speed (wps):3781.396133357527
step: 379	loss: 78018.3929347992	speed (wps):3784.012548214623
epoch: 1	train ppl: 351.5052964477111	val ppl: 594.6791258273113	best val: 594.6791258273113	time (s) spent in epoch: 249.82552528381348

EPOCH 2 ------------------
step: 10	loss: 2271.431143283844	speed (wps):3766.260678466379
step: 51	loss: 10593.023791313171	speed (wps):3798.1785033119354
step: 92	loss: 18896.96952342987	speed (wps):3801.2586872489783
step: 133	loss: 27162.316040992737	speed (wps):3803.272483329801
step: 174	loss: 35289.75738286972	speed (wps):3803.912620425031
step: 215	loss: 43576.816313266754	speed (wps):3804.4151882332326
step: 256	loss: 51872.402641773224	speed (wps):3804.8433826658543
step: 297	loss: 60158.103346824646	speed (wps):3805.035025379312
step: 338	loss: 68398.8152551651	speed (wps):3805.2582973594835
step: 379	loss: 76686.62785768509	speed (wps):3805.4080935058446
epoch: 2	train ppl: 318.80477549221735	val ppl: 492.8225467980378	best val: 492.8225467980378	time (s) spent in epoch: 248.57750606536865

EPOCH 3 ------------------
step: 10	loss: 2249.921872615814	speed (wps):3760.0003114304245
step: 51	loss: 10493.739793300629	speed (wps):3796.253344015264
step: 92	loss: 18726.845235824585	speed (wps):3799.837823122748
step: 133	loss: 26945.249874591827	speed (wps):3802.063376792572
step: 174	loss: 35022.24725484848	speed (wps):3803.1127297390403
step: 215	loss: 43258.74865293503	speed (wps):3803.688236466338
step: 256	loss: 51519.110894203186	speed (wps):3804.2454576885425
step: 297	loss: 59758.604917526245	speed (wps):3804.69472222413
step: 338	loss: 67948.71119499207	speed (wps):3805.1149843472745
step: 379	loss: 76179.2491722107	speed (wps):3805.3890541667915
epoch: 3	train ppl: 306.7487356542077	val ppl: 445.27656174517887	best val: 445.27656174517887	time (s) spent in epoch: 248.55591773986816

EPOCH 4 ------------------
step: 10	loss: 2234.828953742981	speed (wps):3765.6358716761633
step: 51	loss: 10437.155737876892	speed (wps):3798.3625034996962
step: 92	loss: 18634.418370723724	speed (wps):3802.7587634295
step: 133	loss: 26811.598184108734	speed (wps):3804.234986251736
step: 174	loss: 34851.03225469589	speed (wps):3805.537537089832
step: 215	loss: 43053.081612586975	speed (wps):3806.556127435936
step: 256	loss: 51282.815165519714	speed (wps):3807.152092752029
step: 297	loss: 59493.39210510254	speed (wps):3807.708205071
step: 338	loss: 67665.65336465836	speed (wps):3808.122755777045
step: 379	loss: 75883.50458621979	speed (wps):3808.2001721544784
epoch: 4	train ppl: 300.39858415990915	val ppl: 409.4458199675591	best val: 409.4458199675591	time (s) spent in epoch: 248.37982034683228

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2820.093033313751	speed (wps):9796.067082072956
step: 30	loss: 7519.470500946045	speed (wps):10179.081277990379
step: 50	loss: 12195.853266716003	speed (wps):10265.21512384355
step: 70	loss: 16804.50459241867	speed (wps):10301.311929992935
step: 90	loss: 21305.544793605804	speed (wps):10322.679801309825
step: 110	loss: 25739.104928970337	speed (wps):10336.045259011102
step: 130	loss: 30104.90413427353	speed (wps):10345.30826764565
step: 150	loss: 34403.64858388901	speed (wps):10353.18711605401
step: 170	loss: 38635.858483314514	speed (wps):10360.06964121054
step: 190	loss: 42859.44946050644	speed (wps):10364.196650494358
epoch: 0	train ppl: 588.9073974717837	val ppl: 818.9612349463588	best val: 818.9612349463588	time (s) spent in epoch: 91.45596981048584

EPOCH 1 ------------------
step: 10	loss: 2301.673641204834	speed (wps):10242.787385970829
step: 30	loss: 6452.921257019043	speed (wps):10367.919723471327
step: 50	loss: 10564.44798707962	speed (wps):10394.773994551955
step: 70	loss: 14655.153095722198	speed (wps):10402.695175631634
step: 90	loss: 18721.200530529022	speed (wps):10402.56752792875
step: 110	loss: 22761.85072183609	speed (wps):10402.171228646208
step: 130	loss: 26783.879008293152	speed (wps):10409.704046597457
step: 150	loss: 30782.8884267807	speed (wps):10416.161814491443
step: 170	loss: 34745.3395652771	speed (wps):10416.418676716075
step: 190	loss: 38719.19867515564	speed (wps):10423.199031934666
epoch: 1	train ppl: 324.81860110344354	val ppl: 417.40296735504756	best val: 417.40296735504756	time (s) spent in epoch: 90.86078667640686

EPOCH 2 ------------------
step: 10	loss: 2202.152304649353	speed (wps):10299.909133501318
step: 30	loss: 6169.940831661224	speed (wps):10388.798874559305
step: 50	loss: 10122.882323265076	speed (wps):10421.686560587554
step: 70	loss: 14064.636476039886	speed (wps):10432.425259004047
step: 90	loss: 17993.55815887451	speed (wps):10439.508812244789
step: 110	loss: 21906.694016456604	speed (wps):10447.339220920938
step: 130	loss: 25810.171308517456	speed (wps):10450.022296593115
step: 150	loss: 29695.14879465103	speed (wps):10455.036892386508
step: 170	loss: 33552.30666875839	speed (wps):10455.421528408653
step: 190	loss: 37421.14109516144	speed (wps):10452.406826425617
epoch: 2	train ppl: 268.571264250313	val ppl: 346.0511976010465	best val: 346.0511976010465	time (s) spent in epoch: 90.59331107139587

EPOCH 3 ------------------
step: 10	loss: 2151.179141998291	speed (wps):10283.080749689945
step: 30	loss: 6033.8364815711975	speed (wps):10412.682842612927
step: 50	loss: 9899.165120124817	speed (wps):10442.347453250666
step: 70	loss: 13761.26357793808	speed (wps):10448.222621840405
step: 90	loss: 17613.06298494339	speed (wps):10452.612031028171
step: 110	loss: 21451.61874771118	speed (wps):10447.028390140535
step: 130	loss: 25283.05413722992	speed (wps):10449.282499871864
step: 150	loss: 29097.9851770401	speed (wps):10442.701915680542
step: 170	loss: 32887.01206922531	speed (wps):10440.415882909854
step: 190	loss: 36692.55584478378	speed (wps):10438.170794980475
epoch: 3	train ppl: 241.19474113062404	val ppl: 311.5133435415129	best val: 311.5133435415129	time (s) spent in epoch: 90.71207928657532

EPOCH 4 ------------------
step: 10	loss: 2117.402501106262	speed (wps):10248.840824297256
step: 30	loss: 5939.165389537811	speed (wps):10395.33506861086
step: 50	loss: 9745.212798118591	speed (wps):10400.849771338902
step: 70	loss: 13553.646759986877	speed (wps):10400.025346882998
step: 90	loss: 17353.20199728012	speed (wps):10402.595120324962
step: 110	loss: 21141.08602285385	speed (wps):10403.982050295333
step: 130	loss: 24924.50485944748	speed (wps):10406.55871696965
step: 150	loss: 28691.365673542023	speed (wps):10406.72042914366
step: 170	loss: 32428.352756500244	speed (wps):10406.163977375732
step: 190	loss: 36184.167206287384	speed (wps):10404.032089384238
epoch: 4	train ppl: 223.59462403229418	val ppl: 293.58079054979845	best val: 293.58079054979845	time (s) spent in epoch: 91.05739068984985

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2822.2416472434998	speed (wps):5795.595687922705
step: 30	loss: 7591.780567169189	speed (wps):5872.301278498528
step: 50	loss: 12328.280229568481	speed (wps):5889.420429065438
step: 70	loss: 16960.450468063354	speed (wps):5897.385968988526
step: 90	loss: 21503.351924419403	speed (wps):5901.255478962541
step: 110	loss: 25953.584520816803	speed (wps):5903.6968727470285
step: 130	loss: 30315.563945770264	speed (wps):5905.665876037871
step: 150	loss: 34612.14384317398	speed (wps):5906.82701850772
step: 170	loss: 38841.20710849762	speed (wps):5907.105494371893
step: 190	loss: 43058.01727294922	speed (wps):5907.74997286767
epoch: 0	train ppl: 604.9133014730099	val ppl: 1407.2971317630468	best val: 1407.2971317630468	time (s) spent in epoch: 160.8646957874298

EPOCH 1 ------------------
step: 10	loss: 2304.5069670677185	speed (wps):5861.949541562153
step: 30	loss: 6461.794457435608	speed (wps):5895.736504897564
step: 50	loss: 10580.820140838623	speed (wps):5902.038950807485
step: 70	loss: 14671.46216392517	speed (wps):5905.1329932337685
step: 90	loss: 18746.152753829956	speed (wps):5907.2289927788415
step: 110	loss: 22798.235516548157	speed (wps):5908.055356641671
step: 130	loss: 26832.285339832306	speed (wps):5908.555412978778
step: 150	loss: 30844.709134101868	speed (wps):5909.294069725037
step: 170	loss: 34823.53983640671	speed (wps):5909.803087429622
step: 190	loss: 38812.57164001465	speed (wps):5909.628288267867
epoch: 1	train ppl: 329.59638191655245	val ppl: 681.1655127213721	best val: 681.1655127213721	time (s) spent in epoch: 160.8212673664093

EPOCH 2 ------------------
step: 10	loss: 2213.6830925941467	speed (wps):5855.018268950415
step: 30	loss: 6203.376476764679	speed (wps):5892.938402775333
step: 50	loss: 10182.939455509186	speed (wps):5900.021273245733
step: 70	loss: 14152.953667640686	speed (wps):5904.370584055968
step: 90	loss: 18110.614025592804	speed (wps):5905.579981936999
step: 110	loss: 22053.893229961395	speed (wps):5906.684969786819
step: 130	loss: 25990.95365047455	speed (wps):5906.149009914095
step: 150	loss: 29914.039702415466	speed (wps):5906.588887440499
step: 170	loss: 33805.48479318619	speed (wps):5908.073235944128
step: 190	loss: 37715.8434677124	speed (wps):5909.36189985964
epoch: 2	train ppl: 281.0873119621003	val ppl: 478.02954909487454	best val: 478.02954909487454	time (s) spent in epoch: 160.8143835067749

EPOCH 3 ------------------
step: 10	loss: 2173.9899611473083	speed (wps):5857.66051962966
step: 30	loss: 6093.2538294792175	speed (wps):5896.908771677131
step: 50	loss: 10009.705383777618	speed (wps):5907.033969816681
step: 70	loss: 13920.500166416168	speed (wps):5913.589964620078
step: 90	loss: 17820.910515785217	speed (wps):5919.253149948562
step: 110	loss: 21707.025227546692	speed (wps):5922.631383796622
step: 130	loss: 25588.91143798828	speed (wps):5925.561963917778
step: 150	loss: 29457.563016414642	speed (wps):5927.539138535301
step: 170	loss: 33302.45522260666	speed (wps):5929.298867065526
step: 190	loss: 37161.54359817505	speed (wps):5930.798640399005
epoch: 3	train ppl: 258.96591528017467	val ppl: 406.6349258601172	best val: 406.6349258601172	time (s) spent in epoch: 160.24505281448364

EPOCH 4 ------------------
step: 10	loss: 2149.997389316559	speed (wps):5889.879128783012
step: 30	loss: 6027.910010814667	speed (wps):5924.445114259863
step: 50	loss: 9899.983012676239	speed (wps):5930.061718732596
step: 70	loss: 13771.403095722198	speed (wps):5934.125359981392
step: 90	loss: 17634.169759750366	speed (wps):5936.648629851818
step: 110	loss: 21481.32925748825	speed (wps):5937.288587054214
step: 130	loss: 25324.999856948853	speed (wps):5938.113889528763
step: 150	loss: 29153.53464603424	speed (wps):5939.160346897314
step: 170	loss: 32959.3204665184	speed (wps):5939.645676389476
step: 190	loss: 36784.18970108032	speed (wps):5939.670271515106
epoch: 4	train ppl: 244.89217984666664	val ppl: 373.2900648065498	best val: 373.2900648065498	time (s) spent in epoch: 160.01940751075745

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=128_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2822.218849658966	speed (wps):4219.903692952909
step: 30	loss: 7632.822160720825	speed (wps):4266.041412035283
step: 50	loss: 12323.490099906921	speed (wps):4276.382544735109
step: 70	loss: 16922.631545066833	speed (wps):4280.89041905598
step: 90	loss: 21446.090416908264	speed (wps):4283.350365661972
step: 110	loss: 25878.492214679718	speed (wps):4285.578223986449
step: 130	loss: 30234.0869140625	speed (wps):4286.932568965808
step: 150	loss: 34531.477410793304	speed (wps):4288.212325438496
step: 170	loss: 38759.66532230377	speed (wps):4289.525794938844
step: 190	loss: 42978.366651535034	speed (wps):4291.236773984001

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.003_batch_size=256_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
Set compute mode to DEFAULT for GPU 00000000:04:00.0.
All done.
