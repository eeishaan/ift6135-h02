
########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3499.8252964019775	speed (wps):6204.840183158318
step: 51	loss: 14272.426896095276	speed (wps):6400.332675661235
step: 92	loss: 24256.01480960846	speed (wps):6424.243040563485
step: 133	loss: 34028.592586517334	speed (wps):6434.960869077975
step: 174	loss: 43585.14303922653	speed (wps):6441.820825595494
step: 215	loss: 53159.886598587036	speed (wps):6444.132842214253
step: 256	loss: 62641.19217157364	speed (wps):6446.37538553543
step: 297	loss: 72033.11106920242	speed (wps):6448.030385744951
step: 338	loss: 81334.70680475235	speed (wps):6449.454944442857
step: 379	loss: 90625.83943367004	speed (wps):6450.70357644315
epoch: 0	train ppl: 880.9524750937394	val ppl: 489.18231620512125	best val: 489.18231620512125	time (s) spent in epoch: 146.94826674461365

EPOCH 1 ------------------
step: 10	loss: 2485.61368227005	speed (wps):6338.401587439241
step: 51	loss: 11633.492364883423	speed (wps):6432.910951197639
step: 92	loss: 20752.924201488495	speed (wps):6442.503909101262
step: 133	loss: 29816.672337055206	speed (wps):6446.3072707041265
step: 174	loss: 38735.613753795624	speed (wps):6448.309677090621
step: 215	loss: 47776.94759130478	speed (wps):6450.317197970313
step: 256	loss: 56797.48392343521	speed (wps):6450.749130309365
step: 297	loss: 65756.11728668213	speed (wps):6451.172731606452
step: 338	loss: 74662.03091859818	speed (wps):6451.974194944819
step: 379	loss: 83599.1500878334	speed (wps):6452.3335942858
epoch: 1	train ppl: 532.0327602903832	val ppl: 387.39100204435033	best val: 387.39100204435033	time (s) spent in epoch: 146.9234278202057

EPOCH 2 ------------------
step: 10	loss: 2401.4068484306335	speed (wps):6331.692828660313
step: 51	loss: 11243.766422271729	speed (wps):6428.483727954399
step: 92	loss: 20073.995044231415	speed (wps):6437.252586654024
step: 133	loss: 28862.3969745636	speed (wps):6442.174938698618
step: 174	loss: 37504.82640504837	speed (wps):6444.444346890459
step: 215	loss: 46284.16973352432	speed (wps):6446.869976608284
step: 256	loss: 55056.49107217789	speed (wps):6448.467726420568
step: 297	loss: 63769.68033313751	speed (wps):6449.364781768782
step: 338	loss: 72442.16263771057	speed (wps):6449.905371169507
step: 379	loss: 81152.10779905319	speed (wps):6450.192439747388
epoch: 2	train ppl: 443.76193813657045	val ppl: 335.1557075422236	best val: 335.1557075422236	time (s) spent in epoch: 146.97362065315247

EPOCH 3 ------------------
step: 10	loss: 2349.884407520294	speed (wps):6333.527432733468
step: 51	loss: 10990.733761787415	speed (wps):6430.276043062238
step: 92	loss: 19621.322083473206	speed (wps):6439.894406822063
step: 133	loss: 28217.97256231308	speed (wps):6444.3401442221
step: 174	loss: 36667.801756858826	speed (wps):6447.088637475383
step: 215	loss: 45260.48201799393	speed (wps):6448.752624119828
step: 256	loss: 53870.79636096954	speed (wps):6449.4194948006
step: 297	loss: 62426.626496315	speed (wps):6450.773537814952
step: 338	loss: 70935.30339956284	speed (wps):6450.927675005563
step: 379	loss: 79495.78717947006	speed (wps):6451.504475080877
epoch: 3	train ppl: 392.8048528006083	val ppl: 308.16218238469844	best val: 308.16218238469844	time (s) spent in epoch: 146.96088075637817

EPOCH 4 ------------------
step: 10	loss: 2313.9112877845764	speed (wps):6331.338292167366
step: 51	loss: 10815.997703075409	speed (wps):6426.435842160292
step: 92	loss: 19305.501370429993	speed (wps):6438.266225234632
step: 133	loss: 27763.00517320633	speed (wps):6442.158917776989
step: 174	loss: 36077.36434459686	speed (wps):6445.534934101342
step: 215	loss: 44533.74839544296	speed (wps):6446.676914675519
step: 256	loss: 53011.85695886612	speed (wps):6447.192637367021
step: 297	loss: 61444.38322067261	speed (wps):6447.623369648663
step: 338	loss: 69834.84179735184	speed (wps):6447.339106217858
step: 379	loss: 78283.79924297333	speed (wps):6448.74894932125
epoch: 4	train ppl: 358.7266439110007	val ppl: 283.70523459385913	best val: 283.70523459385913	time (s) spent in epoch: 147.01816987991333

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3251.382977962494	speed (wps):8130.286620555971
step: 51	loss: 13545.114171504974	speed (wps):8478.123827897425
step: 92	loss: 23273.223156929016	speed (wps):8524.54577902756
step: 133	loss: 32710.269072055817	speed (wps):8544.531118010527
step: 174	loss: 41892.44737148285	speed (wps):8553.204692861847
step: 215	loss: 51137.683222293854	speed (wps):8557.338070376165
step: 256	loss: 60298.93625974655	speed (wps):8559.808511999448
step: 297	loss: 69357.54031896591	speed (wps):8561.249955753236
step: 338	loss: 78329.74994421005	speed (wps):8563.759481714485
step: 379	loss: 87280.73273181915	speed (wps):8566.50407746474
epoch: 0	train ppl: 686.0139050157846	val ppl: 391.6432400509281	best val: 391.6432400509281	time (s) spent in epoch: 110.69710659980774

EPOCH 1 ------------------
step: 10	loss: 2396.4243245124817	speed (wps):8294.347320748539
step: 51	loss: 11197.909064292908	speed (wps):8515.046795303657
step: 92	loss: 19954.64469909668	speed (wps):8539.845332128849
step: 133	loss: 28649.645960330963	speed (wps):8552.969830828806
step: 174	loss: 37169.76853609085	speed (wps):8559.989649088395
step: 215	loss: 45825.24419307709	speed (wps):8565.40107173493
step: 256	loss: 54474.277980327606	speed (wps):8566.27891610184
step: 297	loss: 63051.46189212799	speed (wps):8566.712247412222
step: 338	loss: 71575.79761505127	speed (wps):8567.802202025296
step: 379	loss: 80126.39765739441	speed (wps):8568.7711035318
epoch: 1	train ppl: 409.71714295663224	val ppl: 307.19024733223995	best val: 307.19024733223995	time (s) spent in epoch: 110.69280338287354

EPOCH 2 ------------------
step: 10	loss: 2307.0680570602417	speed (wps):8316.05715140042
step: 51	loss: 10768.867819309235	speed (wps):8523.667888633234
step: 92	loss: 19206.43493413925	speed (wps):8543.918741318706
step: 133	loss: 27608.362498283386	speed (wps):8556.800151332802
step: 174	loss: 35851.363945007324	speed (wps):8560.853641932601
step: 215	loss: 44232.98031330109	speed (wps):8562.91821352348
step: 256	loss: 52621.942677497864	speed (wps):8564.349954714316
step: 297	loss: 60951.212537288666	speed (wps):8566.570467740816
step: 338	loss: 69239.07270431519	speed (wps):8568.721358431414
step: 379	loss: 77587.63045549393	speed (wps):8569.80365156111
epoch: 2	train ppl: 339.7656241695358	val ppl: 269.92358853321144	best val: 269.92358853321144	time (s) spent in epoch: 110.67316246032715

EPOCH 3 ------------------
step: 10	loss: 2254.43097114563	speed (wps):8286.620684965908
step: 51	loss: 10521.808693408966	speed (wps):8516.771370491773
step: 92	loss: 18768.105642795563	speed (wps):8542.986641717609
step: 133	loss: 26981.3179063797	speed (wps):8553.861793201051
step: 174	loss: 35030.44512271881	speed (wps):8559.610322856706
step: 215	loss: 43222.59348630905	speed (wps):8563.455011701531
step: 256	loss: 51453.469421863556	speed (wps):8564.168940795766
step: 297	loss: 59623.62571001053	speed (wps):8566.383191563838
step: 338	loss: 67749.58257198334	speed (wps):8567.6015978555
step: 379	loss: 75938.080701828	speed (wps):8568.325034031825
epoch: 3	train ppl: 300.59832739672015	val ppl: 245.2008313121387	best val: 245.2008313121387	time (s) spent in epoch: 110.70409369468689

EPOCH 4 ------------------
step: 10	loss: 2214.47429895401	speed (wps):8293.167242600837
step: 51	loss: 10341.100318431854	speed (wps):8510.904317269958
step: 92	loss: 18454.796919822693	speed (wps):8541.958293542077
step: 133	loss: 26518.862328529358	speed (wps):8553.247470423577
step: 174	loss: 34432.56177902222	speed (wps):8558.403048597147
step: 215	loss: 42499.76751327515	speed (wps):8562.417898439035
step: 256	loss: 50595.14899253845	speed (wps):8564.267663665814
step: 297	loss: 58637.620866298676	speed (wps):8566.343692931561
step: 338	loss: 66644.73173379898	speed (wps):8568.597382056301
step: 379	loss: 74710.10496616364	speed (wps):8569.449499514956
epoch: 4	train ppl: 274.2200506948962	val ppl: 226.89120586281788	best val: 226.89120586281788	time (s) spent in epoch: 110.68477869033813

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:04:00.0.
All done.
