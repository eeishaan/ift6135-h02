
Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/2.1.1

[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 195105 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199501 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199547 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 455, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 399, in run_epoch
    loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 4.13 GiB already allocated; 148.88 MiB free; 49.40 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 452, in <module>
    train_ppl, train_loss = run_epoch(model, train_data, True, lr)
  File "ptb-lm.py", line 406, in run_epoch
    loss.backward()
  File "/miniconda/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 3.84 GiB already allocated; 107.25 MiB free; 383.57 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199701 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 452, in <module>
    train_ppl, train_loss = run_epoch(model, train_data, True, lr)
  File "ptb-lm.py", line 406, in run_epoch
    loss.backward()
  File "/miniconda/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 4.16 GiB already allocated; 145.50 MiB free; 17.18 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199798 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199843 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199888 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651560.gpu-srv1.helios.SC: line 1: 199934 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
