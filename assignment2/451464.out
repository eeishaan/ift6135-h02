
########## Setting Up Experiment ######################

Putting log in ./results/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=7_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3370.7336568832397	speed (wps):1553.9077631586608
step: 142	loss: 36364.795405864716	speed (wps):1608.3030932199242
step: 274	loss: 67068.56081008911	speed (wps):1610.5458513443298
step: 406	loss: 96586.22503519058	speed (wps):1611.4313830522424
step: 538	loss: 125658.12348365784	speed (wps):1612.003770378018
step: 670	loss: 154345.96125602722	speed (wps):1612.1900181948174
step: 802	loss: 182332.8541636467	speed (wps):1612.3114987406345
step: 934	loss: 210137.21827745438	speed (wps):1612.9095259468077
step: 1066	loss: 237752.1540904045	speed (wps):1613.9695611780216
step: 1198	loss: 265033.3238697052	speed (wps):1614.821077832223
epoch: 0	train ppl: 529.9794318589306	val ppl: 316.3373298668411	best val: 316.3373298668411	time (s) spent in epoch: 585.8817715644836

EPOCH 1 ------------------
step: 10	loss: 2313.771848678589	speed (wps):1591.6652616776273
step: 142	loss: 29089.095571041107	speed (wps):1620.2551354293803
step: 274	loss: 56157.787573337555	speed (wps):1621.6002607852838
step: 406	loss: 82808.62121343613	speed (wps):1622.0774519873978
step: 538	loss: 109494.71650838852	speed (wps):1621.8707787676774
step: 670	loss: 136091.85819864273	speed (wps):1622.0617964765077
step: 802	loss: 162197.42742300034	speed (wps):1622.0427069897119
step: 934	loss: 188338.3529472351	speed (wps):1622.0856359489255
step: 1066	loss: 214400.95268011093	speed (wps):1622.2249134400288
step: 1198	loss: 240196.89141988754	speed (wps):1622.1875020131731
epoch: 1	train ppl: 301.7356937540342	val ppl: 238.5971170821606	best val: 238.5971170821606	time (s) spent in epoch: 583.4391663074493

EPOCH 2 ------------------
step: 10	loss: 2211.858551502228	speed (wps):1594.9574752292765
step: 142	loss: 27666.725850105286	speed (wps):1620.1753289348928
step: 274	loss: 53658.781962394714	speed (wps):1621.392826353224
step: 406	loss: 79232.30583190918	speed (wps):1621.714117662626
step: 538	loss: 104916.73419237137	speed (wps):1622.0515159972404
step: 670	loss: 130563.49817276001	speed (wps):1622.3961423827172
step: 802	loss: 155727.6746559143	speed (wps):1622.3337373662616
step: 934	loss: 180996.79718255997	speed (wps):1622.4151890155601
step: 1066	loss: 206229.23555850983	speed (wps):1622.4447841250667
step: 1198	loss: 231140.5280661583	speed (wps):1622.4899646312165
epoch: 2	train ppl: 244.3284527850611	val ppl: 202.83921017911237	best val: 202.83921017911237	time (s) spent in epoch: 583.339085817337

EPOCH 3 ------------------
step: 10	loss: 2148.9442443847656	speed (wps):1596.2476284487889
step: 142	loss: 26830.019433498383	speed (wps):1620.6731787710125
step: 274	loss: 52138.10458421707	speed (wps):1621.1917730751347
step: 406	loss: 76976.58122301102	speed (wps):1621.954830550924
step: 538	loss: 101963.60658884048	speed (wps):1622.1352777824338
step: 670	loss: 126918.82380008698	speed (wps):1622.2823575347636
step: 802	loss: 151423.36499214172	speed (wps):1622.265907523303
step: 934	loss: 176102.2920179367	speed (wps):1622.4685589087992
step: 1066	loss: 200771.6866827011	speed (wps):1622.3634914923398
step: 1198	loss: 225039.75675821304	speed (wps):1622.3586357184836
epoch: 3	train ppl: 211.7532603741309	val ppl: 183.8865661640047	best val: 183.8865661640047	time (s) spent in epoch: 583.4018681049347

EPOCH 4 ------------------
step: 10	loss: 2103.493354320526	speed (wps):1593.4323402300688
step: 142	loss: 26193.254776000977	speed (wps):1619.7270418760506
step: 274	loss: 50990.02091407776	speed (wps):1620.8378502314656
step: 406	loss: 75249.14201974869	speed (wps):1621.5873283125175
step: 538	loss: 99669.92136478424	speed (wps):1621.9260075625527
step: 670	loss: 124083.3649110794	speed (wps):1622.0056125877393
step: 802	loss: 148076.252887249	speed (wps):1622.111195617202
step: 934	loss: 172283.9549255371	speed (wps):1622.1450871580182
step: 1066	loss: 196474.59629297256	speed (wps):1622.2151681989449
step: 1198	loss: 220213.59671592712	speed (wps):1622.228792536478
epoch: 4	train ppl: 188.9496557301142	val ppl: 168.82202387015826	best val: 168.82202387015826	time (s) spent in epoch: 583.4289147853851

DONE

Saving learning curves to ./results/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=7_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=7_batch_size=20_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3316.0697054862976	speed (wps):1250.4738061610656
step: 142	loss: 36407.904036045074	speed (wps):1279.1038722302642
step: 274	loss: 67148.80124807358	speed (wps):1280.3554395827039
step: 406	loss: 96709.54509735107	speed (wps):1280.7998877648995
step: 538	loss: 125836.14624977112	speed (wps):1281.031147186398
step: 670	loss: 154543.1713938713	speed (wps):1281.1478358660888
step: 802	loss: 182537.43060827255	speed (wps):1281.2096396381226
step: 934	loss: 210385.09210824966	speed (wps):1281.2792416601455
step: 1066	loss: 238039.60386037827	speed (wps):1281.301311592873
step: 1198	loss: 265363.1422328949	speed (wps):1281.3276226302437
epoch: 0	train ppl: 534.0707902875989	val ppl: 319.73977949510476	best val: 319.73977949510476	time (s) spent in epoch: 739.0655407905579

EPOCH 1 ------------------
step: 10	loss: 2320.3400564193726	speed (wps):1264.0383555846977
step: 142	loss: 29127.88095474243	speed (wps):1280.0853071023457
step: 274	loss: 56278.78642082214	speed (wps):1280.7090857788937
step: 406	loss: 83008.99342298508	speed (wps):1280.9425673711748
step: 538	loss: 109761.10574960709	speed (wps):1281.0221306612464
step: 670	loss: 136395.66365003586	speed (wps):1281.077356092484
step: 802	loss: 162550.176050663	speed (wps):1281.1168943498926
step: 934	loss: 188714.68545913696	speed (wps):1281.167492235839
step: 1066	loss: 214829.67250585556	speed (wps):1281.1826656673004
step: 1198	loss: 240651.42767429352	speed (wps):1281.2180290656556
epoch: 1	train ppl: 305.18012134716224	val ppl: 242.73294715515257	best val: 242.73294715515257	time (s) spent in epoch: 739.1200737953186

EPOCH 2 ------------------
step: 10	loss: 2215.4689478874207	speed (wps):1263.56996866911
step: 142	loss: 27717.82154560089	speed (wps):1280.1712774423775
step: 274	loss: 53749.256246089935	speed (wps):1280.836091636097
step: 406	loss: 79370.16931772232	speed (wps):1280.9946558666168
step: 538	loss: 105077.04134941101	speed (wps):1281.0650637309532
step: 670	loss: 130733.47424268723	speed (wps):1281.144216715787
step: 802	loss: 155917.29034423828	speed (wps):1281.1481592957164
step: 934	loss: 181241.05707883835	speed (wps):1281.167990302458
step: 1066	loss: 206529.71351385117	speed (wps):1281.2126432794014
step: 1198	loss: 231472.49279022217	speed (wps):1281.2373679087632
epoch: 2	train ppl: 246.18809037486741	val ppl: 207.51896628256458	best val: 207.51896628256458	time (s) spent in epoch: 739.0853488445282

EPOCH 3 ------------------
step: 10	loss: 2155.1126098632812	speed (wps):1264.192532833692
step: 142	loss: 26864.64519262314	speed (wps):1280.3034287112964
step: 274	loss: 52194.37548160553	speed (wps):1281.014701886514
step: 406	loss: 77038.07438611984	speed (wps):1281.2241961521427
step: 538	loss: 102064.60529565811	speed (wps):1281.3127298812537
step: 670	loss: 127041.86441659927	speed (wps):1281.3756101978904
step: 802	loss: 151553.55285167694	speed (wps):1281.378863930274
step: 934	loss: 176268.93782138824	speed (wps):1281.4119226566631
step: 1066	loss: 200978.26722860336	speed (wps):1281.3999093496036
step: 1198	loss: 225279.39433574677	speed (wps):1281.4150559286898
epoch: 3	train ppl: 212.82618857122594	val ppl: 183.53514695532118	best val: 183.53514695532118	time (s) spent in epoch: 738.911379814148

EPOCH 4 ------------------
step: 10	loss: 2106.684582233429	speed (wps):1264.2835922612371
step: 142	loss: 26177.99214363098	speed (wps):1279.884742834272
step: 274	loss: 51007.16861963272	speed (wps):1280.6750636286847
step: 406	loss: 75274.677901268	speed (wps):1280.8758786399642
step: 538	loss: 99703.1021809578	speed (wps):1280.8166266752876
step: 670	loss: 124121.74511194229	speed (wps):1280.8246573917422
step: 802	loss: 148125.84015369415	speed (wps):1280.7679527671391
step: 934	loss: 172366.70318126678	speed (wps):1280.7850262593402
step: 1066	loss: 196568.3301305771	speed (wps):1280.7782507313334
step: 1198	loss: 220330.49558639526	speed (wps):1280.7821162270257
epoch: 4	train ppl: 189.45881240541883	val ppl: 173.59466696347957	best val: 173.59466696347957	time (s) spent in epoch: 739.3789532184601

DONE

Saving learning curves to ./results/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=7_batch_size=20_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:04:00.0.
All done.
