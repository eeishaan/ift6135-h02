
Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/2.1.1

[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 167130 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 176, in <module>
    os.mkdir(experiment_path)
FileExistsError: [Errno 17] File exists: './results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5'
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 176, in <module>
    os.mkdir(experiment_path)
FileExistsError: [Errno 17] File exists: './results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5'
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 167675 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 455, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 399, in run_epoch
    loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 904, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1970, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1295, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 171.00 MiB (GPU 0; 4.63 GiB total capacity; 3.93 GiB already allocated; 15.75 MiB free; 384.54 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 455, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 399, in run_epoch
    loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 4.13 GiB already allocated; 148.88 MiB free; 49.40 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 452, in <module>
    train_ppl, train_loss = run_epoch(model, train_data, True, lr)
  File "ptb-lm.py", line 406, in run_epoch
    loss.backward()
  File "/miniconda/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 3.84 GiB already allocated; 107.25 MiB free; 383.57 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 168071 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 168117 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 170940 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 455, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 399, in run_epoch
    loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 904, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1970, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1295, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 171.00 MiB (GPU 0; 4.63 GiB total capacity; 3.93 GiB already allocated; 15.75 MiB free; 384.54 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 171198 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 452, in <module>
    train_ppl, train_loss = run_epoch(model, train_data, True, lr)
  File "ptb-lm.py", line 406, in run_epoch
    loss.backward()
  File "/miniconda/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 3.84 GiB already allocated; 107.25 MiB free; 383.57 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 455, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 383, in run_epoch
    outputs = model.forward(batch.data, batch.mask).transpose(1,0)
  File "/rap/jvb-000-aa/COURS2019/etudiants/user36/ift6135-h02/assignment2/models.py", line 524, in forward
    return F.log_softmax(self.output_layer(self.transformer_stack(embeddings, mask)), dim=-1)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/rap/jvb-000-aa/COURS2019/etudiants/user36/ift6135-h02/assignment2/models.py", line 511, in forward
    x = layer(x, mask)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/rap/jvb-000-aa/COURS2019/etudiants/user36/ift6135-h02/assignment2/models.py", line 496, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/rap/jvb-000-aa/COURS2019/etudiants/user36/ift6135-h02/assignment2/models.py", line 607, in forward
    return x + self.dropout(sublayer(self.norm(x)))
RuntimeError: CUDA out of memory. Tried to allocate 17.50 MiB (GPU 0; 4.63 GiB total capacity; 4.30 GiB already allocated; 9.50 MiB free; 9.97 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 452, in <module>
    train_ppl, train_loss = run_epoch(model, train_data, True, lr)
  File "ptb-lm.py", line 406, in run_epoch
    loss.backward()
  File "/miniconda/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 4.16 GiB already allocated; 145.50 MiB free; 17.18 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 172092 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 173640 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 455, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 399, in run_epoch
    loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 904, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1970, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1295, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 171.00 MiB (GPU 0; 4.63 GiB total capacity; 3.93 GiB already allocated; 15.75 MiB free; 384.54 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 174431 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 174476 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/651040.gpu-srv1.helios.SC: line 1: 174522 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 452, in <module>
    train_ppl, train_loss = run_epoch(model, train_data, True, lr)
  File "ptb-lm.py", line 406, in run_epoch
    loss.backward()
  File "/miniconda/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 341.88 MiB (GPU 0; 4.63 GiB total capacity; 4.16 GiB already allocated; 145.50 MiB free; 17.18 MiB cached)

==============NVSMI LOG==============

Timestamp                           : Sun Mar 24 00:06:58 2019
Driver Version                      : 410.73
CUDA Version                        : 10.0

Attached GPUs                       : 8
GPU 00000000:04:00.0
    Accounting Mode                 : Enabled
    Accounting Mode Buffer Size     : 4000
    Accounted Processes
        Process ID                  : 167195
            GPU Utilization         : 81 %
            Memory Utilization      : 34 %
            Max memory usage        : 2440 MiB
            Time                    : 920701 ms
            Is Running              : 0
        Process ID                  : 167405
            GPU Utilization         : 80 %
            Memory Utilization      : 33 %
            Max memory usage        : 2915 MiB
            Time                    : 1256263 ms
            Is Running              : 0
        Process ID                  : 167739
            GPU Utilization         : 77 %
            Memory Utilization      : 34 %
            Max memory usage        : 3726 MiB
            Time                    : 631311 ms
            Is Running              : 0
        Process ID                  : 167919
            GPU Utilization         : 83 %
            Memory Utilization      : 33 %
            Max memory usage        : 4722 MiB
            Time                    : 228544 ms
            Is Running              : 0
        Process ID                  : 167986
            GPU Utilization         : 79 %
            Memory Utilization      : 35 %
            Max memory usage        : 4595 MiB
            Time                    : 89637 ms
            Is Running              : 0
        Process ID                  : 168042
            GPU Utilization         : 13 %
            Memory Utilization      : 4 %
            Max memory usage        : 4650 MiB
            Time                    : 13810 ms
            Is Running              : 0
        Process ID                  : 168185
            GPU Utilization         : 69 %
            Memory Utilization      : 28 %
            Max memory usage        : 1778 MiB
            Time                    : 608369 ms
            Is Running              : 0
        Process ID                  : 168568
            GPU Utilization         : 82 %
            Memory Utilization      : 34 %
            Max memory usage        : 2440 MiB
            Time                    : 911799 ms
            Is Running              : 0
        Process ID                  : 168748
            GPU Utilization         : 68 %
            Memory Utilization      : 27 %
            Max memory usage        : 2052 MiB
            Time                    : 834705 ms
            Is Running              : 0
        Process ID                  : 168967
            GPU Utilization         : 80 %
            Memory Utilization      : 33 %
            Max memory usage        : 2915 MiB
            Time                    : 1255199 ms
            Is Running              : 0
        Process ID                  : 170827
            GPU Utilization         : 78 %
            Memory Utilization      : 35 %
            Max memory usage        : 3306 MiB
            Time                    : 467632 ms
            Is Running              : 0
        Process ID                  : 171008
            GPU Utilization         : 78 %
            Memory Utilization      : 34 %
            Max memory usage        : 3726 MiB
            Time                    : 631530 ms
            Is Running              : 0
        Process ID                  : 171132
            GPU Utilization         : 86 %
            Memory Utilization      : 35 %
            Max memory usage        : 4722 MiB
            Time                    : 228439 ms
            Is Running              : 0
        Process ID                  : 171267
            GPU Utilization         : 17 %
            Memory Utilization      : 5 %
            Max memory usage        : 4650 MiB
            Time                    : 13448 ms
            Is Running              : 0
        Process ID                  : 171318
            GPU Utilization         : 78 %
            Memory Utilization      : 35 %
            Max memory usage        : 4723 MiB
            Time                    : 117022 ms
            Is Running              : 0
        Process ID                  : 171381
            GPU Utilization         : 10 %
            Memory Utilization      : 2 %
            Max memory usage        : 4651 MiB
            Time                    : 12460 ms
            Is Running              : 0
        Process ID                  : 171432
            GPU Utilization         : 70 %
            Memory Utilization      : 28 %
            Max memory usage        : 1778 MiB
            Time                    : 609444 ms
            Is Running              : 0
        Process ID                  : 171602
            GPU Utilization         : 81 %
            Memory Utilization      : 34 %
            Max memory usage        : 2440 MiB
            Time                    : 914659 ms
            Is Running              : 0
        Process ID                  : 172161
            GPU Utilization         : 80 %
            Memory Utilization      : 33 %
            Max memory usage        : 2915 MiB
            Time                    : 1259089 ms
            Is Running              : 0
        Process ID                  : 173345
            GPU Utilization         : 78 %
            Memory Utilization      : 35 %
            Max memory usage        : 3306 MiB
            Time                    : 466046 ms
            Is Running              : 0
        Process ID                  : 173479
            GPU Utilization         : 88 %
            Memory Utilization      : 36 %
            Max memory usage        : 4067 MiB
            Time                    : 814918 ms
            Is Running              : 0
        Process ID                  : 173712
            GPU Utilization         : 85 %
            Memory Utilization      : 34 %
            Max memory usage        : 4722 MiB
            Time                    : 229155 ms
            Is Running              : 0
        Process ID                  : 174591
            GPU Utilization         : 6 %
            Memory Utilization      : 1 %
            Max memory usage        : 4651 MiB
            Time                    : 12906 ms
            Is Running              : 0
        Process ID                  : 6720
            GPU Utilization         : 5 %
            Memory Utilization      : 1 %
            Max memory usage        : 0 MiB
            Time                    : 13079 ms
            Is Running              : 0

