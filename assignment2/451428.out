
########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2958.826115131378	speed (wps):7260.6465248638715
step: 51	loss: 12545.543763637543	speed (wps):7703.653350103679
step: 92	loss: 22000.9645819664	speed (wps):7778.880164489403
step: 133	loss: 31111.8616437912	speed (wps):7807.1866570377215
step: 174	loss: 39837.38992214203	speed (wps):7822.135641528219
step: 215	loss: 48546.85047626495	speed (wps):7832.333691096138
step: 256	loss: 57126.55667543411	speed (wps):7839.344455973564
step: 297	loss: 65562.70743608475	speed (wps):7844.308285504306
step: 338	loss: 73890.87186336517	speed (wps):7848.106731425424
step: 379	loss: 82191.48367404938	speed (wps):7851.256079910784
epoch: 0	train ppl: 464.68295594991014	val ppl: 408.46511509732153	best val: 408.46511509732153	time (s) spent in epoch: 120.38640332221985

EPOCH 1 ------------------
step: 10	loss: 2217.9950404167175	speed (wps):7676.461655295193
step: 51	loss: 10309.351544380188	speed (wps):7833.940403833488
step: 92	loss: 18345.48185110092	speed (wps):7854.260899102499
step: 133	loss: 26302.423179149628	speed (wps):7862.919247236648
step: 174	loss: 34082.60732412338	speed (wps):7866.7086347669365
step: 215	loss: 41975.28057575226	speed (wps):7869.138584895892
step: 256	loss: 49873.90473127365	speed (wps):7870.941940230503
step: 297	loss: 57706.773726940155	speed (wps):7871.638340655277
step: 338	loss: 65485.97971200943	speed (wps):7872.366670142634
step: 379	loss: 73292.58750915527	speed (wps):7872.985706666749
epoch: 1	train ppl: 244.97262164037357	val ppl: 305.6105749682598	best val: 305.6105749682598	time (s) spent in epoch: 120.04547691345215

EPOCH 2 ------------------
step: 10	loss: 2108.8024711608887	speed (wps):7673.454497719221
step: 51	loss: 9801.854848861694	speed (wps):7830.660529584885
step: 92	loss: 17457.759730815887	speed (wps):7847.925137384006
step: 133	loss: 25065.150368213654	speed (wps):7857.613093584697
step: 174	loss: 32508.30642223358	speed (wps):7862.876124211209
step: 215	loss: 40087.49551296234	speed (wps):7865.656211756552
step: 256	loss: 47699.94359254837	speed (wps):7868.766333773217
step: 297	loss: 55249.530420303345	speed (wps):7870.773745295198
step: 338	loss: 62770.32888889313	speed (wps):7872.935299724765
step: 379	loss: 70329.302546978	speed (wps):7874.7512294986755
epoch: 2	train ppl: 196.7934470955299	val ppl: 260.9642286273407	best val: 260.9642286273407	time (s) spent in epoch: 119.98612856864929

EPOCH 3 ------------------
step: 10	loss: 2043.7929558753967	speed (wps):7691.689144498754
step: 51	loss: 9502.701573371887	speed (wps):7856.082048492816
step: 92	loss: 16943.4614944458	speed (wps):7888.284255982119
step: 133	loss: 24344.536278247833	speed (wps):7899.00495682114
step: 174	loss: 31564.019055366516	speed (wps):7917.240618684277
step: 215	loss: 38936.555552482605	speed (wps):7930.109534476152
step: 256	loss: 46343.243181705475	speed (wps):7938.786050693004
step: 297	loss: 53695.721876621246	speed (wps):7945.138871291163
step: 338	loss: 61022.19728946686	speed (wps):7949.55361769449
step: 379	loss: 68396.2766456604	speed (wps):7953.660930716957
epoch: 3	train ppl: 170.36545874831157	val ppl: 234.61297476660948	best val: 234.61297476660948	time (s) spent in epoch: 118.79619359970093

EPOCH 4 ------------------
step: 10	loss: 1994.5971703529358	speed (wps):7775.0943629021085
step: 51	loss: 9264.748647212982	speed (wps):7931.318335754222
step: 92	loss: 16538.59525203705	speed (wps):7954.710828843797
step: 133	loss: 23772.841885089874	speed (wps):7963.378548672217
step: 174	loss: 30821.585841178894	speed (wps):7968.993646352788
step: 215	loss: 38026.18351459503	speed (wps):7973.466431418795
step: 256	loss: 45265.70838928223	speed (wps):7975.309580688328
step: 297	loss: 52449.99797105789	speed (wps):7975.219830224987
step: 338	loss: 59618.749697208405	speed (wps):7977.425723049527
step: 379	loss: 66834.02787208557	speed (wps):7978.506040768202
epoch: 4	train ppl: 151.6148747958939	val ppl: 217.16428510194942	best val: 217.16428510194942	time (s) spent in epoch: 118.44744944572449

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2937.3599529266357	speed (wps):5399.690157846955
step: 51	loss: 12521.364138126373	speed (wps):5636.197022892047
step: 92	loss: 21999.678671360016	speed (wps):5665.957321498665
step: 133	loss: 31196.2109875679	speed (wps):5677.859438255176
step: 174	loss: 40001.66709661484	speed (wps):5684.176006954844
step: 215	loss: 48776.70595407486	speed (wps):5688.473967802384
step: 256	loss: 57425.56979417801	speed (wps):5690.575658228443
step: 297	loss: 65930.82574129105	speed (wps):5692.901082264779
step: 338	loss: 74341.43698215485	speed (wps):5694.417749742745
step: 379	loss: 82709.85414981842	speed (wps):5692.802851807892
epoch: 0	train ppl: 483.49907020618105	val ppl: 381.05437553455425	best val: 381.05437553455425	time (s) spent in epoch: 165.96895337104797

EPOCH 1 ------------------
step: 10	loss: 2237.240524291992	speed (wps):5600.97652468171
step: 51	loss: 10395.751485824585	speed (wps):5678.944622631638
step: 92	loss: 18502.967581748962	speed (wps):5688.755683482623
step: 133	loss: 26533.646111488342	speed (wps):5694.074382099006
step: 174	loss: 34389.408955574036	speed (wps):5698.399182622102
step: 215	loss: 42357.387001514435	speed (wps):5700.876079071189
step: 256	loss: 50324.62382555008	speed (wps):5702.229297573667
step: 297	loss: 58226.60059452057	speed (wps):5704.844927948714
step: 338	loss: 66074.88895177841	speed (wps):5708.805074134781
step: 379	loss: 73947.1353673935	speed (wps):5714.948995512423
epoch: 1	train ppl: 257.2066291462286	val ppl: 287.48036670831357	best val: 287.48036670831357	time (s) spent in epoch: 165.02155828475952

EPOCH 2 ------------------
step: 10	loss: 2129.766936302185	speed (wps):5681.183353353065
step: 51	loss: 9883.399889469147	speed (wps):5769.120371521162
step: 92	loss: 17616.52596473694	speed (wps):5780.727784512438
step: 133	loss: 25300.98360300064	speed (wps):5785.826792644724
step: 174	loss: 32827.81959295273	speed (wps):5788.039696986797
step: 215	loss: 40492.04913139343	speed (wps):5789.673657157222
step: 256	loss: 48179.54712629318	speed (wps):5790.764972446981
step: 297	loss: 55812.9191160202	speed (wps):5792.3509741123735
step: 338	loss: 63414.204506874084	speed (wps):5793.371007718827
step: 379	loss: 71055.08467912674	speed (wps):5794.092812183028
epoch: 2	train ppl: 208.03388524523265	val ppl: 253.43203026163172	best val: 253.43203026163172	time (s) spent in epoch: 162.9738359451294

EPOCH 3 ------------------
step: 10	loss: 2072.8223752975464	speed (wps):5686.143313545912
step: 51	loss: 9618.684267997742	speed (wps):5774.1426393412
step: 92	loss: 17150.335404872894	speed (wps):5785.18036192139
step: 133	loss: 24637.27246761322	speed (wps):5790.565682909777
step: 174	loss: 31956.29762649536	speed (wps):5792.6856465900555
step: 215	loss: 39428.25966358185	speed (wps):5794.360000425204
step: 256	loss: 46939.80506658554	speed (wps):5795.570746425184
step: 297	loss: 54410.79613685608	speed (wps):5796.3909525769595
step: 338	loss: 61846.74778699875	speed (wps):5796.952943240706
step: 379	loss: 69330.17674207687	speed (wps):5797.112781929786
epoch: 3	train ppl: 183.01537905775425	val ppl: 233.4059514729335	best val: 233.4059514729335	time (s) spent in epoch: 162.8837172985077

EPOCH 4 ------------------
step: 10	loss: 2025.972638130188	speed (wps):5682.39941154626
step: 51	loss: 9424.352867603302	speed (wps):5776.151687448665
step: 92	loss: 16822.173154354095	speed (wps):5787.006655110502
step: 133	loss: 24181.072340011597	speed (wps):5790.616606729306
step: 174	loss: 31360.781359672546	speed (wps):5787.732994654428
step: 215	loss: 38697.238743305206	speed (wps):5787.603406631314
step: 256	loss: 46083.63735675812	speed (wps):5789.716195451695
step: 297	loss: 53423.67743253708	speed (wps):5791.162797093463
step: 338	loss: 60737.690410614014	speed (wps):5792.104373335142
step: 379	loss: 68102.61924028397	speed (wps):5793.03696924309
epoch: 4	train ppl: 166.94903489906312	val ppl: 219.47448021749818	best val: 219.47448021749818	time (s) spent in epoch: 162.99313473701477

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:05:00.0.
All done.
