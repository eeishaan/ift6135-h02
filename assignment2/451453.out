
########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3463.273057937622	speed (wps):5220.653027360582
step: 92	loss: 24367.101714611053	speed (wps):5591.347829951516
step: 174	loss: 43770.96516609192	speed (wps):5615.705775242657
step: 256	loss: 62912.77614116669	speed (wps):5620.295045172605
step: 338	loss: 81712.59183883667	speed (wps):5621.854881953757
step: 420	loss: 100295.35669088364	speed (wps):5622.623628283796
step: 502	loss: 118747.81946182251	speed (wps):5623.270195365106
step: 584	loss: 136867.75048971176	speed (wps):5623.666683016541
step: 666	loss: 155074.7184228897	speed (wps):5623.779272496339
step: 748	loss: 173157.1926355362	speed (wps):5624.039833339624
epoch: 0	train ppl: 717.5282025156949	val ppl: 429.66859684040486	best val: 429.66859684040486	time (s) spent in epoch: 168.30464053153992

EPOCH 1 ------------------
step: 10	loss: 2457.137246131897	speed (wps):5427.382935752084
step: 92	loss: 20260.43789625168	speed (wps):5600.6799579968265
step: 174	loss: 37913.4331536293	speed (wps):5611.901575697923
step: 256	loss: 55705.16503095627	speed (wps):5615.638202495411
step: 338	loss: 73332.06219434738	speed (wps):5617.9439945756785
step: 420	loss: 90840.80608606339	speed (wps):5619.583960772888
step: 502	loss: 108347.82680749893	speed (wps):5620.549919124867
step: 584	loss: 125590.32203197479	speed (wps):5621.2317280174975
step: 666	loss: 142961.04170322418	speed (wps):5621.803609512161
step: 748	loss: 160225.48359155655	speed (wps):5622.104289577474
epoch: 1	train ppl: 448.6077005428445	val ppl: 340.1726931308513	best val: 340.1726931308513	time (s) spent in epoch: 168.35572695732117

EPOCH 2 ------------------
step: 10	loss: 2383.9201831817627	speed (wps):5432.101319113432
step: 92	loss: 19553.10823917389	speed (wps):5603.037980099332
step: 174	loss: 36580.63901424408	speed (wps):5614.028432176579
step: 256	loss: 53797.90040016174	speed (wps):5617.816454443358
step: 338	loss: 70872.30349302292	speed (wps):5619.601215409962
step: 420	loss: 87842.66318321228	speed (wps):5620.45607218583
step: 502	loss: 104833.48130226135	speed (wps):5621.154187083994
step: 584	loss: 121566.78429841995	speed (wps):5621.655019473145
step: 666	loss: 138483.2176709175	speed (wps):5621.855132667001
step: 748	loss: 155290.44921159744	speed (wps):5622.223069866636
epoch: 2	train ppl: 373.1725558827541	val ppl: 294.621932175753	best val: 294.621932175753	time (s) spent in epoch: 168.35649180412292

EPOCH 3 ------------------
step: 10	loss: 2335.039174556732	speed (wps):5427.755771802402
step: 92	loss: 19112.34382867813	speed (wps):5600.185016878991
step: 174	loss: 35753.8639998436	speed (wps):5611.9201174560585
step: 256	loss: 52608.6355137825	speed (wps):5616.176683062077
step: 338	loss: 69341.40984296799	speed (wps):5618.15029595975
step: 420	loss: 85966.04900360107	speed (wps):5619.337426928339
step: 502	loss: 102620.40555000305	speed (wps):5620.138653198153
step: 584	loss: 119031.90111398697	speed (wps):5620.660797783367
step: 666	loss: 135619.9169230461	speed (wps):5621.044811474917
step: 748	loss: 152118.78836870193	speed (wps):5621.563373608672
epoch: 3	train ppl: 331.3680350405292	val ppl: 267.48410334487147	best val: 267.48410334487147	time (s) spent in epoch: 168.3723075389862

EPOCH 4 ------------------
step: 10	loss: 2298.10763835907	speed (wps):5428.91907580099
step: 92	loss: 18808.563845157623	speed (wps):5600.578794511068
step: 174	loss: 35172.94955253601	speed (wps):5610.872968254888
step: 256	loss: 51758.42318058014	speed (wps):5614.765536279808
step: 338	loss: 68236.07458114624	speed (wps):5616.533191486916
step: 420	loss: 84613.7428021431	speed (wps):5617.943225361852
step: 502	loss: 101016.37734889984	speed (wps):5618.711320039149
step: 584	loss: 117166.90131902695	speed (wps):5619.124981586485
step: 666	loss: 133502.0935344696	speed (wps):5619.636951779453
step: 748	loss: 149761.51784420013	speed (wps):5619.776896349414
epoch: 4	train ppl: 303.27463053065225	val ppl: 251.77472950309837	best val: 251.77472950309837	time (s) spent in epoch: 168.43364429473877

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3634.1986322402954	speed (wps):4308.602624409299
step: 92	loss: 24394.01593208313	speed (wps):4495.900772928192
step: 174	loss: 43698.07664871216	speed (wps):4508.691269654606
step: 256	loss: 62787.86902666092	speed (wps):4513.278145429223
step: 338	loss: 81515.23138046265	speed (wps):4516.079108810323
step: 420	loss: 100019.1466140747	speed (wps):4517.251044996402
step: 502	loss: 118414.84511852264	speed (wps):4518.301878789067
step: 584	loss: 136475.19055366516	speed (wps):4519.0074223145275
step: 666	loss: 154606.69312477112	speed (wps):4519.497808757894
step: 748	loss: 172587.64533758163	speed (wps):4519.898702676016
epoch: 0	train ppl: 701.3531362747974	val ppl: 420.54353687354796	best val: 420.54353687354796	time (s) spent in epoch: 209.45864248275757

EPOCH 1 ------------------
step: 10	loss: 2454.4205284118652	speed (wps):4405.061522654039
step: 92	loss: 20177.489519119263	speed (wps):4508.531086433621
step: 174	loss: 37730.06649017334	speed (wps):4514.850532918698
step: 256	loss: 55430.86624622345	speed (wps):4517.280889260216
step: 338	loss: 72969.73762989044	speed (wps):4518.780414289479
step: 420	loss: 90372.48540401459	speed (wps):4519.226524807355
step: 502	loss: 107791.10794067383	speed (wps):4519.632301224336
step: 584	loss: 124937.08712100983	speed (wps):4520.021993352266
step: 666	loss: 142219.30474996567	speed (wps):4520.206201978768
step: 748	loss: 159402.5905752182	speed (wps):4520.429313407523
epoch: 1	train ppl: 435.01987135205695	val ppl: 326.5676451339399	best val: 326.5676451339399	time (s) spent in epoch: 209.44513654708862

EPOCH 2 ------------------
step: 10	loss: 2372.7137517929077	speed (wps):4400.2001512834695
step: 92	loss: 19470.85325241089	speed (wps):4508.440406255358
step: 174	loss: 36419.077990055084	speed (wps):4514.826878249559
step: 256	loss: 53547.000098228455	speed (wps):4517.459163760215
step: 338	loss: 70542.50572443008	speed (wps):4518.678530845115
step: 420	loss: 87422.30149269104	speed (wps):4519.437946131364
step: 502	loss: 104344.86823320389	speed (wps):4519.836062209068
step: 584	loss: 121014.25084114075	speed (wps):4520.141534610718
step: 666	loss: 137847.76218891144	speed (wps):4520.406900507199
step: 748	loss: 154574.37314271927	speed (wps):4520.696323770297
epoch: 2	train ppl: 363.3276822975912	val ppl: 287.4272502314786	best val: 287.4272502314786	time (s) spent in epoch: 209.4377098083496

EPOCH 3 ------------------
step: 10	loss: 2328.492178916931	speed (wps):4401.228171158052
step: 92	loss: 19036.82095527649	speed (wps):4506.635285449689
step: 174	loss: 35604.25490140915	speed (wps):4514.424167027079
step: 256	loss: 52366.96396112442	speed (wps):4516.446902969292
step: 338	loss: 69054.37164783478	speed (wps):4517.619041014657
step: 420	loss: 85623.88044118881	speed (wps):4518.578246046419
step: 502	loss: 102215.76124191284	speed (wps):4519.290360534049
step: 584	loss: 118543.32421064377	speed (wps):4519.663370344846
step: 666	loss: 135057.21407413483	speed (wps):4519.868218880528
step: 748	loss: 151494.73820447922	speed (wps):4520.094150369399
epoch: 3	train ppl: 323.6559721451391	val ppl: 263.8335939797986	best val: 263.8335939797986	time (s) spent in epoch: 209.46382665634155

EPOCH 4 ------------------
step: 10	loss: 2291.5565371513367	speed (wps):4396.4485655298
step: 92	loss: 18731.111888885498	speed (wps):4506.2866964250115
step: 174	loss: 35023.73655796051	speed (wps):4513.242579862258
step: 256	loss: 51522.42081642151	speed (wps):4516.102015264227
step: 338	loss: 67934.1208744049	speed (wps):4517.501114224678
step: 420	loss: 84235.1128578186	speed (wps):4518.399990378772
step: 502	loss: 100571.75669193268	speed (wps):4518.853399741286
step: 584	loss: 116664.85745191574	speed (wps):4519.249740744546
step: 666	loss: 132917.85240888596	speed (wps):4519.52131615698
step: 748	loss: 149114.4174861908	speed (wps):4519.783781981774
epoch: 4	train ppl: 295.7050276668065	val ppl: 245.0691381878669	best val: 245.0691381878669	time (s) spent in epoch: 209.47304129600525

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3394.1134643554688	speed (wps):3858.713099820438
step: 92	loss: 24211.851365566254	speed (wps):4026.253824390301
step: 174	loss: 43739.37238693237	speed (wps):4037.418804074302
step: 256	loss: 63016.41521215439	speed (wps):4041.517359527383
step: 338	loss: 81919.91147041321	speed (wps):4043.8139043535616
step: 420	loss: 100594.71357584	speed (wps):4045.0900212204915
step: 502	loss: 119149.40316915512	speed (wps):4045.7763236843643
step: 584	loss: 137388.7680888176	speed (wps):4046.070161638701
step: 666	loss: 155694.74851846695	speed (wps):4046.2318727170314
step: 748	loss: 173894.3369102478	speed (wps):4046.639836469973
epoch: 0	train ppl: 738.7786692418096	val ppl: 444.10564329643165	best val: 444.10564329643165	time (s) spent in epoch: 233.78300285339355

EPOCH 1 ------------------
step: 10	loss: 2479.42777633667	speed (wps):3950.6000751988113
step: 92	loss: 20416.597895622253	speed (wps):4037.746354875962
step: 174	loss: 38198.91819238663	speed (wps):4043.513381497863
step: 256	loss: 56127.28912830353	speed (wps):4045.7113227533155
step: 338	loss: 73898.61735105515	speed (wps):4046.694292232128
step: 420	loss: 91564.85112905502	speed (wps):4047.2538649359917
step: 502	loss: 109204.08495664597	speed (wps):4047.5348512523215
step: 584	loss: 126591.19784355164	speed (wps):4047.808825341501
step: 666	loss: 144120.00194072723	speed (wps):4048.081627842791
step: 748	loss: 161571.1178946495	speed (wps):4048.291942735927
epoch: 1	train ppl: 472.3126069223556	val ppl: 349.0114637880574	best val: 349.0114637880574	time (s) spent in epoch: 233.70689249038696

EPOCH 2 ------------------
step: 10	loss: 2399.821081161499	speed (wps):3948.5361490493765
step: 92	loss: 19730.46757221222	speed (wps):4036.35722632365
step: 174	loss: 36919.35698032379	speed (wps):4042.2769740884037
step: 256	loss: 54287.94202327728	speed (wps):4044.4220114134996
step: 338	loss: 71538.21466207504	speed (wps):4045.455067242489
step: 420	loss: 88685.42511224747	speed (wps):4046.1171890145847
step: 502	loss: 105844.86943721771	speed (wps):4046.4832434131235
step: 584	loss: 122763.56070518494	speed (wps):4046.8711237232396
step: 666	loss: 139842.35473632812	speed (wps):4047.1548373628575
step: 748	loss: 156839.72190380096	speed (wps):4047.401886865895
epoch: 2	train ppl: 395.97794281099607	val ppl: 304.299714250462	best val: 304.299714250462	time (s) spent in epoch: 233.7560999393463

EPOCH 3 ------------------
step: 10	loss: 2356.2915802001953	speed (wps):3953.167519654785
step: 92	loss: 19315.09461402893	speed (wps):4037.847115719211
step: 174	loss: 36147.979979515076	speed (wps):4043.2563391161807
step: 256	loss: 53182.07407236099	speed (wps):4045.175155110981
step: 338	loss: 70095.21412849426	speed (wps):4046.005474063767
step: 420	loss: 86912.93749809265	speed (wps):4046.616006761769
step: 502	loss: 103757.96064138412	speed (wps):4046.7876846354093
step: 584	loss: 120350.61071872711	speed (wps):4046.9579008905466
step: 666	loss: 137118.18269014359	speed (wps):4047.047200317698
step: 748	loss: 153816.33932113647	speed (wps):4047.2541735112577
epoch: 3	train ppl: 353.55529799592233	val ppl: 279.66687738791506	best val: 279.66687738791506	time (s) spent in epoch: 233.76904439926147

EPOCH 4 ------------------
step: 10	loss: 2324.114842414856	speed (wps):3946.3426694684363
step: 92	loss: 19015.72898387909	speed (wps):4035.6019763238746
step: 174	loss: 35583.21455001831	speed (wps):4040.929251987677
step: 256	loss: 52361.22948408127	speed (wps):4043.092346250241
step: 338	loss: 69018.79855394363	speed (wps):4044.2592445264213
step: 420	loss: 85588.95349025726	speed (wps):4044.884407021223
step: 502	loss: 102170.89175701141	speed (wps):4045.341383752427
step: 584	loss: 118525.46537399292	speed (wps):4045.7440023660843
step: 666	loss: 135047.7170944214	speed (wps):4045.9888818133263
step: 748	loss: 151519.96948957443	speed (wps):4046.3291664579447
epoch: 4	train ppl: 324.0849146553786	val ppl: 261.2354810567961	best val: 261.2354810567961	time (s) spent in epoch: 233.81909704208374

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3404.588236808777	speed (wps):3087.9803856349204
step: 92	loss: 24107.516884803772	speed (wps):3183.803311965442
step: 174	loss: 43444.39378976822	speed (wps):3190.1132129409484
step: 256	loss: 62597.009534835815	speed (wps):3192.2970359636734
step: 338	loss: 81396.4646267891	speed (wps):3193.430050625252
step: 420	loss: 99986.54927253723	speed (wps):3194.1593686040696
step: 502	loss: 118471.42545223236	speed (wps):3194.6096645975213
step: 584	loss: 136628.08270454407	speed (wps):3194.924252612939
step: 666	loss: 154869.90186929703	speed (wps):3195.178529916048
step: 748	loss: 172973.78513336182	speed (wps):3195.332046652212
epoch: 0	train ppl: 713.9391104491444	val ppl: 425.0734213288718	best val: 425.0734213288718	time (s) spent in epoch: 296.17237281799316

EPOCH 1 ------------------
step: 10	loss: 2467.132968902588	speed (wps):3137.2668651127933
step: 92	loss: 20326.169273853302	speed (wps):3189.2362274891107
step: 174	loss: 38015.18933773041	speed (wps):3192.2279714883034
step: 256	loss: 55866.54903173447	speed (wps):3193.6666743028295
step: 338	loss: 73547.25273370743	speed (wps):3194.3936957148258
step: 420	loss: 91130.67043066025	speed (wps):3194.7355254951804
step: 502	loss: 108712.5417470932	speed (wps):3194.8860136097287
step: 584	loss: 126031.65930271149	speed (wps):3194.884980864908
step: 666	loss: 143497.5604915619	speed (wps):3194.9917086262403
step: 748	loss: 160864.6019077301	speed (wps):3195.070717441744
epoch: 1	train ppl: 459.8868202558543	val ppl: 342.88212875847853	best val: 342.88212875847853	time (s) spent in epoch: 296.2093827724457

EPOCH 2 ------------------
step: 10	loss: 2396.586410999298	speed (wps):3137.7461671718647
step: 92	loss: 19668.28388929367	speed (wps):3188.3780199713856
step: 174	loss: 36811.856372356415	speed (wps):3191.368373777453
step: 256	loss: 54128.30998182297	speed (wps):3192.6379381123606
step: 338	loss: 71292.94516324997	speed (wps):3193.394093740295
step: 420	loss: 88364.4355392456	speed (wps):3193.688486356707
step: 502	loss: 105465.00278234482	speed (wps):3193.9560215717524
step: 584	loss: 122314.41816568375	speed (wps):3193.97254210879
step: 666	loss: 139320.1754617691	speed (wps):3194.0604303783502
step: 748	loss: 156253.3765411377	speed (wps):3194.2045171252958
epoch: 2	train ppl: 387.2312908765265	val ppl: 299.1885631725845	best val: 299.1885631725845	time (s) spent in epoch: 296.29096961021423

EPOCH 3 ------------------
step: 10	loss: 2352.103600502014	speed (wps):3137.4312515049996
step: 92	loss: 19252.619502544403	speed (wps):3187.8851645171217
step: 174	loss: 36023.31893205643	speed (wps):3191.114393751638
step: 256	loss: 52989.78379011154	speed (wps):3192.460884801118
step: 338	loss: 69839.92272138596	speed (wps):3193.3320948299793
step: 420	loss: 86586.69331550598	speed (wps):3193.5528027189257
step: 502	loss: 103356.84106349945	speed (wps):3193.718367725218
step: 584	loss: 119900.12669324875	speed (wps):3193.8695028504994
step: 666	loss: 136600.35014390945	speed (wps):3194.0345129420575
step: 748	loss: 153233.3372759819	speed (wps):3194.150207988559
epoch: 3	train ppl: 345.7754366105285	val ppl: 279.08599585263664	best val: 279.08599585263664	time (s) spent in epoch: 296.2956907749176

EPOCH 4 ------------------
step: 10	loss: 2313.5142993927	speed (wps):3136.847310281226
step: 92	loss: 18923.460566997528	speed (wps):3188.0926040315876
step: 174	loss: 35414.33638572693	speed (wps):3191.269351197987
step: 256	loss: 52115.69757938385	speed (wps):3192.2713332286507
step: 338	loss: 68708.78965616226	speed (wps):3192.9760991476696
step: 420	loss: 85208.9854645729	speed (wps):3193.225342608062
step: 502	loss: 101739.2049908638	speed (wps):3193.5259557971826
step: 584	loss: 118034.47501420975	speed (wps):3193.7924931602006
step: 666	loss: 134477.75448322296	speed (wps):3193.9154055900267
step: 748	loss: 150863.5009598732	speed (wps):3194.0473043582656
epoch: 4	train ppl: 316.31683500735517	val ppl: 259.45484317040246	best val: 259.45484317040246	time (s) spent in epoch: 296.3000910282135

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3496.0690021514893	speed (wps):8134.014021359848
step: 51	loss: 14279.210479259491	speed (wps):8459.14454190355
step: 92	loss: 24253.735218048096	speed (wps):8500.118108370621
step: 133	loss: 34003.97665262222	speed (wps):8514.635901403028
step: 174	loss: 43528.811559677124	speed (wps):8525.292580563277
step: 215	loss: 53084.86520528793	speed (wps):8529.698242111905
step: 256	loss: 62545.91637611389	speed (wps):8532.321323026345
step: 297	loss: 71901.33662462234	speed (wps):8536.147519514872
step: 338	loss: 81181.92934274673	speed (wps):8537.24368812935
step: 379	loss: 90438.9033293724	speed (wps):8538.004726811483
epoch: 0	train ppl: 868.077886142747	val ppl: 481.50394468852147	best val: 481.50394468852147	time (s) spent in epoch: 111.08852767944336

EPOCH 1 ------------------
step: 10	loss: 2477.1965670585632	speed (wps):8329.410366696624
step: 51	loss: 11584.435918331146	speed (wps):8497.960437785241
step: 92	loss: 20664.356186389923	speed (wps):8524.020527911163
step: 133	loss: 29684.625055789948	speed (wps):8529.041325059523
step: 174	loss: 38548.946981430054	speed (wps):8534.637298858039
step: 215	loss: 47539.889245033264	speed (wps):8538.764999115965
step: 256	loss: 56508.190569877625	speed (wps):8540.259981353576
step: 297	loss: 65411.28284215927	speed (wps):8543.343081427014
step: 338	loss: 74274.37938928604	speed (wps):8543.71373579726
step: 379	loss: 83139.69150781631	speed (wps):8544.38895410224
epoch: 1	train ppl: 513.8858317031501	val ppl: 382.1668075009447	best val: 382.1668075009447	time (s) spent in epoch: 111.00050616264343

EPOCH 2 ------------------
step: 10	loss: 2389.4813752174377	speed (wps):8325.144987286849
step: 51	loss: 11164.405961036682	speed (wps):8499.710187045197
step: 92	loss: 19918.02795648575	speed (wps):8526.025994776564
step: 133	loss: 28641.336975097656	speed (wps):8530.030598793495
step: 174	loss: 37214.32264328003	speed (wps):8534.56566298769
step: 215	loss: 45923.59908103943	speed (wps):8536.446680490904
step: 256	loss: 54623.51045131683	speed (wps):8537.782252917452
step: 297	loss: 63268.85174751282	speed (wps):8540.303894099154
step: 338	loss: 71864.46700572968	speed (wps):8541.93359321267
step: 379	loss: 80499.95615720749	speed (wps):8541.507179944212
epoch: 2	train ppl: 422.4552008573275	val ppl: 325.85781300113433	best val: 325.85781300113433	time (s) spent in epoch: 111.04185509681702

EPOCH 3 ------------------
step: 10	loss: 2332.1202158927917	speed (wps):8330.25362672506
step: 51	loss: 10894.307956695557	speed (wps):8500.249376242735
step: 92	loss: 19446.388590335846	speed (wps):8523.043214198799
step: 133	loss: 27964.80022907257	speed (wps):8532.264026138977
step: 174	loss: 36323.85626792908	speed (wps):8536.419323520586
step: 215	loss: 44835.45523405075	speed (wps):8540.486925983701
step: 256	loss: 53360.00109195709	speed (wps):8543.642128017047
step: 297	loss: 61825.80226421356	speed (wps):8544.72052530346
step: 338	loss: 70256.83586835861	speed (wps):8545.016941646774
step: 379	loss: 78729.58292722702	speed (wps):8546.070309291206
epoch: 3	train ppl: 370.53762549915757	val ppl: 297.11084876252363	best val: 297.11084876252363	time (s) spent in epoch: 110.98318457603455

EPOCH 4 ------------------
step: 10	loss: 2293.9256501197815	speed (wps):8339.439827052143
step: 51	loss: 10715.42183637619	speed (wps):8505.954388298473
step: 92	loss: 19123.250002861023	speed (wps):8526.565554963854
step: 133	loss: 27492.66842842102	speed (wps):8533.96598266044
step: 174	loss: 35715.718150138855	speed (wps):8536.98028173636
step: 215	loss: 44101.788992881775	speed (wps):8540.376764832105
step: 256	loss: 52495.60295343399	speed (wps):8539.833999616634
step: 297	loss: 60832.28980541229	speed (wps):8541.1786681679
step: 338	loss: 69137.49973535538	speed (wps):8541.78159518253
step: 379	loss: 77485.3765130043	speed (wps):8543.089090971105
epoch: 4	train ppl: 337.76519354672405	val ppl: 276.18572578703754	best val: 276.18572578703754	time (s) spent in epoch: 111.02151274681091

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3365.9253692626953	speed (wps):6504.795881347812
step: 51	loss: 13961.999163627625	speed (wps):6695.0151676150135
step: 92	loss: 23851.40529870987	speed (wps):6718.64648829649
step: 133	loss: 33488.69593381882	speed (wps):6727.770450018174
step: 174	loss: 42889.14967298508	speed (wps):6732.708785480829
step: 215	loss: 52343.71154308319	speed (wps):6735.972775328381
step: 256	loss: 61712.842569351196	speed (wps):6738.151492897132
step: 297	loss: 71005.90371370316	speed (wps):6739.866063176643
step: 338	loss: 80210.70090532303	speed (wps):6740.826164314807
step: 379	loss: 89422.91410446167	speed (wps):6741.8577977261775
epoch: 0	train ppl: 806.7137523824136	val ppl: 475.5788842708282	best val: 475.5788842708282	time (s) spent in epoch: 140.63885831832886

EPOCH 1 ------------------
step: 10	loss: 2462.252917289734	speed (wps):6606.152290005875
step: 51	loss: 11519.668414592743	speed (wps):6719.696217111896
step: 92	loss: 20552.32191324234	speed (wps):6732.875703145139
step: 133	loss: 29506.153531074524	speed (wps):6738.016722631434
step: 174	loss: 38316.11515045166	speed (wps):6740.576880354204
step: 215	loss: 47246.54448032379	speed (wps):6742.067055534544
step: 256	loss: 56157.19824075699	speed (wps):6743.232626132218
step: 297	loss: 65008.13462972641	speed (wps):6744.142052200842
step: 338	loss: 73806.29399299622	speed (wps):6744.484505734623
step: 379	loss: 82635.36561250687	speed (wps):6744.695249891965
epoch: 1	train ppl: 494.38135168474787	val ppl: 363.87503629584216	best val: 363.87503629584216	time (s) spent in epoch: 140.58306288719177

EPOCH 2 ------------------
step: 10	loss: 2371.618432998657	speed (wps):6614.966380677279
step: 51	loss: 11098.359389305115	speed (wps):6720.583518881509
step: 92	loss: 19805.799703598022	speed (wps):6734.121031248114
step: 133	loss: 28456.40694141388	speed (wps):6739.320512895319
step: 174	loss: 36971.35625123978	speed (wps):6741.3041549059135
step: 215	loss: 45629.715836048126	speed (wps):6742.807147826221
step: 256	loss: 54285.87370157242	speed (wps):6743.243624007575
step: 297	loss: 62890.37302494049	speed (wps):6743.841883874099
step: 338	loss: 71437.04589128494	speed (wps):6744.04427665465
step: 379	loss: 80023.59670639038	speed (wps):6744.51373410819
epoch: 2	train ppl: 407.85321732958283	val ppl: 316.48736822417595	best val: 316.48736822417595	time (s) spent in epoch: 140.58960604667664

EPOCH 3 ------------------
step: 10	loss: 2319.8810172080994	speed (wps):6611.338954197381
step: 51	loss: 10837.410593032837	speed (wps):6717.022141379144
step: 92	loss: 19334.27093744278	speed (wps):6730.995896457631
step: 133	loss: 27795.23912191391	speed (wps):6735.267061498886
step: 174	loss: 36125.46377658844	speed (wps):6737.475668978268
step: 215	loss: 44595.364775657654	speed (wps):6739.642811081102
step: 256	loss: 53086.817519664764	speed (wps):6740.503324831237
step: 297	loss: 61517.06522464752	speed (wps):6741.609621454306
step: 338	loss: 69909.01562452316	speed (wps):6742.419428444304
step: 379	loss: 78339.82649326324	speed (wps):6743.010851511715
epoch: 3	train ppl: 359.62496672955604	val ppl: 288.7617242448664	best val: 288.7617242448664	time (s) spent in epoch: 140.6185643672943

EPOCH 4 ------------------
step: 10	loss: 2284.4640851020813	speed (wps):6613.401004223138
step: 51	loss: 10656.54356956482	speed (wps):6721.484741118586
step: 92	loss: 19017.581129074097	speed (wps):6731.505694740042
step: 133	loss: 27348.092238903046	speed (wps):6736.763039403464
step: 174	loss: 35540.66851615906	speed (wps):6737.6480078800505
step: 215	loss: 43872.55212068558	speed (wps):6739.027791161765
step: 256	loss: 52232.99487352371	speed (wps):6739.2479386840805
step: 297	loss: 60535.71225643158	speed (wps):6739.652276264104
step: 338	loss: 68794.55732345581	speed (wps):6740.795091935676
step: 379	loss: 77109.32097673416	speed (wps):6741.456726686809
epoch: 4	train ppl: 328.3093540910246	val ppl: 267.8579289701729	best val: 267.8579289701729	time (s) spent in epoch: 140.6512951850891

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3414.9553966522217	speed (wps):4827.90207586757
step: 51	loss: 14019.44667339325	speed (wps):4932.555389796213
step: 92	loss: 23949.673285484314	speed (wps):4945.5058792184755
step: 133	loss: 33654.21303987503	speed (wps):4950.690271709177
step: 174	loss: 43148.27476263046	speed (wps):4953.679797861002
step: 215	loss: 52684.281821250916	speed (wps):4955.351669796797
step: 256	loss: 62111.17221355438	speed (wps):4956.208244175144
step: 297	loss: 71455.16034126282	speed (wps):4957.118558504323
step: 338	loss: 80715.77910423279	speed (wps):4957.4249436461005
step: 379	loss: 89959.1011762619	speed (wps):4957.929655005312
epoch: 0	train ppl: 840.1183048710617	val ppl: 471.9641074147106	best val: 471.9641074147106	time (s) spent in epoch: 191.1285457611084

EPOCH 1 ------------------
step: 10	loss: 2474.5231080055237	speed (wps):4893.039584572206
step: 51	loss: 11583.910055160522	speed (wps):4946.105546156716
step: 92	loss: 20659.83190059662	speed (wps):4952.867435267705
step: 133	loss: 29673.111975193024	speed (wps):4955.435362403615
step: 174	loss: 38555.64815282822	speed (wps):4956.978653160077
step: 215	loss: 47553.93784761429	speed (wps):4957.772233083293
step: 256	loss: 56540.62687635422	speed (wps):4958.127899135446
step: 297	loss: 65459.194836616516	speed (wps):4958.1303484753125
step: 338	loss: 74331.17619991302	speed (wps):4958.284008226794
step: 379	loss: 83210.02863168716	speed (wps):4958.586007373336
epoch: 1	train ppl: 516.4848218906124	val ppl: 374.64780361176014	best val: 374.64780361176014	time (s) spent in epoch: 191.11891508102417

EPOCH 2 ------------------
step: 10	loss: 2395.2109789848328	speed (wps):4889.786881825335
step: 51	loss: 11187.225124835968	speed (wps):4946.384526205841
step: 92	loss: 19969.73009109497	speed (wps):4952.804238845724
step: 133	loss: 28693.069667816162	speed (wps):4955.184440421673
step: 174	loss: 37292.54512786865	speed (wps):4955.888435689901
step: 215	loss: 46018.51393699646	speed (wps):4956.726266165402
step: 256	loss: 54748.61887216568	speed (wps):4957.584990718535
step: 297	loss: 63422.01601743698	speed (wps):4957.359053507336
step: 338	loss: 72059.720890522	speed (wps):4957.454959928662
step: 379	loss: 80718.93490076065	speed (wps):4957.474902540023
epoch: 2	train ppl: 429.8130045508893	val ppl: 326.40449880899644	best val: 326.40449880899644	time (s) spent in epoch: 191.16324663162231

EPOCH 3 ------------------
step: 10	loss: 2340.9784960746765	speed (wps):4887.690777268803
step: 51	loss: 10941.288087368011	speed (wps):4942.46331682863
step: 92	loss: 19525.66692352295	speed (wps):4950.426210830414
step: 133	loss: 28068.64364385605	speed (wps):4952.02646017401
step: 174	loss: 36483.46439361572	speed (wps):4954.209417902728
step: 215	loss: 45034.34628486633	speed (wps):4955.675210047859
step: 256	loss: 53598.23159456253	speed (wps):4956.649994186463
step: 297	loss: 62112.69134044647	speed (wps):4957.2202394019005
step: 338	loss: 70589.3012547493	speed (wps):4957.490277752796
step: 379	loss: 79101.15015983582	speed (wps):4957.610964983841
epoch: 3	train ppl: 381.0503938262294	val ppl: 297.60923534854606	best val: 297.60923534854606	time (s) spent in epoch: 191.14874458312988

EPOCH 4 ------------------
step: 10	loss: 2305.2788138389587	speed (wps):4891.662040426541
step: 51	loss: 10773.708584308624	speed (wps):4944.394446130294
step: 92	loss: 19225.575358867645	speed (wps):4951.646494099098
step: 133	loss: 27639.580919742584	speed (wps):4953.975748527431
step: 174	loss: 35921.96192264557	speed (wps):4955.758701177312
step: 215	loss: 44341.05842113495	speed (wps):4956.203393724839
step: 256	loss: 52788.11415433884	speed (wps):4956.620293468186
step: 297	loss: 61176.825194358826	speed (wps):4957.055252513448
step: 338	loss: 69527.43561267853	speed (wps):4957.115257019267
step: 379	loss: 77922.16401338577	speed (wps):4957.255669440658
epoch: 4	train ppl: 349.1021274177054	val ppl: 279.5426692084382	best val: 279.5426692084382	time (s) spent in epoch: 191.1714551448822

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3729.4715070724487	speed (wps):5282.139653031945
step: 92	loss: 24054.5548081398	speed (wps):5597.070490153935
step: 174	loss: 42952.89579153061	speed (wps):5618.3501481361145
step: 256	loss: 61674.86770153046	speed (wps):5625.528905115534
step: 338	loss: 80045.88403224945	speed (wps):5629.512093467415
step: 420	loss: 98163.79013061523	speed (wps):5631.568012756053
step: 502	loss: 116159.13998603821	speed (wps):5633.088532756346
step: 584	loss: 133784.71539020538	speed (wps):5634.572852927041
step: 666	loss: 151494.9298644066	speed (wps):5635.481603676841
step: 748	loss: 169048.34829330444	speed (wps):5636.198558508118
epoch: 0	train ppl: 612.3089019941061	val ppl: 353.060709918806	best val: 353.060709918806	time (s) spent in epoch: 167.9311559200287

EPOCH 1 ------------------
step: 10	loss: 2395.4984188079834	speed (wps):5443.910307181532
step: 92	loss: 19659.16784286499	speed (wps):5617.596211007044
step: 174	loss: 36757.221426963806	speed (wps):5628.904413179996
step: 256	loss: 54005.30259370804	speed (wps):5632.448998323779
step: 338	loss: 71078.64162683487	speed (wps):5634.504675929569
step: 420	loss: 88015.99734067917	speed (wps):5635.757108823513
step: 502	loss: 104973.81308555603	speed (wps):5636.888564882928
step: 584	loss: 121649.18396472931	speed (wps):5637.445297569156
step: 666	loss: 138469.49983358383	speed (wps):5637.845007663483
step: 748	loss: 155176.41425848007	speed (wps):5638.24630287744
epoch: 1	train ppl: 370.5547374444483	val ppl: 285.7357259453933	best val: 285.7357259453933	time (s) spent in epoch: 167.87961649894714

EPOCH 2 ------------------
step: 10	loss: 2319.6355843544006	speed (wps):5446.338541770967
step: 92	loss: 18961.256976127625	speed (wps):5617.200470170639
step: 174	loss: 35448.32536458969	speed (wps):5628.352818736199
step: 256	loss: 52126.409606933594	speed (wps):5632.344258291503
step: 338	loss: 68676.15823507309	speed (wps):5634.722024501919
step: 420	loss: 85111.96825742722	speed (wps):5635.8373776565595
step: 502	loss: 101569.17808532715	speed (wps):5636.726691719632
step: 584	loss: 117767.97166824341	speed (wps):5637.42255007848
step: 666	loss: 134133.7465929985	speed (wps):5637.778928403995
step: 748	loss: 150404.30548667908	speed (wps):5638.109254235145
epoch: 2	train ppl: 310.10466436608544	val ppl: 249.19482025338786	best val: 249.19482025338786	time (s) spent in epoch: 167.89451789855957

EPOCH 3 ------------------
step: 10	loss: 2270.1754236221313	speed (wps):5444.689263703227
step: 92	loss: 18524.750373363495	speed (wps):5615.4859726639115
step: 174	loss: 34641.116886138916	speed (wps):5626.515567238902
step: 256	loss: 50958.50707530975	speed (wps):5630.128314475923
step: 338	loss: 67182.71866321564	speed (wps):5632.462753331903
step: 420	loss: 83277.29485750198	speed (wps):5633.830411695725
step: 502	loss: 99402.9054737091	speed (wps):5634.720051180904
step: 584	loss: 115288.64384412766	speed (wps):5635.232145821741
step: 666	loss: 131338.25665950775	speed (wps):5635.705451719103
step: 748	loss: 147309.61661577225	speed (wps):5636.02644554119
epoch: 3	train ppl: 276.1242706415008	val ppl: 227.49004924180673	best val: 227.49004924180673	time (s) spent in epoch: 167.95009660720825

EPOCH 4 ------------------
step: 10	loss: 2234.0200066566467	speed (wps):5448.299003963984
step: 92	loss: 18223.04987668991	speed (wps):5615.846099240347
step: 174	loss: 34060.23419857025	speed (wps):5626.841334354759
step: 256	loss: 50105.669531822205	speed (wps):5630.323795819721
step: 338	loss: 66076.52119874954	speed (wps):5630.274783766227
step: 420	loss: 81935.40202856064	speed (wps):5629.921603221381
step: 502	loss: 97819.8511338234	speed (wps):5631.306825242636
step: 584	loss: 113442.49509811401	speed (wps):5632.302101731255
step: 666	loss: 129250.18748998642	speed (wps):5633.080927158779
step: 748	loss: 144994.91806030273	speed (wps):5633.705428245957
epoch: 4	train ppl: 253.13530691318638	val ppl: 215.14551037049347	best val: 215.14551037049347	time (s) spent in epoch: 168.01325368881226

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3278.749041557312	speed (wps):3857.4479723508566
step: 92	loss: 23422.91855573654	speed (wps):4026.575631329447
step: 174	loss: 42374.91817712784	speed (wps):4037.4357179244043
step: 256	loss: 61210.98914384842	speed (wps):4040.9342057059407
step: 338	loss: 79698.29331636429	speed (wps):4042.846211782174
step: 420	loss: 97959.31934595108	speed (wps):4043.8395666540987
step: 502	loss: 116115.24492263794	speed (wps):4044.625440317599
step: 584	loss: 133947.1287035942	speed (wps):4045.103714245954
step: 666	loss: 151862.30531215668	speed (wps):4045.57321544301
step: 748	loss: 169622.24387168884	speed (wps):4045.952701578354
epoch: 0	train ppl: 628.8402616898876	val ppl: 374.2758406919904	best val: 374.2758406919904	time (s) spent in epoch: 233.82909297943115

EPOCH 1 ------------------
step: 10	loss: 2423.4910321235657	speed (wps):3949.381141026034
step: 92	loss: 19914.177901744843	speed (wps):4035.441350642061
step: 174	loss: 37228.11004161835	speed (wps):4041.6661475800524
step: 256	loss: 54702.23116159439	speed (wps):4043.597787144499
step: 338	loss: 72016.95400714874	speed (wps):4044.5865881872546
step: 420	loss: 89208.08074951172	speed (wps):4045.237065607226
step: 502	loss: 106389.59814310074	speed (wps):4045.8378728066095
step: 584	loss: 123305.11003255844	speed (wps):4046.1555223588834
step: 666	loss: 140375.71046829224	speed (wps):4046.3152153861615
step: 748	loss: 157339.56834077835	speed (wps):4046.53947010639
epoch: 1	train ppl: 402.35378285932876	val ppl: 301.0922233786627	best val: 301.0922233786627	time (s) spent in epoch: 233.80983066558838

EPOCH 2 ------------------
step: 10	loss: 2345.072114467621	speed (wps):3950.5786309165896
step: 92	loss: 19227.48403072357	speed (wps):4037.0784003541344
step: 174	loss: 35955.06726980209	speed (wps):4042.431896127734
step: 256	loss: 52875.21786689758	speed (wps):4044.3628580751306
step: 338	loss: 69676.90393686295	speed (wps):4045.216392789131
step: 420	loss: 86366.23119354248	speed (wps):4045.7215139729083
step: 502	loss: 103088.07903766632	speed (wps):4045.8280428323883
step: 584	loss: 119560.02022743225	speed (wps):4046.0833089446646
step: 666	loss: 136176.06142759323	speed (wps):4046.1991688420703
step: 748	loss: 152705.88007688522	speed (wps):4046.3216792841463
epoch: 2	train ppl: 338.45647180135194	val ppl: 271.17106589686836	best val: 271.17106589686836	time (s) spent in epoch: 233.83090496063232

EPOCH 3 ------------------
step: 10	loss: 2302.3460698127747	speed (wps):3946.856896127287
step: 92	loss: 18820.98862886429	speed (wps):4034.0822290745355
step: 174	loss: 35207.58415699005	speed (wps):4040.2656362815374
step: 256	loss: 51781.51404619217	speed (wps):4042.265686221654
step: 338	loss: 68255.67356109619	speed (wps):4043.1033398607956
step: 420	loss: 84618.68410348892	speed (wps):4043.845288478433
step: 502	loss: 101016.50931119919	speed (wps):4044.1628186268626
step: 584	loss: 117157.36817359924	speed (wps):4044.2374523099866
step: 666	loss: 133474.1471004486	speed (wps):4044.355518329687
step: 748	loss: 149707.565741539	speed (wps):4044.5383856317176
epoch: 3	train ppl: 302.5720662159675	val ppl: 245.85327616077177	best val: 245.85327616077177	time (s) spent in epoch: 233.91160821914673

EPOCH 4 ------------------
step: 10	loss: 2268.444776535034	speed (wps):3948.862031686221
step: 92	loss: 18518.39809179306	speed (wps):4035.6171859162937
step: 174	loss: 34633.437638282776	speed (wps):4040.9893585515765
step: 256	loss: 50946.3375043869	speed (wps):4043.0484502887057
step: 338	loss: 67164.69642162323	speed (wps):4044.1489606799732
step: 420	loss: 83273.32615852356	speed (wps):4044.7662170677722
step: 502	loss: 99421.01633548737	speed (wps):4045.1069896782305
step: 584	loss: 115321.78500652313	speed (wps):4045.443032534312
step: 666	loss: 131386.89455509186	speed (wps):4045.655204173314
step: 748	loss: 147398.45668315887	speed (wps):4045.8972125499045
epoch: 4	train ppl: 277.24458593452357	val ppl: 228.86336996792244	best val: 228.86336996792244	time (s) spent in epoch: 233.84644651412964

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3634.119758605957	speed (wps):3089.218003253871
step: 92	loss: 23800.174469947815	speed (wps):3184.272508995079
step: 174	loss: 42760.06110429764	speed (wps):3190.5737895234206
step: 256	loss: 61537.30133533478	speed (wps):3192.6450047806966
step: 338	loss: 79979.25603866577	speed (wps):3193.7315888149874
step: 420	loss: 98203.92432689667	speed (wps):3194.3240012417837
step: 502	loss: 116361.98755264282	speed (wps):3194.8571575277033
step: 584	loss: 134163.94072532654	speed (wps):3195.115451064896
step: 666	loss: 152048.91413927078	speed (wps):3195.3925015199725
step: 748	loss: 169766.68823242188	speed (wps):3195.567536054403
epoch: 0	train ppl: 631.5255987759484	val ppl: 369.17727523591515	best val: 369.17727523591515	time (s) spent in epoch: 296.15430521965027

EPOCH 1 ------------------
step: 10	loss: 2420.540313720703	speed (wps):3136.603018430655
step: 92	loss: 19890.832974910736	speed (wps):3188.5614139382074
step: 174	loss: 37185.59935569763	speed (wps):3191.9052938866585
step: 256	loss: 54618.34776639938	speed (wps):3193.130197495231
step: 338	loss: 71919.82196092606	speed (wps):3193.9163581629737
step: 420	loss: 89088.65985870361	speed (wps):3194.3770405965224
step: 502	loss: 106275.13050317764	speed (wps):3194.7187722122253
step: 584	loss: 123187.25760221481	speed (wps):3194.866290630256
step: 666	loss: 140247.46021986008	speed (wps):3195.038977974641
step: 748	loss: 157215.1558971405	speed (wps):3195.1244166638958
epoch: 1	train ppl: 400.62885054917854	val ppl: 299.7667136345547	best val: 299.7667136345547	time (s) spent in epoch: 296.19472646713257

EPOCH 2 ------------------
step: 10	loss: 2350.96807718277	speed (wps):3144.744310195989
step: 92	loss: 19236.298751831055	speed (wps):3195.653575232523
step: 174	loss: 35958.66638422012	speed (wps):3198.42610482627
step: 256	loss: 52872.557191848755	speed (wps):3199.7802325515845
step: 338	loss: 69658.58726263046	speed (wps):3200.34803597789
step: 420	loss: 86331.1605143547	speed (wps):3200.6875046041177
step: 502	loss: 103038.81367444992	speed (wps):3200.966814309673
step: 584	loss: 119500.09438991547	speed (wps):3201.032691777859
step: 666	loss: 136127.0809340477	speed (wps):3201.1381854954184
step: 748	loss: 152650.55109024048	speed (wps):3201.230260768746
epoch: 2	train ppl: 337.8797451539746	val ppl: 263.90655780228957	best val: 263.90655780228957	time (s) spent in epoch: 295.6252427101135

EPOCH 3 ------------------
step: 10	loss: 2301.698124408722	speed (wps):3144.9115865680405
step: 92	loss: 18814.253878593445	speed (wps):3196.4020662538383
step: 174	loss: 35183.593871593475	speed (wps):3199.1820141936932
step: 256	loss: 51739.877145290375	speed (wps):3200.1230233090805
step: 338	loss: 68194.40486907959	speed (wps):3200.6050491686137
step: 420	loss: 84539.08608675003	speed (wps):3200.933333373041
step: 502	loss: 100913.6534690857	speed (wps):3201.206654091031
step: 584	loss: 117055.10842323303	speed (wps):3201.369826768232
step: 666	loss: 133347.80370473862	speed (wps):3201.4645129986716
step: 748	loss: 149575.17325878143	speed (wps):3201.4991791396583
epoch: 3	train ppl: 300.9278394869019	val ppl: 241.33732000953026	best val: 241.33732000953026	time (s) spent in epoch: 295.6031811237335

EPOCH 4 ------------------
step: 10	loss: 2262.0572471618652	speed (wps):3146.7208006292244
step: 92	loss: 18476.75453186035	speed (wps):3195.07446188162
step: 174	loss: 34554.438400268555	speed (wps):3198.416635043041
step: 256	loss: 50839.03955459595	speed (wps):3199.5742931686445
step: 338	loss: 67035.7821559906	speed (wps):3200.1977759028837
step: 420	loss: 83128.07411909103	speed (wps):3200.6536539142116
step: 502	loss: 99251.06753587723	speed (wps):3200.797504885917
step: 584	loss: 115127.52355098724	speed (wps):3200.911665822112
step: 666	loss: 131166.654484272	speed (wps):3201.0161376380497
step: 748	loss: 147143.81085157394	speed (wps):3201.18385862077
epoch: 4	train ppl: 274.8058787237347	val ppl: 233.0318311948673	best val: 233.0318311948673	time (s) spent in epoch: 295.63201570510864

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3201.615433692932	speed (wps):5321.30390407739
step: 92	loss: 23067.183527946472	speed (wps):5610.604750447682
step: 174	loss: 41712.61041164398	speed (wps):5629.891371401811
step: 256	loss: 60197.31200456619	speed (wps):5636.798331824181
step: 338	loss: 78314.71423625946	speed (wps):5640.211473432373
step: 420	loss: 96190.75007915497	speed (wps):5642.559605085181
step: 502	loss: 113966.64818286896	speed (wps):5644.355647676109
step: 584	loss: 131372.69015789032	speed (wps):5645.451925095159
step: 666	loss: 148855.80861330032	speed (wps):5646.1785632816745
step: 748	loss: 166169.08893585205	speed (wps):5646.65292434853
epoch: 0	train ppl: 550.7026898964001	val ppl: 320.7139197648873	best val: 320.7139197648873	time (s) spent in epoch: 167.61359119415283

EPOCH 1 ------------------
step: 10	loss: 2371.323833465576	speed (wps):5461.256566735641
step: 92	loss: 19423.961424827576	speed (wps):5628.926198487597
step: 174	loss: 36300.018038749695	speed (wps):5639.363043416403
step: 256	loss: 53314.18725967407	speed (wps):5643.2893847319365
step: 338	loss: 70185.90827703476	speed (wps):5645.163625849199
step: 420	loss: 86915.98938703537	speed (wps):5646.301487566671
step: 502	loss: 103673.66794109344	speed (wps):5646.810208685318
step: 584	loss: 120142.87792682648	speed (wps):5647.2879740148655
step: 666	loss: 136751.32869958878	speed (wps):5647.676508790829
step: 748	loss: 153250.980052948	speed (wps):5647.856845719846
epoch: 1	train ppl: 344.6051406444245	val ppl: 261.26298301752325	best val: 261.26298301752325	time (s) spent in epoch: 167.58577013015747

EPOCH 2 ------------------
step: 10	loss: 2289.459059238434	speed (wps):5463.242225420771
step: 92	loss: 18722.50705242157	speed (wps):5627.588350069236
step: 174	loss: 35000.95142364502	speed (wps):5637.995932716112
step: 256	loss: 51459.19872522354	speed (wps):5642.339858122469
step: 338	loss: 67800.89863300323	speed (wps):5644.132591546016
step: 420	loss: 84036.3080906868	speed (wps):5645.095702683375
step: 502	loss: 100298.29941511154	speed (wps):5645.737172473217
step: 584	loss: 116290.90154647827	speed (wps):5646.204342295134
step: 666	loss: 132450.83671331406	speed (wps):5646.760655521397
step: 748	loss: 148514.17447566986	speed (wps):5646.938200398062
epoch: 2	train ppl: 288.7205925230764	val ppl: 233.68645410997888	best val: 233.68645410997888	time (s) spent in epoch: 167.61390852928162

EPOCH 3 ------------------
step: 10	loss: 2244.1301012039185	speed (wps):5461.343722963488
step: 92	loss: 18287.235708236694	speed (wps):5625.782890056736
step: 174	loss: 34188.296875953674	speed (wps):5636.43424356925
step: 256	loss: 50290.432155132294	speed (wps):5640.753815621194
step: 338	loss: 66304.64067220688	speed (wps):5643.081437255654
step: 420	loss: 82202.09191083908	speed (wps):5644.287546573976
step: 502	loss: 98129.00725841522	speed (wps):5645.288709669862
step: 584	loss: 113806.32917642593	speed (wps):5645.825098681005
step: 666	loss: 129649.19922590256	speed (wps):5646.22464315977
step: 748	loss: 145427.91512012482	speed (wps):5646.505816219909
epoch: 3	train ppl: 257.0991370023241	val ppl: 221.01499885682657	best val: 221.01499885682657	time (s) spent in epoch: 167.6272737979889

EPOCH 4 ------------------
step: 10	loss: 2208.2035279273987	speed (wps):5460.617121355054
step: 92	loss: 17983.301799297333	speed (wps):5624.188694922377
step: 174	loss: 33615.06478071213	speed (wps):5635.235963440936
step: 256	loss: 49450.41630029678	speed (wps):5639.0880383229605
step: 338	loss: 65214.35346841812	speed (wps):5640.808126130088
step: 420	loss: 80862.85727024078	speed (wps):5642.231772990236
step: 502	loss: 96560.39752483368	speed (wps):5642.866168565433
step: 584	loss: 111976.71884298325	speed (wps):5643.37192984522
step: 666	loss: 127575.3730559349	speed (wps):5643.76047125093
step: 748	loss: 143116.59154176712	speed (wps):5644.028494319287
epoch: 4	train ppl: 235.81067598487817	val ppl: 212.89480607381967	best val: 212.89480607381967	time (s) spent in epoch: 167.70266437530518

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3249.6960735321045	speed (wps):3862.365361086489
step: 92	loss: 23178.479216098785	speed (wps):4037.495332972224
step: 174	loss: 41949.98610496521	speed (wps):4048.870991568458
step: 256	loss: 60642.93433666229	speed (wps):4053.106425422849
step: 338	loss: 78995.78671693802	speed (wps):4055.3357671748076
step: 420	loss: 97116.69228315353	speed (wps):4056.4421088006234
step: 502	loss: 115127.76823282242	speed (wps):4057.3060187221736
step: 584	loss: 132814.8295402527	speed (wps):4057.7580689981446
step: 666	loss: 150560.31027555466	speed (wps):4058.2093232433676
step: 748	loss: 168162.8692984581	speed (wps):4058.542525541954
epoch: 0	train ppl: 595.2893294761435	val ppl: 352.2239836971479	best val: 352.2239836971479	time (s) spent in epoch: 233.09338760375977

EPOCH 1 ------------------
step: 10	loss: 2403.69087934494	speed (wps):3942.424062646867
step: 92	loss: 19746.330902576447	speed (wps):4047.4033853283418
step: 174	loss: 36895.59435367584	speed (wps):4053.9992236049347
step: 256	loss: 54204.11553859711	speed (wps):4056.480549530855
step: 338	loss: 71373.65138292313	speed (wps):4057.6519063952637
step: 420	loss: 88417.58523702621	speed (wps):4058.232152416264
step: 502	loss: 105458.69170665741	speed (wps):4058.7274947020655
step: 584	loss: 122226.49829626083	speed (wps):4059.184868740403
step: 666	loss: 139147.97711133957	speed (wps):4059.42444422697
step: 748	loss: 155962.4109697342	speed (wps):4059.561230781364
epoch: 1	train ppl: 381.98000930221053	val ppl: 282.6452936838229	best val: 282.6452936838229	time (s) spent in epoch: 233.04646801948547

EPOCH 2 ------------------
step: 10	loss: 2324.724636077881	speed (wps):3944.015255271171
step: 92	loss: 19067.154676914215	speed (wps):4046.553740362198
step: 174	loss: 35641.783463954926	speed (wps):4052.886487061033
step: 256	loss: 52409.002323150635	speed (wps):4055.3440654736096
step: 338	loss: 69066.4470410347	speed (wps):4056.5633322230487
step: 420	loss: 85616.02033138275	speed (wps):4057.386872481509
step: 502	loss: 102191.71414613724	speed (wps):4057.9245575026184
step: 584	loss: 118507.35127449036	speed (wps):4058.324870010671
step: 666	loss: 134966.37719869614	speed (wps):4058.533083906004
step: 748	loss: 151349.73940849304	speed (wps):4058.7270626755867
epoch: 2	train ppl: 321.5836706185754	val ppl: 256.2879681022659	best val: 256.2879681022659	time (s) spent in epoch: 233.09975266456604

EPOCH 3 ------------------
step: 10	loss: 2285.3408241271973	speed (wps):3943.0690510230615
step: 92	loss: 18653.912942409515	speed (wps):4046.7400288985964
step: 174	loss: 34873.31953048706	speed (wps):4052.7850071114217
step: 256	loss: 51297.61729001999	speed (wps):4055.2158446122908
step: 338	loss: 67617.98688650131	speed (wps):4056.4409483502895
step: 420	loss: 83836.28485441208	speed (wps):4056.8761170009293
step: 502	loss: 100078.935983181	speed (wps):4057.1650655637804
step: 584	loss: 116061.99337005615	speed (wps):4057.3216382020223
step: 666	loss: 132226.98746442795	speed (wps):4057.575297927956
step: 748	loss: 148325.06754398346	speed (wps):4057.704367882399
epoch: 3	train ppl: 287.1297384890735	val ppl: 234.9578838721795	best val: 234.9578838721795	time (s) spent in epoch: 233.1629421710968

EPOCH 4 ------------------
step: 10	loss: 2253.7364625930786	speed (wps):3938.7678065126556
step: 92	loss: 18343.473761081696	speed (wps):4044.558942446537
step: 174	loss: 34295.850105285645	speed (wps):4051.400819887627
step: 256	loss: 50461.470732688904	speed (wps):4054.0075369330734
step: 338	loss: 66527.46958732605	speed (wps):4055.1427639158355
step: 420	loss: 82495.79350948334	speed (wps):4055.9483271103345
step: 502	loss: 98492.30060338974	speed (wps):4056.577637479788
step: 584	loss: 114245.02334594727	speed (wps):4056.8804023099224
step: 666	loss: 130166.89029693604	speed (wps):4057.125286207889
step: 748	loss: 146036.25854969025	speed (wps):4057.264498826228
epoch: 4	train ppl: 263.2460585325986	val ppl: 225.2070372865305	best val: 225.2070372865305	time (s) spent in epoch: 233.18068981170654

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3258.5147500038147	speed (wps):8180.2312172951515
step: 51	loss: 13343.367881774902	speed (wps):8489.037511638375
step: 92	loss: 22892.171444892883	speed (wps):8527.817830685792
step: 133	loss: 32207.93118953705	speed (wps):8543.179957623337
step: 174	loss: 41279.86277341843	speed (wps):8552.494254819305
step: 215	loss: 50405.46541213989	speed (wps):8559.74761396385
step: 256	loss: 59448.498804569244	speed (wps):8562.044945647958
step: 297	loss: 68380.53711652756	speed (wps):8565.912638969665
step: 338	loss: 77217.27284669876	speed (wps):8567.84094570981
step: 379	loss: 86034.77173805237	speed (wps):8568.412656669308
epoch: 0	train ppl: 624.4860058200718	val ppl: 356.32622869550227	best val: 356.32622869550227	time (s) spent in epoch: 110.68440747261047

EPOCH 1 ------------------
step: 10	loss: 2364.98361825943	speed (wps):8360.166469380589
step: 51	loss: 11036.546242237091	speed (wps):8544.792845770848
step: 92	loss: 19661.73100233078	speed (wps):8563.679419059028
step: 133	loss: 28210.21066904068	speed (wps):8568.912291288581
step: 174	loss: 36590.12182474136	speed (wps):8571.185042855192
step: 215	loss: 45097.011053562164	speed (wps):8570.97852612143
step: 256	loss: 53603.37650537491	speed (wps):8572.205041961019
step: 297	loss: 62036.5865778923	speed (wps):8573.08682424228
step: 338	loss: 70437.03518629074	speed (wps):8572.203590336472
step: 379	loss: 78857.83716440201	speed (wps):8573.537676697493
epoch: 1	train ppl: 372.5363572085788	val ppl: 280.93050613143873	best val: 280.93050613143873	time (s) spent in epoch: 110.62728333473206

EPOCH 2 ------------------
step: 10	loss: 2272.8087949752808	speed (wps):8341.09759556242
step: 51	loss: 10598.453738689423	speed (wps):8531.094564061143
step: 92	loss: 18910.304408073425	speed (wps):8548.103999944587
step: 133	loss: 27163.691806793213	speed (wps):8558.833724650536
step: 174	loss: 35263.806788921356	speed (wps):8564.272056300591
step: 215	loss: 43501.57635688782	speed (wps):8566.39866096573
step: 256	loss: 51755.8583855629	speed (wps):8567.77133822671
step: 297	loss: 59945.948123931885	speed (wps):8568.476458643478
step: 338	loss: 68111.37675046921	speed (wps):8569.473246645148
step: 379	loss: 76318.70777130127	speed (wps):8570.533687543846
epoch: 2	train ppl: 308.99462881197866	val ppl: 244.49736853357646	best val: 244.49736853357646	time (s) spent in epoch: 110.65870642662048

EPOCH 3 ------------------
step: 10	loss: 2219.8983550071716	speed (wps):8368.194902483414
step: 51	loss: 10347.50637292862	speed (wps):8529.660821727433
step: 92	loss: 18456.61864042282	speed (wps):8552.778651913015
step: 133	loss: 26526.050209999084	speed (wps):8559.52461954153
step: 174	loss: 34429.93778705597	speed (wps):8565.598436059912
step: 215	loss: 42473.96869182587	speed (wps):8568.189741396574
step: 256	loss: 50555.66240787506	speed (wps):8569.791019535554
step: 297	loss: 58575.72882890701	speed (wps):8570.723496885746
step: 338	loss: 66574.0487575531	speed (wps):8571.288438477086
step: 379	loss: 74623.77723932266	speed (wps):8572.75609854096
epoch: 3	train ppl: 272.3657704455723	val ppl: 224.80125466244422	best val: 224.80125466244422	time (s) spent in epoch: 110.63190269470215

EPOCH 4 ------------------
step: 10	loss: 2178.1372356414795	speed (wps):8370.655254562394
step: 51	loss: 10162.118618488312	speed (wps):8526.892409134567
step: 92	loss: 18135.631654262543	speed (wps):8546.617948113279
step: 133	loss: 26053.630719184875	speed (wps):8554.910547440912
step: 174	loss: 33814.170389175415	speed (wps):8558.993632985232
step: 215	loss: 41733.54728937149	speed (wps):8562.2201802673
step: 256	loss: 49684.11854505539	speed (wps):8563.947080841539
step: 297	loss: 57584.57248210907	speed (wps):8566.171680242089
step: 338	loss: 65461.53442621231	speed (wps):8567.586410031707
step: 379	loss: 73395.19728422165	speed (wps):8567.700622974002
epoch: 4	train ppl: 248.55665556916335	val ppl: 215.62642362945712	best val: 215.62642362945712	time (s) spent in epoch: 110.69817852973938

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 4121.137113571167	speed (wps):4848.530325208901
step: 51	loss: 14396.1487698555	speed (wps):4946.276856509166
step: 92	loss: 24023.1840467453	speed (wps):4957.377295441965
step: 133	loss: 33390.12231588364	speed (wps):4961.589950700418
step: 174	loss: 42528.01620721817	speed (wps):4964.007039055442
step: 215	loss: 51739.56183433533	speed (wps):4965.1959834304425
step: 256	loss: 60879.86294031143	speed (wps):4965.720880677224
step: 297	loss: 69907.2423195839	speed (wps):4966.516719945372
step: 338	loss: 78864.04383182526	speed (wps):4967.020670189318
step: 379	loss: 87790.06494998932	speed (wps):4967.627769247877
epoch: 0	train ppl: 709.3539080881711	val ppl: 370.3579672389728	best val: 370.3579672389728	time (s) spent in epoch: 190.75658631324768

EPOCH 1 ------------------
step: 10	loss: 2398.558886051178	speed (wps):4899.691786972203
step: 51	loss: 11176.37808084488	speed (wps):4956.864040350967
step: 92	loss: 19916.61979675293	speed (wps):4964.173596101853
step: 133	loss: 28576.7791056633	speed (wps):4966.128014742024
step: 174	loss: 37088.475720882416	speed (wps):4967.408016612804
step: 215	loss: 45723.56254339218	speed (wps):4968.064548811686
step: 256	loss: 54359.23087120056	speed (wps):4968.778793358959
step: 297	loss: 62913.56784820557	speed (wps):4968.683092885729
step: 338	loss: 71436.04611873627	speed (wps):4969.198013302273
step: 379	loss: 79972.86754131317	speed (wps):4969.385342913488
epoch: 1	train ppl: 405.09065667653147	val ppl: 289.78964124814553	best val: 289.78964124814553	time (s) spent in epoch: 190.70021796226501

EPOCH 2 ------------------
step: 10	loss: 2307.4247074127197	speed (wps):4905.347218668975
step: 51	loss: 10770.771350860596	speed (wps):4955.600060974207
step: 92	loss: 19209.69810962677	speed (wps):4962.431597783958
step: 133	loss: 27581.173975467682	speed (wps):4964.36012487108
step: 174	loss: 35815.52307128906	speed (wps):4965.771367708687
step: 215	loss: 44173.29171419144	speed (wps):4966.922511554189
step: 256	loss: 52560.79987287521	speed (wps):4967.436874339036
step: 297	loss: 60881.77127838135	speed (wps):4967.72722851912
step: 338	loss: 69168.99279594421	speed (wps):4967.526450477192
step: 379	loss: 77484.36706066132	speed (wps):4967.600508291005
epoch: 2	train ppl: 337.1775482289225	val ppl: 260.7997858285122	best val: 260.7997858285122	time (s) spent in epoch: 190.76280808448792

EPOCH 3 ------------------
step: 10	loss: 2251.914291381836	speed (wps):4900.510293415656
step: 51	loss: 10513.80825996399	speed (wps):4953.542088403341
step: 92	loss: 18755.610480308533	speed (wps):4959.340474779818
step: 133	loss: 26949.13686275482	speed (wps):4961.724483825681
step: 174	loss: 34984.87475395203	speed (wps):4963.160964319956
step: 215	loss: 43168.95310640335	speed (wps):4964.489886023757
step: 256	loss: 51381.5659737587	speed (wps):4965.020946611036
step: 297	loss: 59542.318325042725	speed (wps):4965.630329110772
step: 338	loss: 67666.62905454636	speed (wps):4966.139085659308
step: 379	loss: 75834.2054271698	speed (wps):4966.186210639627
epoch: 3	train ppl: 298.37727478228294	val ppl: 237.30638064947115	best val: 237.30638064947115	time (s) spent in epoch: 190.8266351222992

EPOCH 4 ------------------
step: 10	loss: 2211.6707634925842	speed (wps):4893.018039999426
step: 51	loss: 10329.898943901062	speed (wps):4951.053331724439
step: 92	loss: 18428.83958339691	speed (wps):4958.286392619749
step: 133	loss: 26492.8747010231	speed (wps):4961.022938561801
step: 174	loss: 34392.14025974274	speed (wps):4962.22576730431
step: 215	loss: 42428.63254070282	speed (wps):4963.548120054469
step: 256	loss: 50519.57109451294	speed (wps):4964.470985174716
step: 297	loss: 58550.68491458893	speed (wps):4964.895481887024
step: 338	loss: 66558.01474571228	speed (wps):4965.069360188822
step: 379	loss: 74616.44604682922	speed (wps):4964.9936937046605
epoch: 4	train ppl: 272.3837507620104	val ppl: 225.27333781353593	best val: 225.27333781353593	time (s) spent in epoch: 190.8613657951355

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:04:00.0.
All done.
