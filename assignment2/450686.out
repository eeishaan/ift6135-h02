
########## Setting Up Experiment ######################

Putting log in GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_1
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3548.367028236389	speed (wps):6008.3373642722945
step: 51	loss: 16772.35263824463	speed (wps):6257.637947380371
step: 92	loss: 29995.9539270401	speed (wps):6288.181011371446
step: 133	loss: 43219.36869621277	speed (wps):6299.279206977549
step: 174	loss: 56442.98590660095	speed (wps):6305.9041938598075
step: 215	loss: 69665.31617164612	speed (wps):6310.16726181613
step: 256	loss: 82888.14531326294	speed (wps):6313.096026127964
step: 297	loss: 96109.60262775421	speed (wps):6314.981372702588
step: 338	loss: 109330.27894973755	speed (wps):6316.43686898631
step: 379	loss: 122551.03731632233	speed (wps):6317.7361271489535
epoch: 0	train ppl: 10038.695154889512	val ppl: 10000.495711831987	best val: 10000.495711831987	time (s) spent in epoch: 150.233966588974

EPOCH 1 ------------------
step: 10	loss: 3546.860318183899	speed (wps):6191.095675719391
step: 51	loss: 16767.522253990173	speed (wps):6294.2177447425065
step: 92	loss: 29986.74934387207	speed (wps):6306.588224321395
step: 133	loss: 43206.33068084717	speed (wps):6311.91471225751
step: 174	loss: 56425.74746131897	speed (wps):6313.611832944777
step: 215	loss: 69643.7388420105	speed (wps):6315.710179352425
step: 256	loss: 82861.30728244781	speed (wps):6316.474315123397
step: 297	loss: 96078.32871437073	speed (wps):6317.823110873937
step: 338	loss: 109295.3145980835	speed (wps):6319.823097232804
step: 379	loss: 122511.58307552338	speed (wps):6321.389505959048
epoch: 1	train ppl: 10008.983671861437	val ppl: 9970.361281512178	best val: 9970.361281512178	time (s) spent in epoch: 150.03156328201294

EPOCH 2 ------------------
step: 10	loss: 3545.688829421997	speed (wps):6193.385970220613
step: 51	loss: 16760.934085845947	speed (wps):6300.88073385077
step: 92	loss: 29975.926699638367	speed (wps):6314.768125090019
step: 133	loss: 43190.624113082886	speed (wps):6325.876748512899
step: 174	loss: 56405.597467422485	speed (wps):6333.012964504934
step: 215	loss: 69619.88913059235	speed (wps):6337.329079538551
step: 256	loss: 82833.96213054657	speed (wps):6340.796856548151
step: 297	loss: 96046.91107273102	speed (wps):6343.871362800036
step: 338	loss: 109258.8855266571	speed (wps):6346.173165349833
step: 379	loss: 122471.03631973267	speed (wps):6347.6790420914
epoch: 2	train ppl: 9979.01266720927	val ppl: 9940.312915168492	best val: 9940.312915168492	time (s) spent in epoch: 149.37411856651306

EPOCH 3 ------------------
step: 10	loss: 3544.8692512512207	speed (wps):6218.102952345385
step: 51	loss: 16756.61950111389	speed (wps):6330.3341045058305
step: 92	loss: 29967.281107902527	speed (wps):6344.972378962791
step: 133	loss: 43177.78700351715	speed (wps):6350.277401388852
step: 174	loss: 56388.49497318268	speed (wps):6353.081743773768
step: 215	loss: 69598.59041213989	speed (wps):6355.404638675629
step: 256	loss: 82807.88513183594	speed (wps):6356.690392298237
step: 297	loss: 96016.87069892883	speed (wps):6357.744621604442
step: 338	loss: 109224.62853431702	speed (wps):6358.988726573942
step: 379	loss: 122432.43520259857	speed (wps):6359.519833137148
epoch: 3	train ppl: 9949.706769012564	val ppl: 9910.348019225772	best val: 9910.348019225772	time (s) spent in epoch: 149.11054110527039

EPOCH 4 ------------------
step: 10	loss: 3543.7066078186035	speed (wps):6232.825826040078
step: 51	loss: 16750.734219551086	speed (wps):6338.255895633993
step: 92	loss: 29956.61060333252	speed (wps):6348.983744901065
step: 133	loss: 43162.72948265076	speed (wps):6353.619785500216
step: 174	loss: 56368.48967552185	speed (wps):6356.134893653732
step: 215	loss: 69574.63432312012	speed (wps):6358.10775820829
step: 256	loss: 82779.13310527802	speed (wps):6358.410126001556
step: 297	loss: 95983.53882789612	speed (wps):6359.38699917205
step: 338	loss: 109187.48913288116	speed (wps):6360.228889639099
step: 379	loss: 122390.5938911438	speed (wps):6360.784606612603
epoch: 4	train ppl: 9918.534339780972	val ppl: 9880.476396538272	best val: 9880.476396538272	time (s) spent in epoch: 149.07729578018188

DONE

Saving learning curves to GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_1/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3544.9495601654053	speed (wps):3947.6096388036503
step: 51	loss: 16757.12401866913	speed (wps):4074.228938924418
step: 92	loss: 29968.06577205658	speed (wps):4088.6750085161875
step: 133	loss: 43178.815298080444	speed (wps):4094.7524323411662
step: 174	loss: 56388.38342189789	speed (wps):4097.58581781329
step: 215	loss: 69599.0193271637	speed (wps):4099.182422741603
step: 256	loss: 82808.89580249786	speed (wps):4100.65907340849
step: 297	loss: 96018.64537239075	speed (wps):4101.499091516003
step: 338	loss: 109227.81782627106	speed (wps):4102.672562075921
step: 379	loss: 122436.50268554688	speed (wps):4103.203949585979
epoch: 0	train ppl: 9952.282435099783	val ppl: 9910.706877657603	best val: 9910.706877657603	time (s) spent in epoch: 231.0949366092682

EPOCH 1 ------------------
step: 10	loss: 3543.548092842102	speed (wps):4040.336041625438
step: 51	loss: 16749.83606815338	speed (wps):4091.8783617793556
step: 92	loss: 29956.09263420105	speed (wps):4097.773348077407
step: 133	loss: 43162.06067562103	speed (wps):4101.110165467618
step: 174	loss: 56366.37881278992	speed (wps):4102.548943947083
step: 215	loss: 69571.18951797485	speed (wps):4103.686495565091
step: 256	loss: 82775.61159610748	speed (wps):4104.431194862486
step: 297	loss: 95979.99729156494	speed (wps):4104.806724107094
step: 338	loss: 109183.898396492	speed (wps):4104.980392134422
step: 379	loss: 122386.5357875824	speed (wps):4105.062634208727
epoch: 1	train ppl: 9914.661408914593	val ppl: 9874.137210265944	best val: 9874.137210265944	time (s) spent in epoch: 230.89453196525574

EPOCH 2 ------------------
step: 10	loss: 3541.9543981552124	speed (wps):4041.8668929589976
step: 51	loss: 16743.343863487244	speed (wps):4096.628015231109
step: 92	loss: 29944.401879310608	speed (wps):4101.1693806469575
step: 133	loss: 43145.21880149841	speed (wps):4102.721526330601
step: 174	loss: 56344.15710926056	speed (wps):4103.870994083783
step: 215	loss: 69543.63221168518	speed (wps):4104.85660895548
step: 256	loss: 82742.77563095093	speed (wps):4105.3276202527295
step: 297	loss: 95941.27567768097	speed (wps):4105.816946524217
step: 338	loss: 109140.32797336578	speed (wps):4105.796695182857
step: 379	loss: 122337.48583316803	speed (wps):4105.677689565235
epoch: 2	train ppl: 9878.94037475405	val ppl: 9837.698670421578	best val: 9837.698670421578	time (s) spent in epoch: 230.88457536697388

EPOCH 3 ------------------
step: 10	loss: 3540.8144187927246	speed (wps):4040.2177365251373
step: 51	loss: 16736.901292800903	speed (wps):4094.0833492058027
step: 92	loss: 29932.585320472717	speed (wps):4100.766504222623
step: 133	loss: 43127.876682281494	speed (wps):4103.149678359324
step: 174	loss: 56322.0162153244	speed (wps):4104.4756858481915
step: 215	loss: 69516.63559913635	speed (wps):4105.389239886342
step: 256	loss: 82710.17531871796	speed (wps):4105.959469102836
step: 297	loss: 95904.02165412903	speed (wps):4106.373274094126
step: 338	loss: 109097.82316207886	speed (wps):4106.29786482603
step: 379	loss: 122289.07511234283	speed (wps):4106.441691683235
epoch: 3	train ppl: 9842.298275647381	val ppl: 9801.401902373253	best val: 9801.401902373253	time (s) spent in epoch: 230.82575178146362

EPOCH 4 ------------------
step: 10	loss: 3539.2803716659546	speed (wps):4045.4202222452127
step: 51	loss: 16730.02610206604	speed (wps):4094.3651445462224
step: 92	loss: 29920.305733680725	speed (wps):4100.135065069918
step: 133	loss: 43109.69732761383	speed (wps):4102.9171922199885
step: 174	loss: 56297.699604034424	speed (wps):4104.945883464236
step: 215	loss: 69487.13265419006	speed (wps):4105.797154188163
step: 256	loss: 82675.5338382721	speed (wps):4105.923413671975
step: 297	loss: 95863.76983165741	speed (wps):4106.327772949827
step: 338	loss: 109052.1760559082	speed (wps):4106.289860141477
step: 379	loss: 122238.68844509125	speed (wps):4106.271747178015
epoch: 4	train ppl: 9805.493632305093	val ppl: 9765.216935374234	best val: 9765.216935374234	time (s) spent in epoch: 230.84613132476807

DONE

Saving learning curves to GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:84:00.0.
All done.
