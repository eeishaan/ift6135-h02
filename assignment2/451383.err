
Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/2.1.1

[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 158187 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 161683 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 161828 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
Traceback (most recent call last):
  File "ptb-lm.py", line 454, in <module>
    val_ppl, val_loss = run_epoch(model, valid_data)
  File "ptb-lm.py", line 399, in run_epoch
    loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 904, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1970, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/miniconda/lib/python3.6/site-packages/torch/nn/functional.py", line 1295, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 171.00 MiB (GPU 0; 4.63 GiB total capacity; 3.93 GiB already allocated; 15.75 MiB free; 384.54 MiB cached)
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 165014 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 165058 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 165101 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 165144 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 165187 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
*** stack smashing detected ***: python terminated
/var/spool/torque/mom_priv/jobs/650968.gpu-srv1.helios.SC: line 1: 165436 User defined signal 2   bash -c "module --force purge && singularity exec $SINGULARITY_ARGS $EXEC_ARGS"
[33mWARNING:[0m underlay of /usr/bin/nvidia-smi required more than 50 (668) bind mounts
[31mFATAL:  [0m interrupted by signal terminated

==============NVSMI LOG==============

Timestamp                           : Sat Mar 23 19:30:24 2019
Driver Version                      : 410.73
CUDA Version                        : 10.0

Attached GPUs                       : 8
GPU 00000000:05:00.0
    Accounting Mode                 : Enabled
    Accounting Mode Buffer Size     : 4000
    Accounted Processes
        Process ID                  : 158299
            GPU Utilization         : 79 %
            Memory Utilization      : 33 %
            Max memory usage        : 2440 MiB
            Time                    : 930266 ms
            Is Running              : 0
        Process ID                  : 158462
            GPU Utilization         : 68 %
            Memory Utilization      : 27 %
            Max memory usage        : 2052 MiB
            Time                    : 837014 ms
            Is Running              : 0
        Process ID                  : 161749
            GPU Utilization         : 78 %
            Memory Utilization      : 35 %
            Max memory usage        : 3306 MiB
            Time                    : 462520 ms
            Is Running              : 0
        Process ID                  : 161892
            GPU Utilization         : 77 %
            Memory Utilization      : 34 %
            Max memory usage        : 3726 MiB
            Time                    : 622631 ms
            Is Running              : 0
        Process ID                  : 162078
            GPU Utilization         : 82 %
            Memory Utilization      : 34 %
            Max memory usage        : 4722 MiB
            Time                    : 225819 ms
            Is Running              : 0
        Process ID                  : 165253
            GPU Utilization         : 81 %
            Memory Utilization      : 34 %
            Max memory usage        : 2440 MiB
            Time                    : 922906 ms
            Is Running              : 0
        Process ID                  : 6714
            GPU Utilization         : 81 %
            Memory Utilization      : 34 %
            Max memory usage        : 0 MiB
            Time                    : 923075 ms
            Is Running              : 0
        Process ID                  : 165500
            GPU Utilization         : 78 %
            Memory Utilization      : 32 %
            Max memory usage        : 2915 MiB
            Time                    : 724722 ms
            Is Running              : 0
        Process ID                  : 6714
            GPU Utilization         : 81 %
            Memory Utilization      : 34 %
            Max memory usage        : 0 MiB
            Time                    : 923075 ms
            Is Running              : 0

