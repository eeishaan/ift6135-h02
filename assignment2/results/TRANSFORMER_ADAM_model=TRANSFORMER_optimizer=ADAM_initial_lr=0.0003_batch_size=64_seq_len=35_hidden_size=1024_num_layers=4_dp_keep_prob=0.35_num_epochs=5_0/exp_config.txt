batch_size    64
code_file    ptb-lm.py
data    data
debug    False
dp_keep_prob    0.35
emb_size    200
evaluate    False
hidden_size    1024
initial_lr    0.0003
model    TRANSFORMER
num_epochs    5
num_layers    4
optimizer    ADAM
save_best    False
save_dir    TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
seed    1111
seq_len    35
