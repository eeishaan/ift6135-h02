
########## Setting Up Experiment ######################

Putting log in ./results/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=7_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3370.7336568832397	speed (wps):1566.9506436958216
step: 142	loss: 36364.795405864716	speed (wps):1618.5587845598966
step: 274	loss: 67068.56081008911	speed (wps):1620.571610929814
step: 406	loss: 96586.22503519058	speed (wps):1621.3950573397926
step: 538	loss: 125658.12348365784	speed (wps):1621.8842826709754
step: 670	loss: 154345.96125602722	speed (wps):1622.0068398576327
step: 802	loss: 182332.8541636467	speed (wps):1622.2531307580948
step: 934	loss: 210137.21827745438	speed (wps):1622.241330194082
step: 1066	loss: 237752.1540904045	speed (wps):1622.3290312724002
step: 1198	loss: 265033.3238697052	speed (wps):1622.3660422846733
epoch: 0	train ppl: 529.9794318589306	val ppl: 316.3373298668411	best val: 316.3373298668411	time (s) spent in epoch: 583.4601428508759

EPOCH 1 ------------------
step: 10	loss: 2313.771848678589	speed (wps):1595.282615362965
step: 142	loss: 29089.095571041107	speed (wps):1620.7979580894562
step: 274	loss: 56157.787573337555	speed (wps):1621.5920014706655
step: 406	loss: 82808.62121343613	speed (wps):1621.6479641280905
step: 538	loss: 109494.71650838852	speed (wps):1621.6241128001875
step: 670	loss: 136091.85819864273	speed (wps):1621.8268836976167
step: 802	loss: 162197.42742300034	speed (wps):1621.84167049327
step: 934	loss: 188338.3529472351	speed (wps):1621.8390996464088
step: 1066	loss: 214400.95268011093	speed (wps):1621.8701339505587
step: 1198	loss: 240196.89141988754	speed (wps):1621.9350113629325
epoch: 1	train ppl: 301.7356937540342	val ppl: 238.5971170821606	best val: 238.5971170821606	time (s) spent in epoch: 583.5017440319061

EPOCH 2 ------------------
step: 10	loss: 2211.858551502228	speed (wps):1595.0626369022375
step: 142	loss: 27666.725850105286	speed (wps):1620.9182384110593
step: 274	loss: 53658.781962394714	speed (wps):1621.6996983334363
step: 406	loss: 79232.30583190918	speed (wps):1622.1183460676243
step: 538	loss: 104916.73419237137	speed (wps):1622.3084036857178
step: 670	loss: 130563.49817276001	speed (wps):1622.185975742748
step: 802	loss: 155727.6746559143	speed (wps):1622.3187714911014
step: 934	loss: 180996.79718255997	speed (wps):1622.369292641633
step: 1066	loss: 206229.23555850983	speed (wps):1622.417018806012
step: 1198	loss: 231140.5280661583	speed (wps):1622.4909203233703
epoch: 2	train ppl: 244.3284527850611	val ppl: 202.83921017911237	best val: 202.83921017911237	time (s) spent in epoch: 583.3457565307617

EPOCH 3 ------------------
step: 10	loss: 2148.9442443847656	speed (wps):1591.5232929525973
step: 142	loss: 26830.019433498383	speed (wps):1620.6690748438211
step: 274	loss: 52138.10458421707	speed (wps):1621.7368773637154
step: 406	loss: 76976.58122301102	speed (wps):1622.2684158539487
step: 538	loss: 101963.60658884048	speed (wps):1622.3009962420806
step: 670	loss: 126918.82380008698	speed (wps):1622.3691820176534
step: 802	loss: 151423.36499214172	speed (wps):1622.4786804007522
step: 934	loss: 176102.2920179367	speed (wps):1622.4273953741063
step: 1066	loss: 200771.6866827011	speed (wps):1622.4626030301279
step: 1198	loss: 225039.75675821304	speed (wps):1622.3446310182644
epoch: 3	train ppl: 211.7532603741309	val ppl: 183.8865661640047	best val: 183.8865661640047	time (s) spent in epoch: 583.42431473732

EPOCH 4 ------------------
step: 10	loss: 2103.493354320526	speed (wps):1595.3553508141285
step: 142	loss: 26193.254776000977	speed (wps):1620.9055787894029
step: 274	loss: 50990.02091407776	speed (wps):1621.4938341047853
step: 406	loss: 75249.14201974869	speed (wps):1621.5223535111777
step: 538	loss: 99669.92136478424	speed (wps):1621.6953681388272
step: 670	loss: 124083.3649110794	speed (wps):1621.8152907140027
step: 802	loss: 148076.252887249	speed (wps):1621.7701835961689
step: 934	loss: 172283.9549255371	speed (wps):1621.7936718063577
step: 1066	loss: 196474.59629297256	speed (wps):1621.7601619790864
step: 1198	loss: 220213.59671592712	speed (wps):1621.678223043293
epoch: 4	train ppl: 188.9496557301142	val ppl: 168.82202387015826	best val: 168.82202387015826	time (s) spent in epoch: 583.627937078476

DONE

Saving learning curves to ./results/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=7_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:84:00.0.
All done.
