
########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2771.4247131347656	speed (wps):5017.309044571466
step: 51	loss: 12127.73211479187	speed (wps):5197.032131002018
step: 92	loss: 20868.347153663635	speed (wps):5218.025060282235
step: 133	loss: 29185.59480905533	speed (wps):5225.644933903221
step: 174	loss: 37135.48066854477	speed (wps):5230.089379469886
step: 215	loss: 45094.25830364227	speed (wps):5233.453747649091
step: 256	loss: 52991.655094623566	speed (wps):5235.22841874789
step: 297	loss: 60744.735164642334	speed (wps):5236.575215059993
step: 338	loss: 68414.44932460785	speed (wps):5237.38679090767
step: 379	loss: 76084.86728906631	speed (wps):5238.09460796468
epoch: 0	train ppl: 294.0243714172509	val ppl: 197.19222256132403	best val: 197.19222256132403	time (s) spent in epoch: 180.6886956691742

EPOCH 1 ------------------
step: 10	loss: 2035.014100074768	speed (wps):5160.000115834276
step: 51	loss: 9425.744504928589	speed (wps):5227.484993862726
step: 92	loss: 16749.719960689545	speed (wps):5235.56031246326
step: 133	loss: 23978.444237709045	speed (wps):5237.6775924496405
step: 174	loss: 30952.91838645935	speed (wps):5238.67624629671
step: 215	loss: 38053.11669111252	speed (wps):5239.072998759365
step: 256	loss: 45195.33032655716	speed (wps):5240.051518623261
step: 297	loss: 52244.80490684509	speed (wps):5240.648061125813
step: 338	loss: 59270.545566082	speed (wps):5240.832265019079
step: 379	loss: 66338.59857559204	speed (wps):5241.303921992196
epoch: 1	train ppl: 145.17702170015878	val ppl: 174.36922131049775	best val: 174.36922131049775	time (s) spent in epoch: 180.5570638179779

EPOCH 2 ------------------
step: 10	loss: 1901.0234832763672	speed (wps):5159.293272078708
step: 51	loss: 8791.670062541962	speed (wps):5225.274250823566
step: 92	loss: 15672.142543792725	speed (wps):5232.5860386302365
step: 133	loss: 22501.145989894867	speed (wps):5235.849538436889
step: 174	loss: 29082.95175552368	speed (wps):5237.460230726293
step: 215	loss: 35806.61103487015	speed (wps):5238.310295855369
step: 256	loss: 42596.39599323273	speed (wps):5238.803644640972
step: 297	loss: 49308.02232027054	speed (wps):5239.075965978031
step: 338	loss: 56000.024349689484	speed (wps):5239.68147137425
step: 379	loss: 62741.80513858795	speed (wps):5240.009991062094
epoch: 2	train ppl: 111.19496303492323	val ppl: 169.0982164118371	best val: 169.0982164118371	time (s) spent in epoch: 180.5920774936676

EPOCH 3 ------------------
step: 10	loss: 1816.5890741348267	speed (wps):5158.671595144439
step: 51	loss: 8431.426553726196	speed (wps):5227.298461296948
step: 92	loss: 15041.886880397797	speed (wps):5234.243981106274
step: 133	loss: 21615.474700927734	speed (wps):5236.890424344381
step: 174	loss: 27951.78297519684	speed (wps):5238.040808606153
step: 215	loss: 34425.963480472565	speed (wps):5239.476165169669
step: 256	loss: 40984.77369308472	speed (wps):5240.025681970219
step: 297	loss: 47472.19440460205	speed (wps):5240.386779144089
step: 338	loss: 53946.78461551666	speed (wps):5240.770201091036
step: 379	loss: 60492.28394269943	speed (wps):5241.089774083115
epoch: 3	train ppl: 94.09065752193078	val ppl: 167.8733738641705	best val: 167.8733738641705	time (s) spent in epoch: 180.56227660179138

EPOCH 4 ------------------
step: 10	loss: 1756.4386820793152	speed (wps):5161.66056917604
step: 51	loss: 8187.832026481628	speed (wps):5226.105023938764
step: 92	loss: 14657.914390563965	speed (wps):5234.509483622639
step: 133	loss: 21069.928936958313	speed (wps):5236.674141622461
step: 174	loss: 27225.79149246216	speed (wps):5238.541999452048
step: 215	loss: 33505.36702632904	speed (wps):5238.844322800937
step: 256	loss: 39877.26326465607	speed (wps):5239.378472800364
step: 297	loss: 46179.86477613449	speed (wps):5239.747627500668
step: 338	loss: 52472.63605594635	speed (wps):5239.983159331164
step: 379	loss: 58836.539454460144	speed (wps):5239.98517289987
epoch: 4	train ppl: 83.07765920880014	val ppl: 170.41780404440158	best val: 167.8733738641705	time (s) spent in epoch: 180.58197259902954

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2859.968628883362	speed (wps):5378.7580370933
step: 51	loss: 12334.049170017242	speed (wps):5631.195867319546
step: 92	loss: 21266.101684570312	speed (wps):5663.732695085497
step: 133	loss: 29790.92116355896	speed (wps):5676.978903245527
step: 174	loss: 37946.494908332825	speed (wps):5683.369754014821
step: 215	loss: 46069.8313999176	speed (wps):5687.067072529431
step: 256	loss: 54086.48225545883	speed (wps):5689.617514081549
step: 297	loss: 61924.8313498497	speed (wps):5691.126172011843
step: 338	loss: 69666.2851190567	speed (wps):5692.09325363371
step: 379	loss: 77381.04648828506	speed (wps):5692.882697608578
epoch: 0	train ppl: 322.1929313282178	val ppl: 198.74406510925076	best val: 198.74406510925076	time (s) spent in epoch: 165.89168286323547

EPOCH 1 ------------------
step: 10	loss: 2051.690249443054	speed (wps):5571.21662180482
step: 51	loss: 9484.842085838318	speed (wps):5671.52755740977
step: 92	loss: 16851.391112804413	speed (wps):5683.1464566716495
step: 133	loss: 24122.65970468521	speed (wps):5687.654393563626
step: 174	loss: 31145.088839530945	speed (wps):5690.696514597873
step: 215	loss: 38284.44259643555	speed (wps):5691.8798109058225
step: 256	loss: 45436.45218372345	speed (wps):5692.223473869032
step: 297	loss: 52491.29532814026	speed (wps):5692.882908648483
step: 338	loss: 59491.94534301758	speed (wps):5693.334685724949
step: 379	loss: 66520.61308860779	speed (wps):5693.46068422881
epoch: 1	train ppl: 146.5889255548613	val ppl: 160.7020157118186	best val: 160.7020157118186	time (s) spent in epoch: 165.8412914276123

EPOCH 2 ------------------
step: 10	loss: 1890.6146836280823	speed (wps):5567.765260481711
step: 51	loss: 8714.641563892365	speed (wps):5668.105687701999
step: 92	loss: 15534.980459213257	speed (wps):5681.122474950217
step: 133	loss: 22291.477723121643	speed (wps):5687.445935242915
step: 174	loss: 28806.70865058899	speed (wps):5689.783146344449
step: 215	loss: 35460.37455558777	speed (wps):5691.406923896338
step: 256	loss: 42145.26140213013	speed (wps):5692.537792985403
step: 297	loss: 48748.48588228226	speed (wps):5692.9442826897575
step: 338	loss: 55321.53347015381	speed (wps):5693.6635281560475
step: 379	loss: 61939.37095165253	speed (wps):5694.302849139089
epoch: 2	train ppl: 104.41085247018968	val ppl: 147.9301373922645	best val: 147.9301373922645	time (s) spent in epoch: 165.80682158470154

EPOCH 3 ------------------
step: 10	loss: 1777.951340675354	speed (wps):5568.195734261017
step: 51	loss: 8204.878561496735	speed (wps):5667.643824113441
step: 92	loss: 14656.284847259521	speed (wps):5679.360471539115
step: 133	loss: 21052.395708560944	speed (wps):5682.751027879762
step: 174	loss: 27204.238028526306	speed (wps):5686.037486689013
step: 215	loss: 33503.15320730209	speed (wps):5688.334642087838
step: 256	loss: 39844.80487823486	speed (wps):5689.242805322832
step: 297	loss: 46108.3580327034	speed (wps):5690.323694677083
step: 338	loss: 52358.38756322861	speed (wps):5691.065294327868
step: 379	loss: 58657.70904302597	speed (wps):5691.565319555485
epoch: 3	train ppl: 81.77966470280796	val ppl: 142.84178907168786	best val: 142.84178907168786	time (s) spent in epoch: 165.8880546092987

EPOCH 4 ------------------
step: 10	loss: 1694.1632747650146	speed (wps):5571.847086925217
step: 51	loss: 7812.411868572235	speed (wps):5670.770829325191
step: 92	loss: 13966.657831668854	speed (wps):5683.617166788971
step: 133	loss: 20068.905341625214	speed (wps):5688.079117180063
step: 174	loss: 25929.188735485077	speed (wps):5691.072266223168
step: 215	loss: 31934.179071187973	speed (wps):5692.183851196153
step: 256	loss: 37988.70517849922	speed (wps):5693.046813423935
step: 297	loss: 43965.87890625	speed (wps):5693.752501529039
step: 338	loss: 49941.85671210289	speed (wps):5694.496999857358
step: 379	loss: 55965.13882994652	speed (wps):5694.905469442278
epoch: 4	train ppl: 66.84191934508931	val ppl: 145.83940733520308	best val: 142.84178907168786	time (s) spent in epoch: 165.78993272781372

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2785.1701045036316	speed (wps):3650.923330492131
step: 51	loss: 12243.37134361267	speed (wps):3762.201090089329
step: 92	loss: 21094.153323173523	speed (wps):3775.48323941415
step: 133	loss: 29580.3786110878	speed (wps):3780.5623588898115
step: 174	loss: 37708.28933954239	speed (wps):3783.165193071543
step: 215	loss: 45824.74277973175	speed (wps):3784.59259005466
step: 256	loss: 53847.99493789673	speed (wps):3785.4655532966676
step: 297	loss: 61722.354085445404	speed (wps):3786.243091525358
step: 338	loss: 69516.87238693237	speed (wps):3786.75661790115
step: 379	loss: 77307.58511066437	speed (wps):3787.2191022597717
epoch: 0	train ppl: 322.33319923559367	val ppl: 215.2558753646358	best val: 215.2558753646358	time (s) spent in epoch: 249.7935926914215

EPOCH 1 ------------------
step: 10	loss: 2071.140344142914	speed (wps):3746.4443969880003
step: 51	loss: 9609.909584522247	speed (wps):3780.892928909358
step: 92	loss: 17075.856096744537	speed (wps):3784.960224661378
step: 133	loss: 24476.085884571075	speed (wps):3786.2691310248747
step: 174	loss: 31629.933881759644	speed (wps):3786.6584257949635
step: 215	loss: 38921.76644563675	speed (wps):3787.0606867404586
step: 256	loss: 46233.77175092697	speed (wps):3787.3909243734597
step: 297	loss: 53473.079364299774	speed (wps):3787.643516531861
step: 338	loss: 60670.25133371353	speed (wps):3787.7682543358515
step: 379	loss: 67928.82060289383	speed (wps):3787.822842875787
epoch: 1	train ppl: 163.99845078271997	val ppl: 192.96275703663503	best val: 192.96275703663503	time (s) spent in epoch: 249.6850082874298

EPOCH 2 ------------------
step: 10	loss: 1961.0032606124878	speed (wps):3748.338708673581
step: 51	loss: 9076.012651920319	speed (wps):3780.9526792095926
step: 92	loss: 16195.610933303833	speed (wps):3784.8315878365092
step: 133	loss: 23268.44743013382	speed (wps):3785.813522698883
step: 174	loss: 30115.956456661224	speed (wps):3786.890540718907
step: 215	loss: 37116.36153936386	speed (wps):3787.2059853653136
step: 256	loss: 44160.240013599396	speed (wps):3787.359361581071
step: 297	loss: 51166.16805315018	speed (wps):3787.5333217538514
step: 338	loss: 58132.31193780899	speed (wps):3787.722884240328
step: 379	loss: 65172.43368387222	speed (wps):3787.511377466211
epoch: 2	train ppl: 133.90536363408728	val ppl: 184.69970116214043	best val: 184.69970116214043	time (s) spent in epoch: 249.71299815177917

EPOCH 3 ------------------
step: 10	loss: 1898.0550074577332	speed (wps):3746.3980856866215
step: 51	loss: 8819.56123828888	speed (wps):3780.0283118050793
step: 92	loss: 15788.21165561676	speed (wps):3784.310293263132
step: 133	loss: 22693.87289762497	speed (wps):3786.299853465205
step: 174	loss: 29360.441734790802	speed (wps):3786.497111607531
step: 215	loss: 36204.48774814606	speed (wps):3787.0831249995263
step: 256	loss: 43113.60965013504	speed (wps):3787.411919001013
step: 297	loss: 49982.39619970322	speed (wps):3787.613638387339
step: 338	loss: 56830.082449913025	speed (wps):3788.0712104655418
step: 379	loss: 63736.00139141083	speed (wps):3788.2406671013564
epoch: 3	train ppl: 120.54856751340603	val ppl: 185.0593695977291	best val: 184.69970116214043	time (s) spent in epoch: 249.66168451309204

EPOCH 4 ------------------
step: 10	loss: 1859.7442507743835	speed (wps):3748.9485405847686
step: 51	loss: 8681.181285381317	speed (wps):3780.466392381083
step: 92	loss: 15532.51992225647	speed (wps):3784.808979744095
step: 133	loss: 22337.770721912384	speed (wps):3786.968412043553
step: 174	loss: 28902.318198680878	speed (wps):3787.6231618053207
step: 215	loss: 35660.16612768173	speed (wps):3787.864043097236
step: 256	loss: 42480.50832748413	speed (wps):3788.2818738713745
step: 297	loss: 49258.59944343567	speed (wps):3788.2259552346522
step: 338	loss: 56038.85396003723	speed (wps):3788.355023798385
step: 379	loss: 62869.15765285492	speed (wps):3788.769222700671
epoch: 4	train ppl: 112.89085250501594	val ppl: 180.43706985527862	best val: 180.43706985527862	time (s) spent in epoch: 249.62118196487427

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2850.4058933258057	speed (wps):9958.060106521387
step: 30	loss: 7487.485489845276	speed (wps):10348.084392540353
step: 50	loss: 11945.505764484406	speed (wps):10435.316243965293
step: 70	loss: 16180.723943710327	speed (wps):10475.635769643028
step: 90	loss: 20251.06029510498	speed (wps):10496.890223370126
step: 110	loss: 24202.922506332397	speed (wps):10512.248740191952
step: 130	loss: 28065.009264945984	speed (wps):10523.3350013838
step: 150	loss: 31859.051024913788	speed (wps):10531.345572574843
step: 170	loss: 35575.09533882141	speed (wps):10536.240029859538
step: 190	loss: 39269.968440532684	speed (wps):10540.790754987724
epoch: 0	train ppl: 338.8888101155586	val ppl: 187.2953844618528	best val: 187.2953844618528	time (s) spent in epoch: 89.9649748802185

EPOCH 1 ------------------
step: 10	loss: 2013.1253147125244	speed (wps):10392.319279015077
step: 30	loss: 5629.213569164276	speed (wps):10511.77641956953
step: 50	loss: 9179.104681015015	speed (wps):10537.438206419172
step: 70	loss: 12700.001788139343	speed (wps):10548.613092639875
step: 90	loss: 16173.47796678543	speed (wps):10554.033019094286
step: 110	loss: 19605.61201095581	speed (wps):10558.21283743884
step: 130	loss: 23009.210572242737	speed (wps):10561.439277329797
step: 150	loss: 26371.62464618683	speed (wps):10563.152486940631
step: 170	loss: 29680.44313430786	speed (wps):10564.4744222547
step: 190	loss: 32983.34958791733	speed (wps):10566.123470660426
epoch: 1	train ppl: 136.66992298222976	val ppl: 145.28640275549185	best val: 145.28640275549185	time (s) spent in epoch: 89.65516376495361

EPOCH 2 ------------------
step: 10	loss: 1823.786334991455	speed (wps):10393.741745097117
step: 30	loss: 5114.592761993408	speed (wps):10510.756915500637
step: 50	loss: 8341.95855140686	speed (wps):10537.150862623053
step: 70	loss: 11566.665868759155	speed (wps):10548.782658656928
step: 90	loss: 14762.28874206543	speed (wps):10555.631963371454
step: 110	loss: 17928.65084886551	speed (wps):10559.898865243373
step: 130	loss: 21075.699512958527	speed (wps):10563.196910626619
step: 150	loss: 24185.95045566559	speed (wps):10564.575468577501
step: 170	loss: 27254.121396541595	speed (wps):10566.491843152031
step: 190	loss: 30315.344364643097	speed (wps):10567.619842634898
epoch: 2	train ppl: 92.13174616414109	val ppl: 136.11770352395368	best val: 136.11770352395368	time (s) spent in epoch: 89.64600324630737

EPOCH 3 ------------------
step: 10	loss: 1698.8891172409058	speed (wps):10400.310412295892
step: 30	loss: 4767.509117126465	speed (wps):10512.03194302408
step: 50	loss: 7765.418870449066	speed (wps):10537.39348171791
step: 70	loss: 10778.08436870575	speed (wps):10549.42502383521
step: 90	loss: 13769.309439659119	speed (wps):10555.069977821027
step: 110	loss: 16729.169273376465	speed (wps):10559.337259449687
step: 130	loss: 19677.99312353134	speed (wps):10563.012331869406
step: 150	loss: 22588.582220077515	speed (wps):10564.444953630846
step: 170	loss: 25466.745173931122	speed (wps):10566.122625674161
step: 190	loss: 28334.184676408768	speed (wps):10567.597439197747
epoch: 3	train ppl: 68.63016482721125	val ppl: 136.53575046278547	best val: 136.11770352395368	time (s) spent in epoch: 89.6454906463623

EPOCH 4 ------------------
step: 10	loss: 1591.3902115821838	speed (wps):10399.19064182376
step: 30	loss: 4480.707976818085	speed (wps):10520.17687834269
step: 50	loss: 7301.189745664597	speed (wps):10547.62887417634
step: 70	loss: 10135.192393064499	speed (wps):10558.13549101692
step: 90	loss: 12947.617251873016	speed (wps):10565.467038678587
step: 110	loss: 15730.13972401619	speed (wps):10569.600944947317
step: 130	loss: 18502.188959121704	speed (wps):10573.68637838762
step: 150	loss: 21242.248910665512	speed (wps):10575.73709173239
step: 170	loss: 23957.094041109085	speed (wps):10577.94531319915
step: 190	loss: 26655.284292697906	speed (wps):10581.14843235566
epoch: 4	train ppl: 53.403938422011365	val ppl: 144.30392310505297	best val: 136.11770352395368	time (s) spent in epoch: 89.51774597167969

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2750.290467739105	speed (wps):5925.847785907297
step: 30	loss: 7354.869220256805	speed (wps):6007.330484909977
step: 50	loss: 11722.07234621048	speed (wps):6026.153826118407
step: 70	loss: 15904.351813793182	speed (wps):6034.378126556583
step: 90	loss: 19936.64573907852	speed (wps):6039.515445837058
step: 110	loss: 23861.47936105728	speed (wps):6043.124856054316
step: 130	loss: 27701.29217863083	speed (wps):6045.197344111724
step: 150	loss: 31481.692888736725	speed (wps):6046.981158852162
step: 170	loss: 35187.96304702759	speed (wps):6048.639620466554
step: 190	loss: 38876.306676864624	speed (wps):6050.365804299674
epoch: 0	train ppl: 321.0157091802855	val ppl: 193.19822943040296	best val: 193.19822943040296	time (s) spent in epoch: 157.06708693504333

EPOCH 1 ------------------
step: 10	loss: 2011.914622783661	speed (wps):6008.6332423636
step: 30	loss: 5625.883519649506	speed (wps):6052.935894455483
step: 50	loss: 9171.63625240326	speed (wps):6063.635746496679
step: 70	loss: 12689.543738365173	speed (wps):6067.985585882642
step: 90	loss: 16157.660598754883	speed (wps):6070.866060366056
step: 110	loss: 19587.160820961	speed (wps):6071.596056489043
step: 130	loss: 22995.463228225708	speed (wps):6073.136665933161
step: 150	loss: 26358.864674568176	speed (wps):6074.450477070488
step: 170	loss: 29676.90984249115	speed (wps):6074.857199681329
step: 190	loss: 32990.604293346405	speed (wps):6075.542938549107
epoch: 1	train ppl: 137.04165617364114	val ppl: 159.03149864609037	best val: 159.03149864609037	time (s) spent in epoch: 156.45746970176697

EPOCH 2 ------------------
step: 10	loss: 1839.1634058952332	speed (wps):6023.517756055448
step: 30	loss: 5157.445478439331	speed (wps):6058.415060087423
step: 50	loss: 8414.625034332275	speed (wps):6066.782937961476
step: 70	loss: 11678.401036262512	speed (wps):6070.084771624452
step: 90	loss: 14909.559919834137	speed (wps):6072.178749414465
step: 110	loss: 18113.118422031403	speed (wps):6073.400849261949
step: 130	loss: 21301.230692863464	speed (wps):6074.478506220969
step: 150	loss: 24449.024567604065	speed (wps):6075.220775946655
step: 170	loss: 27564.0100812912	speed (wps):6075.451972441103
step: 190	loss: 30676.09713077545	speed (wps):6075.867690464579
epoch: 2	train ppl: 97.39354543072376	val ppl: 152.23527144717556	best val: 152.23527144717556	time (s) spent in epoch: 156.44889616966248

EPOCH 3 ------------------
step: 10	loss: 1730.0603914260864	speed (wps):6022.665817782682
step: 30	loss: 4859.556968212128	speed (wps):6057.650640456712
step: 50	loss: 7927.762539386749	speed (wps):6066.912947973502
step: 70	loss: 11013.263416290283	speed (wps):6070.376957196357
step: 90	loss: 14077.560987472534	speed (wps):6072.500594693043
step: 110	loss: 17116.042046546936	speed (wps):6073.86481309663
step: 130	loss: 20140.37990808487	speed (wps):6074.769285958181
step: 150	loss: 23129.98922109604	speed (wps):6075.348568854756
step: 170	loss: 26089.19046163559	speed (wps):6076.0534222958495
step: 190	loss: 29045.052256584167	speed (wps):6076.292417643054
epoch: 3	train ppl: 76.47089095010321	val ppl: 153.9038394011147	best val: 152.23527144717556	time (s) spent in epoch: 156.43079829216003

EPOCH 4 ------------------
step: 10	loss: 1643.4064722061157	speed (wps):6025.656385706981
step: 30	loss: 4627.499554157257	speed (wps):6061.056510709843
step: 50	loss: 7556.442074775696	speed (wps):6069.169364887459
step: 70	loss: 10496.2713098526	speed (wps):6072.364449225012
step: 90	loss: 13409.650790691376	speed (wps):6074.0396483746645
step: 110	loss: 16306.208975315094	speed (wps):6075.49548312919
step: 130	loss: 19192.91711807251	speed (wps):6076.520909295303
step: 150	loss: 22046.105234622955	speed (wps):6077.180888546737
step: 170	loss: 24876.695963144302	speed (wps):6077.959715288157
step: 190	loss: 27695.097001791	speed (wps):6078.452769986716
epoch: 4	train ppl: 62.498326821438845	val ppl: 161.66800558511545	best val: 152.23527144717556	time (s) spent in epoch: 156.37343525886536

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=256_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=256_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=256_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2787.5968289375305	speed (wps):9953.283086811021
step: 30	loss: 7444.106976985931	speed (wps):10336.597568029496
step: 50	loss: 11958.807137012482	speed (wps):10423.008566151375
step: 70	loss: 16280.835964679718	speed (wps):10460.45723984068
step: 90	loss: 20478.849453926086	speed (wps):10480.28328847997
step: 110	loss: 24569.22665119171	speed (wps):10494.146897383149
step: 130	loss: 28568.33144903183	speed (wps):10502.774439113924
step: 150	loss: 32498.403408527374	speed (wps):10509.848033211556
step: 170	loss: 36352.21586227417	speed (wps):10514.75969158242
step: 190	loss: 40184.79697227478	speed (wps):10518.967374918037
epoch: 0	train ppl: 390.39029517636664	val ppl: 231.54212964795173	best val: 231.54212964795173	time (s) spent in epoch: 90.1578459739685

EPOCH 1 ------------------
step: 10	loss: 2094.9419236183167	speed (wps):10371.848042900183
step: 30	loss: 5848.784065246582	speed (wps):10490.499911214205
step: 50	loss: 9544.511342048645	speed (wps):10514.220136618489
step: 70	loss: 13221.849045753479	speed (wps):10523.80108598986
step: 90	loss: 16871.377601623535	speed (wps):10530.224744442035
step: 110	loss: 20490.853867530823	speed (wps):10533.991703193667
step: 130	loss: 24082.356328964233	speed (wps):10537.428375372605
step: 150	loss: 27635.321826934814	speed (wps):10538.898142367243
step: 170	loss: 31140.639555454254	speed (wps):10541.40385593508
step: 190	loss: 34642.05701112747	speed (wps):10543.440820152071
epoch: 1	train ppl: 175.74499747342512	val ppl: 185.5287779018666	best val: 185.5287779018666	time (s) spent in epoch: 89.84821105003357

EPOCH 2 ------------------
step: 10	loss: 1947.0167589187622	speed (wps):10398.273028434816
step: 30	loss: 5451.85809135437	speed (wps):10521.97702677209
step: 50	loss: 8909.559710025787	speed (wps):10543.559801291192
step: 70	loss: 12372.782950401306	speed (wps):10557.838373863784
step: 90	loss: 15815.754125118256	speed (wps):10571.868161800157
step: 110	loss: 19246.484599113464	speed (wps):10580.046676932536
step: 130	loss: 22654.03365135193	speed (wps):10586.922629519084
step: 150	loss: 26032.54443883896	speed (wps):10591.166514978895
step: 170	loss: 29375.93372821808	speed (wps):10594.138578371572
step: 190	loss: 32713.810625076294	speed (wps):10597.19370001862
epoch: 2	train ppl: 132.31318783399712	val ppl: 168.87815036774677	best val: 168.87815036774677	time (s) spent in epoch: 89.39408469200134

EPOCH 3 ------------------
step: 10	loss: 1860.899350643158	speed (wps):10434.927841363682
step: 30	loss: 5228.457868099213	speed (wps):10555.493291619585
step: 50	loss: 8549.454138278961	speed (wps):10581.332383578516
step: 70	loss: 11874.984323978424	speed (wps):10592.52309526692
step: 90	loss: 15184.871227741241	speed (wps):10599.22097125708
step: 110	loss: 18481.787040233612	speed (wps):10603.825271780206
step: 130	loss: 21770.876622200012	speed (wps):10606.57163960998
step: 150	loss: 25032.138414382935	speed (wps):10608.634317225726
step: 170	loss: 28258.78834962845	speed (wps):10609.778159197125
step: 190	loss: 31487.17300415039	speed (wps):10610.915618271521
epoch: 3	train ppl: 110.35149561581156	val ppl: 161.67177336916154	best val: 161.67177336916154	time (s) spent in epoch: 89.29795360565186

EPOCH 4 ------------------
step: 10	loss: 1797.571783065796	speed (wps):10441.406270664174
step: 30	loss: 5059.818811416626	speed (wps):10554.256272055944
step: 50	loss: 8273.148114681244	speed (wps):10581.771584224789
step: 70	loss: 11494.443905353546	speed (wps):10591.824427973264
step: 90	loss: 14698.584063053131	speed (wps):10597.270485725285
step: 110	loss: 17892.035608291626	speed (wps):10600.807044867777
step: 130	loss: 21084.358789920807	speed (wps):10603.509490403534
step: 150	loss: 24245.345873832703	speed (wps):10605.86430325407
step: 170	loss: 27378.049399852753	speed (wps):10607.545163995403
step: 190	loss: 30508.014361858368	speed (wps):10609.680386199028
epoch: 4	train ppl: 95.45389884733174	val ppl: 159.35673351371335	best val: 159.35673351371335	time (s) spent in epoch: 89.3055534362793

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2809.9837231636047	speed (wps):5913.458617393934
step: 30	loss: 7535.436220169067	speed (wps):5996.631430072875
step: 50	loss: 12078.22500705719	speed (wps):6014.867008154191
step: 70	loss: 16456.043212413788	speed (wps):6022.13271820974
step: 90	loss: 20740.712440013885	speed (wps):6026.471714959586
step: 110	loss: 24937.570095062256	speed (wps):6029.1584718466775
step: 130	loss: 29052.47060060501	speed (wps):6031.341987897261
step: 150	loss: 33109.20483827591	speed (wps):6032.773968781786
step: 170	loss: 37097.886283397675	speed (wps):6033.798508390588
step: 190	loss: 41081.0853767395	speed (wps):6035.222367417496
epoch: 0	train ppl: 449.507313741469	val ppl: 288.19628998255524	best val: 288.19628998255524	time (s) spent in epoch: 157.4462275505066

EPOCH 1 ------------------
step: 10	loss: 2187.166564464569	speed (wps):5999.792720010536
step: 30	loss: 6091.926980018616	speed (wps):6042.137598668049
step: 50	loss: 9952.876846790314	speed (wps):6054.505616891635
step: 70	loss: 13798.73571395874	speed (wps):6059.201315932358
step: 90	loss: 17610.885815620422	speed (wps):6062.604294681721
step: 110	loss: 21400.24013519287	speed (wps):6064.735295889639
step: 130	loss: 25167.709441184998	speed (wps):6066.174424106182
step: 150	loss: 28907.512843608856	speed (wps):6067.305377052619
step: 170	loss: 32607.699601650238	speed (wps):6067.830257551297
step: 190	loss: 36315.2778673172	speed (wps):6068.191106449456
epoch: 1	train ppl: 226.3284174352625	val ppl: 232.94315605900024	best val: 232.94315605900024	time (s) spent in epoch: 156.63716864585876

EPOCH 2 ------------------
step: 10	loss: 2059.466428756714	speed (wps):6021.05562137017
step: 30	loss: 5768.204367160797	speed (wps):6054.464938469904
step: 50	loss: 9452.663729190826	speed (wps):6061.706652574726
step: 70	loss: 13135.964169502258	speed (wps):6066.257223880451
step: 90	loss: 16798.89568567276	speed (wps):6067.832857654796
step: 110	loss: 20452.963914871216	speed (wps):6069.256937252886
step: 130	loss: 24090.119290351868	speed (wps):6070.0917377095375
step: 150	loss: 27698.05882692337	speed (wps):6070.582961190918
step: 170	loss: 31275.329887866974	speed (wps):6071.275818713246
step: 190	loss: 34860.31202316284	speed (wps):6071.44265040189
epoch: 2	train ppl: 182.8281929931125	val ppl: 218.3011676770374	best val: 218.3011676770374	time (s) spent in epoch: 156.55805897712708

EPOCH 3 ------------------
step: 10	loss: 2004.5745015144348	speed (wps):6022.823058956183
step: 30	loss: 5615.409731864929	speed (wps):6056.0141472281175
step: 50	loss: 9190.74863910675	speed (wps):6063.72314783727
step: 70	loss: 12779.612972736359	speed (wps):6067.112921697282
step: 90	loss: 16355.387470722198	speed (wps):6068.9812782182835
step: 110	loss: 19929.886338710785	speed (wps):6070.459875492769
step: 130	loss: 23497.35258579254	speed (wps):6070.904851364866
step: 150	loss: 27031.694960594177	speed (wps):6071.649368351869
step: 170	loss: 30549.1273355484	speed (wps):6072.122372450589
step: 190	loss: 34072.78219938278	speed (wps):6072.625215984089
epoch: 3	train ppl: 162.89398672572744	val ppl: 208.20232113241636	best val: 208.20232113241636	time (s) spent in epoch: 156.53284406661987

EPOCH 4 ------------------
step: 10	loss: 1966.7822980880737	speed (wps):6019.409297598178
step: 30	loss: 5511.155560016632	speed (wps):6056.164313812199
step: 50	loss: 9022.648379802704	speed (wps):6063.540175746093
step: 70	loss: 12537.4294090271	speed (wps):6066.83352021973
step: 90	loss: 16043.941473960876	speed (wps):6068.4220567113
step: 110	loss: 19553.368508815765	speed (wps):6069.399639798813
step: 130	loss: 23051.870193481445	speed (wps):6070.558554549799
step: 150	loss: 26526.319975852966	speed (wps):6071.390097645513
step: 170	loss: 29987.807195186615	speed (wps):6071.788300585399
step: 190	loss: 33456.777646541595	speed (wps):6072.52622828396
epoch: 4	train ppl: 148.78984590047426	val ppl: 200.49990925159995	best val: 200.49990925159995	time (s) spent in epoch: 156.5286350250244

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.9_num_epochs=5/learning_curves.npy
