
########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2839.2592430114746	speed (wps):4908.907209664787
step: 51	loss: 12456.148660182953	speed (wps):5123.748019834502
step: 92	loss: 21742.001128196716	speed (wps):5149.168507121527
step: 133	loss: 30621.293823719025	speed (wps):5158.483262809961
step: 174	loss: 39134.01853322983	speed (wps):5164.447376307045
step: 215	loss: 47629.13449525833	speed (wps):5168.4246786239255
step: 256	loss: 56019.88470077515	speed (wps):5171.067261515128
step: 297	loss: 64266.37461185455	speed (wps):5172.758501269895
step: 338	loss: 72411.162378788	speed (wps):5174.152499787376
step: 379	loss: 80535.02017736435	speed (wps):5175.520624907945
epoch: 0	train ppl: 410.46836714175	val ppl: 442.145025041507	best val: 442.145025041507	time (s) spent in epoch: 182.86166524887085

EPOCH 1 ------------------
step: 10	loss: 2169.9723625183105	speed (wps):5084.9904280936
step: 51	loss: 10094.012537002563	speed (wps):5163.413610550401
step: 92	loss: 17964.7349858284	speed (wps):5172.2210963434045
step: 133	loss: 25758.71075153351	speed (wps):5175.3975812648905
step: 174	loss: 33360.99336147308	speed (wps):5176.978027190667
step: 215	loss: 41080.74312925339	speed (wps):5177.974841446959
step: 256	loss: 48815.91544628143	speed (wps):5178.865476293941
step: 297	loss: 56480.140645504	speed (wps):5179.171437103088
step: 338	loss: 64107.59464263916	speed (wps):5179.57978291983
step: 379	loss: 71768.96542072296	speed (wps):5180.122762525225
epoch: 1	train ppl: 218.60583715316992	val ppl: 339.8549207225993	best val: 339.8549207225993	time (s) spent in epoch: 182.66841173171997

EPOCH 2 ------------------
step: 10	loss: 2068.9103198051453	speed (wps):5077.90277209186
step: 51	loss: 9601.81654214859	speed (wps):5158.7851149836615
step: 92	loss: 17128.7277507782	speed (wps):5168.020973085589
step: 133	loss: 24606.34170293808	speed (wps):5171.691981214061
step: 174	loss: 31893.832411766052	speed (wps):5173.664697661213
step: 215	loss: 39327.15709924698	speed (wps):5174.980599828548
step: 256	loss: 46799.14754152298	speed (wps):5175.854546971113
step: 297	loss: 54211.225798130035	speed (wps):5176.754133689907
step: 338	loss: 61589.625997543335	speed (wps):5177.532184853166
step: 379	loss: 69014.71388101578	speed (wps):5177.697423033078
epoch: 2	train ppl: 178.42562201154706	val ppl: 288.25605787584016	best val: 288.25605787584016	time (s) spent in epoch: 182.75307083129883

EPOCH 3 ------------------
step: 10	loss: 2012.9084372520447	speed (wps):5090.703801982637
step: 51	loss: 9334.166040420532	speed (wps):5164.519184485258
step: 92	loss: 16670.356245040894	speed (wps):5171.5966146231385
step: 133	loss: 23963.711891174316	speed (wps):5174.516127518134
step: 174	loss: 31062.845635414124	speed (wps):5176.305139616084
step: 215	loss: 38314.93399858475	speed (wps):5177.662839892326
step: 256	loss: 45605.21909713745	speed (wps):5178.465726173155
step: 297	loss: 52851.21431350708	speed (wps):5178.930455862546
step: 338	loss: 60072.143869400024	speed (wps):5179.488088675756
step: 379	loss: 67339.77130651474	speed (wps):5179.982787739231
epoch: 3	train ppl: 157.54029030815698	val ppl: 260.5007845117435	best val: 260.5007845117435	time (s) spent in epoch: 182.67865896224976

EPOCH 4 ------------------
step: 10	loss: 1966.4340257644653	speed (wps):5077.138173956288
step: 51	loss: 9141.075789928436	speed (wps):5158.535966186619
step: 92	loss: 16332.0645236969	speed (wps):5168.385269767282
step: 133	loss: 23488.341965675354	speed (wps):5171.6975048671175
step: 174	loss: 30452.225034236908	speed (wps):5173.896809995743
step: 215	loss: 37567.401185035706	speed (wps):5174.84114393298
step: 256	loss: 44734.24942970276	speed (wps):5175.967728439583
step: 297	loss: 51850.071806907654	speed (wps):5176.527781509482
step: 338	loss: 58941.29323720932	speed (wps):5177.2908914514865
step: 379	loss: 66093.84791374207	speed (wps):5177.750887629536
epoch: 4	train ppl: 143.54046008834126	val ppl: 242.09472010052812	best val: 242.09472010052812	time (s) spent in epoch: 182.75907015800476

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2937.3599529266357	speed (wps):5270.190774534227
step: 51	loss: 12521.364138126373	speed (wps):5503.586543565706
step: 92	loss: 21999.678671360016	speed (wps):5531.4179455528865
step: 133	loss: 31196.2109875679	speed (wps):5562.813326130416
step: 174	loss: 40001.66709661484	speed (wps):5588.586275956794
step: 215	loss: 48776.70595407486	speed (wps):5604.731030834623
step: 256	loss: 57425.56979417801	speed (wps):5616.273637775117
step: 297	loss: 65930.82574129105	speed (wps):5624.941808365289
step: 338	loss: 74341.43698215485	speed (wps):5631.307979435693
step: 379	loss: 82709.85414981842	speed (wps):5636.207458617981
epoch: 0	train ppl: 483.49907020618105	val ppl: 381.05437553455425	best val: 381.05437553455425	time (s) spent in epoch: 167.44121980667114

EPOCH 1 ------------------
step: 10	loss: 2237.240524291992	speed (wps):5576.187920132282
step: 51	loss: 10395.751485824585	speed (wps):5652.911677250187
step: 92	loss: 18502.967581748962	speed (wps):5664.839354298202
step: 133	loss: 26533.646111488342	speed (wps):5670.639324369975
step: 174	loss: 34389.408955574036	speed (wps):5674.334403624639
step: 215	loss: 42357.387001514435	speed (wps):5676.423407697388
step: 256	loss: 50324.62382555008	speed (wps):5677.633307882028
step: 297	loss: 58226.60059452057	speed (wps):5680.7986695636455
step: 338	loss: 66074.88895177841	speed (wps):5685.295984056314
step: 379	loss: 73947.1353673935	speed (wps):5691.984761113657
epoch: 1	train ppl: 257.2066291462286	val ppl: 287.48036670831357	best val: 287.48036670831357	time (s) spent in epoch: 165.7079246044159

EPOCH 2 ------------------
step: 10	loss: 2129.766936302185	speed (wps):5645.67046223496
step: 51	loss: 9883.399889469147	speed (wps):5733.429587324298
step: 92	loss: 17616.52596473694	speed (wps):5742.340399187952
step: 133	loss: 25300.98360300064	speed (wps):5747.3560383766335
step: 174	loss: 32827.81959295273	speed (wps):5751.09715684522
step: 215	loss: 40492.04913139343	speed (wps):5752.667748141902
step: 256	loss: 48179.54712629318	speed (wps):5753.9202714881185
step: 297	loss: 55812.9191160202	speed (wps):5754.875986601103
step: 338	loss: 63414.204506874084	speed (wps):5755.9307061695545
step: 379	loss: 71055.08467912674	speed (wps):5756.745010585208
epoch: 2	train ppl: 208.03388524523265	val ppl: 253.43203026163172	best val: 253.43203026163172	time (s) spent in epoch: 164.00966358184814

EPOCH 3 ------------------
step: 10	loss: 2072.8223752975464	speed (wps):5650.382291632837
step: 51	loss: 9618.684267997742	speed (wps):5735.267188813738
step: 92	loss: 17150.335404872894	speed (wps):5747.2323670437
step: 133	loss: 24637.27246761322	speed (wps):5750.834278008863
step: 174	loss: 31956.29762649536	speed (wps):5746.544867214486
step: 215	loss: 39428.25966358185	speed (wps):5745.714284371452
step: 256	loss: 46939.80506658554	speed (wps):5747.4456892374355
step: 297	loss: 54410.79613685608	speed (wps):5748.830241742449
step: 338	loss: 61846.74778699875	speed (wps):5750.018455993535
step: 379	loss: 69330.17674207687	speed (wps):5751.439611676159
epoch: 3	train ppl: 183.01537905775425	val ppl: 233.4059514729335	best val: 233.4059514729335	time (s) spent in epoch: 164.13969469070435

EPOCH 4 ------------------
step: 10	loss: 2025.972638130188	speed (wps):5650.815130706298
step: 51	loss: 9424.352867603302	speed (wps):5736.924813090008
step: 92	loss: 16822.173154354095	speed (wps):5748.9509530245505
step: 133	loss: 24181.072340011597	speed (wps):5752.568813167605
step: 174	loss: 31360.781359672546	speed (wps):5754.843584064109
step: 215	loss: 38697.238743305206	speed (wps):5756.366423905338
step: 256	loss: 46083.63735675812	speed (wps):5757.589627619146
step: 297	loss: 53423.67743253708	speed (wps):5758.688155484784
step: 338	loss: 60737.690410614014	speed (wps):5759.561496945116
step: 379	loss: 68102.61924028397	speed (wps):5760.1833319251755
epoch: 4	train ppl: 166.94903489906312	val ppl: 219.47448021749818	best val: 219.47448021749818	time (s) spent in epoch: 163.92415976524353

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=64_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2943.2217025756836	speed (wps):9899.609410101577
step: 30	loss: 7617.996838092804	speed (wps):10283.426475730685
step: 50	loss: 12260.463156700134	speed (wps):10369.163974715899
step: 70	loss: 16871.20496749878	speed (wps):10407.035364572012
step: 90	loss: 21373.671236038208	speed (wps):10427.799106521368
step: 110	loss: 25738.591532707214	speed (wps):10441.996552615961
step: 130	loss: 30015.529024600983	speed (wps):10451.047636298961
step: 150	loss: 34218.706533908844	speed (wps):10458.93744360494
step: 170	loss: 38345.57837486267	speed (wps):10464.777886748278
step: 190	loss: 42453.518307209015	speed (wps):10469.295983521286
epoch: 0	train ppl: 549.8983567880025	val ppl: 470.45416365177346	best val: 470.45416365177346	time (s) spent in epoch: 90.57348799705505

EPOCH 1 ------------------
step: 10	loss: 2242.310016155243	speed (wps):10320.77679117608
step: 30	loss: 6268.165292739868	speed (wps):10437.207782295991
step: 50	loss: 10253.35410118103	speed (wps):10461.236783201266
step: 70	loss: 14206.134724617004	speed (wps):10473.2142364988
step: 90	loss: 18134.78700876236	speed (wps):10480.472526030288
step: 110	loss: 22032.169785499573	speed (wps):10485.703910930657
step: 130	loss: 25903.616020679474	speed (wps):10488.734351851326
step: 150	loss: 29749.807305336	speed (wps):10491.511309462545
step: 170	loss: 33554.06088113785	speed (wps):10492.705658070307
step: 190	loss: 37365.00095844269	speed (wps):10493.676337550514
epoch: 1	train ppl: 264.6037317405899	val ppl: 324.7501282724783	best val: 324.7501282724783	time (s) spent in epoch: 90.26359033584595

EPOCH 2 ------------------
step: 10	loss: 2107.2669553756714	speed (wps):10321.973550277819
step: 30	loss: 5903.958075046539	speed (wps):10440.292892808926
step: 50	loss: 9670.797593593597	speed (wps):10467.343422098362
step: 70	loss: 13425.445244312286	speed (wps):10478.707511609291
step: 90	loss: 17170.65320968628	speed (wps):10485.866677129226
step: 110	loss: 20894.747574329376	speed (wps):10489.549351812728
step: 130	loss: 24608.40954065323	speed (wps):10492.029688542414
step: 150	loss: 28298.265821933746	speed (wps):10493.454419144395
step: 170	loss: 31953.858318328857	speed (wps):10494.393134264234
step: 190	loss: 35619.62220907211	speed (wps):10495.792259174992
epoch: 2	train ppl: 204.79012620716267	val ppl: 276.3780559772059	best val: 276.3780559772059	time (s) spent in epoch: 90.2481701374054

EPOCH 3 ------------------
step: 10	loss: 2035.5000257492065	speed (wps):10321.451931226507
step: 30	loss: 5709.842200279236	speed (wps):10440.38495783599
step: 50	loss: 9354.675171375275	speed (wps):10465.940994571009
step: 70	loss: 12997.410914897919	speed (wps):10478.180547751102
step: 90	loss: 16627.015483379364	speed (wps):10484.172188477858
step: 110	loss: 20239.177412986755	speed (wps):10488.326083882312
step: 130	loss: 23849.567940235138	speed (wps):10491.136957956625
step: 150	loss: 27433.470361232758	speed (wps):10493.133098127362
step: 170	loss: 30985.849096775055	speed (wps):10494.734115528927
step: 190	loss: 34551.20801925659	speed (wps):10495.913718326408
epoch: 3	train ppl: 174.79427196257933	val ppl: 245.0493158155753	best val: 245.0493158155753	time (s) spent in epoch: 90.24683928489685

EPOCH 4 ------------------
step: 10	loss: 1983.5969185829163	speed (wps):10333.464589481413
step: 30	loss: 5564.9813580513	speed (wps):10440.47833438986
step: 50	loss: 9114.397959709167	speed (wps):10464.378738750009
step: 70	loss: 12665.706210136414	speed (wps):10475.993510671307
step: 90	loss: 16206.712355613708	speed (wps):10483.611165723029
step: 110	loss: 19733.547983169556	speed (wps):10487.517357228491
step: 130	loss: 23255.5827832222	speed (wps):10490.719353151135
step: 150	loss: 26755.02170562744	speed (wps):10492.395647671736
step: 170	loss: 30223.889434337616	speed (wps):10494.629398880776
step: 190	loss: 33701.35108470917	speed (wps):10496.037697346967
epoch: 4	train ppl: 153.99277842106895	val ppl: 228.24278486211378	best val: 228.24278486211378	time (s) spent in epoch: 90.24612331390381

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2919.4890832901	speed (wps):7296.668644495962
step: 30	loss: 7586.995761394501	speed (wps):7561.4498503455725
step: 50	loss: 12226.631207466125	speed (wps):7619.328718648923
step: 70	loss: 16848.84430885315	speed (wps):7646.050401911261
step: 90	loss: 21398.407850265503	speed (wps):7661.20657810728
step: 110	loss: 25828.07315349579	speed (wps):7671.155076428224
step: 130	loss: 30165.622630119324	speed (wps):7677.488814017046
step: 150	loss: 34425.29188632965	speed (wps):7682.227936199026
step: 170	loss: 38611.93450450897	speed (wps):7686.4387560593395
step: 190	loss: 42779.74827051163	speed (wps):7688.583333494718
epoch: 0	train ppl: 578.5773262418214	val ppl: 443.2377670957711	best val: 443.2377670957711	time (s) spent in epoch: 123.27429723739624

EPOCH 1 ------------------
step: 10	loss: 2271.842918395996	speed (wps):7609.07494737397
step: 30	loss: 6348.000247478485	speed (wps):7671.889761529133
step: 50	loss: 10375.332744121552	speed (wps):7685.197309126396
step: 70	loss: 14374.51150894165	speed (wps):7692.350409289787
step: 90	loss: 18347.75314807892	speed (wps):7696.444790554802
step: 110	loss: 22288.393573760986	speed (wps):7699.156351887481
step: 130	loss: 26202.658562660217	speed (wps):7701.735983318278
step: 150	loss: 30087.100706100464	speed (wps):7704.247406240008
step: 170	loss: 33933.88249158859	speed (wps):7706.7718433392765
step: 190	loss: 37785.36727190018	speed (wps):7707.959636565974
epoch: 1	train ppl: 281.63498579061155	val ppl: 301.2403411610259	best val: 301.2403411610259	time (s) spent in epoch: 122.78590869903564

EPOCH 2 ------------------
step: 10	loss: 2131.353054046631	speed (wps):7619.855217984978
step: 30	loss: 5969.767825603485	speed (wps):7690.909592533388
step: 50	loss: 9776.058897972107	speed (wps):7711.962167586734
step: 70	loss: 13569.533972740173	speed (wps):7722.043836774248
step: 90	loss: 17351.627762317657	speed (wps):7732.074776830464
step: 110	loss: 21114.59018945694	speed (wps):7739.320501681111
step: 130	loss: 24864.35915708542	speed (wps):7744.841439758198
step: 150	loss: 28595.831077098846	speed (wps):7749.126057504737
step: 170	loss: 32296.532566547394	speed (wps):7752.097247338888
step: 190	loss: 36005.981702804565	speed (wps):7754.907145628665
epoch: 2	train ppl: 217.00808188457924	val ppl: 264.02428892270865	best val: 264.02428892270865	time (s) spent in epoch: 122.03932785987854

EPOCH 3 ------------------
step: 10	loss: 2062.9145216941833	speed (wps):7677.642990993923
step: 30	loss: 5779.141516685486	speed (wps):7743.588695535815
step: 50	loss: 9467.364616394043	speed (wps):7758.159164742882
step: 70	loss: 13156.092467308044	speed (wps):7764.19958695045
step: 90	loss: 16834.478175640106	speed (wps):7767.596120872341
step: 110	loss: 20497.638201713562	speed (wps):7769.853191169405
step: 130	loss: 24152.669854164124	speed (wps):7770.45115957646
step: 150	loss: 27792.1160697937	speed (wps):7771.334532828931
step: 170	loss: 31400.885949134827	speed (wps):7771.446068644613
step: 190	loss: 35023.02437543869	speed (wps):7771.823101446777
epoch: 3	train ppl: 187.76230593918626	val ppl: 242.2344244644091	best val: 242.2344244644091	time (s) spent in epoch: 121.79502987861633

EPOCH 4 ------------------
step: 10	loss: 2015.330855846405	speed (wps):7677.73196944691
step: 30	loss: 5654.9080991744995	speed (wps):7742.094807015004
step: 50	loss: 9263.181772232056	speed (wps):7756.257515616647
step: 70	loss: 12875.521743297577	speed (wps):7762.6940338335135
step: 90	loss: 16483.423628807068	speed (wps):7767.072416298336
step: 110	loss: 20076.571254730225	speed (wps):7768.664683891406
step: 130	loss: 23666.725554466248	speed (wps):7770.435779803047
step: 150	loss: 27233.592104911804	speed (wps):7772.427848418312
step: 170	loss: 30771.80547952652	speed (wps):7773.761865482741
step: 190	loss: 34322.42683172226	speed (wps):7774.573918226315
epoch: 4	train ppl: 169.20925249333436	val ppl: 226.5669146951146	best val: 226.5669146951146	time (s) spent in epoch: 121.75327825546265

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=512_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2809.2396306991577	speed (wps):4311.870905115864
step: 30	loss: 7501.9419622421265	speed (wps):4360.3280100604825
step: 50	loss: 12161.527831554413	speed (wps):4359.316730825844
step: 70	loss: 16738.682675361633	speed (wps):4358.5753021104165
step: 90	loss: 21160.10326385498	speed (wps):4358.046978806092
step: 110	loss: 25454.521963596344	speed (wps):4357.873733921689
step: 130	loss: 29666.190922260284	speed (wps):4357.6985613215165
step: 150	loss: 33805.82320213318	speed (wps):4357.754448756795
step: 170	loss: 37866.872391700745	speed (wps):4357.943823505959
step: 190	loss: 41915.666761398315	speed (wps):4357.869338747587

########## Setting Up Experiment ######################

Putting log in ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 2825.3493785858154	speed (wps):4954.893172999222
step: 51	loss: 12477.596831321716	speed (wps):5141.147415869381
step: 92	loss: 21717.25172996521	speed (wps):5164.075939635617
step: 133	loss: 30640.370111465454	speed (wps):5173.177695380482
step: 174	loss: 39253.17618370056	speed (wps):5177.324799240515
step: 215	loss: 47899.47459459305	speed (wps):5179.929309607654
step: 256	loss: 56461.88353538513	speed (wps):5181.894056346132
step: 297	loss: 64905.39140701294	speed (wps):5183.181621317485
step: 338	loss: 73267.82924890518	speed (wps):5184.242034830504
step: 379	loss: 81619.86206293106	speed (wps):5185.131345851033
epoch: 0	train ppl: 448.52762372744183	val ppl: 612.261785277213	best val: 612.261785277213	time (s) spent in epoch: 182.53732466697693

EPOCH 1 ------------------
step: 10	loss: 2227.3148131370544	speed (wps):5107.440079000823
step: 51	loss: 10424.480698108673	speed (wps):5169.781604921314
step: 92	loss: 18568.391942977905	speed (wps):5179.590603058044
step: 133	loss: 26650.724976062775	speed (wps):5183.009962516988
step: 174	loss: 34555.261249542236	speed (wps):5184.218207010623
step: 215	loss: 42590.187957286835	speed (wps):5185.31363060762
step: 256	loss: 50630.14799118042	speed (wps):5186.129947848504
step: 297	loss: 58612.271955013275	speed (wps):5186.054961526003
step: 338	loss: 66552.45737552643	speed (wps):5186.868027468543
step: 379	loss: 74532.40951061249	speed (wps):5187.095663960616
epoch: 1	train ppl: 269.6256083669402	val ppl: 359.35217598834413	best val: 359.35217598834413	time (s) spent in epoch: 182.42796921730042

EPOCH 2 ------------------
step: 10	loss: 2156.2872195243835	speed (wps):5105.784303844696
step: 51	loss: 10042.178874015808	speed (wps):5172.897279180239
step: 92	loss: 17919.90091562271	speed (wps):5179.842391018922
step: 133	loss: 25748.533115386963	speed (wps):5183.729298420473
step: 174	loss: 33418.876061439514	speed (wps):5185.025466009606
step: 215	loss: 41233.352580070496	speed (wps):5186.29630835218
step: 256	loss: 49070.32896280289	speed (wps):5186.909771985212
step: 297	loss: 56861.67249202728	speed (wps):5187.223190200566
step: 338	loss: 64623.668320178986	speed (wps):5188.150245015451
step: 379	loss: 72431.12793922424	speed (wps):5188.669626135676
epoch: 2	train ppl: 231.07657603805407	val ppl: 317.371971065015	best val: 317.371971065015	time (s) spent in epoch: 182.3619556427002

EPOCH 3 ------------------
step: 10	loss: 2116.6010642051697	speed (wps):5107.270717742776
step: 51	loss: 9857.60425567627	speed (wps):5174.843174041176
step: 92	loss: 17596.87735080719	speed (wps):5182.420541577838
step: 133	loss: 25289.764420986176	speed (wps):5185.861503733572
step: 174	loss: 32830.10812997818	speed (wps):5188.327728473743
step: 215	loss: 40514.79384422302	speed (wps):5190.563249449685
step: 256	loss: 48237.87035703659	speed (wps):5192.136143127398
step: 297	loss: 55922.639684677124	speed (wps):5194.562782295815
step: 338	loss: 63578.61374378204	speed (wps):5198.243652592361
step: 379	loss: 71284.32811021805	speed (wps):5201.167001163886
epoch: 3	train ppl: 212.17109327775356	val ppl: 292.5628661408514	best val: 292.5628661408514	time (s) spent in epoch: 181.8748197555542

EPOCH 4 ------------------
step: 10	loss: 2086.433951854706	speed (wps):5147.849014912913
step: 51	loss: 9723.03139925003	speed (wps):5211.212811314486
step: 92	loss: 17369.562368392944	speed (wps):5217.06470814235
step: 133	loss: 24974.196968078613	speed (wps):5219.737889665002
step: 174	loss: 32419.110555648804	speed (wps):5222.105329325252
step: 215	loss: 40023.70466709137	speed (wps):5223.300329062416
step: 256	loss: 47667.41390943527	speed (wps):5223.850033735758
step: 297	loss: 55271.78959131241	speed (wps):5224.518597975817
step: 338	loss: 62841.01281642914	speed (wps):5225.220658857373
step: 379	loss: 70472.15910434723	speed (wps):5225.530572416323
epoch: 4	train ppl: 199.83960293062717	val ppl: 281.72745437593466	best val: 281.72745437593466	time (s) spent in epoch: 181.1011393070221

DONE

Saving learning curves to ./results/TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:05:00.0.
All done.
