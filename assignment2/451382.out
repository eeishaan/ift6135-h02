
########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3404.588236808777	speed (wps):3050.0843967356655
step: 92	loss: 24107.516884803772	speed (wps):3178.5311833473575
step: 174	loss: 43444.39378976822	speed (wps):3186.627782286669
step: 256	loss: 62597.009534835815	speed (wps):3189.46001326287
step: 338	loss: 81396.4646267891	speed (wps):3191.029768266309
step: 420	loss: 99986.54927253723	speed (wps):3191.908337851805
step: 502	loss: 118471.42545223236	speed (wps):3192.524477830874
step: 584	loss: 136628.08270454407	speed (wps):3192.9150261352656
step: 666	loss: 154869.90186929703	speed (wps):3193.1284387937776
step: 748	loss: 172973.78513336182	speed (wps):3193.374513011367
epoch: 0	train ppl: 713.9391104491444	val ppl: 425.0734213288718	best val: 425.0734213288718	time (s) spent in epoch: 296.35435938835144

EPOCH 1 ------------------
step: 10	loss: 2467.132968902588	speed (wps):3138.169774137735
step: 92	loss: 20326.169273853302	speed (wps):3189.504081311394
step: 174	loss: 38015.18933773041	speed (wps):3192.6958545971993
step: 256	loss: 55866.54903173447	speed (wps):3193.6116600735622
step: 338	loss: 73547.25273370743	speed (wps):3194.2441736306446
step: 420	loss: 91130.67043066025	speed (wps):3194.713442952234
step: 502	loss: 108712.5417470932	speed (wps):3194.971379691094
step: 584	loss: 126031.65930271149	speed (wps):3195.0745512749927
step: 666	loss: 143497.5604915619	speed (wps):3195.208024009469
step: 748	loss: 160864.6019077301	speed (wps):3195.3294553184596
epoch: 1	train ppl: 459.8868202558543	val ppl: 342.88212875847853	best val: 342.88212875847853	time (s) spent in epoch: 296.18040227890015

EPOCH 2 ------------------
step: 10	loss: 2396.586410999298	speed (wps):3137.0948778620914
step: 92	loss: 19668.28388929367	speed (wps):3189.7215127867157
step: 174	loss: 36811.856372356415	speed (wps):3192.959486667817
step: 256	loss: 54128.30998182297	speed (wps):3193.9838456526118
step: 338	loss: 71292.94516324997	speed (wps):3194.388704157838
step: 420	loss: 88364.4355392456	speed (wps):3194.754320926469
step: 502	loss: 105465.00278234482	speed (wps):3194.9780196104084
step: 584	loss: 122314.41816568375	speed (wps):3195.1706284351726
step: 666	loss: 139320.1754617691	speed (wps):3195.3481707514484
step: 748	loss: 156253.3765411377	speed (wps):3195.3182862247745
epoch: 2	train ppl: 387.2312908765265	val ppl: 299.1885631725845	best val: 299.1885631725845	time (s) spent in epoch: 296.1870400905609

EPOCH 3 ------------------
step: 10	loss: 2352.103600502014	speed (wps):3138.1991240657458
step: 92	loss: 19252.619502544403	speed (wps):3190.3316994518987
step: 174	loss: 36023.31893205643	speed (wps):3193.6474911337123
step: 256	loss: 52989.78379011154	speed (wps):3195.0713278106546
step: 338	loss: 69839.92272138596	speed (wps):3196.7747026463167
step: 420	loss: 86586.69331550598	speed (wps):3197.740698331963
step: 502	loss: 103356.84106349945	speed (wps):3198.439442768284
step: 584	loss: 119900.12669324875	speed (wps):3198.987021659689
step: 666	loss: 136600.35014390945	speed (wps):3199.3540839288084
step: 748	loss: 153233.3372759819	speed (wps):3199.6178268060485
epoch: 3	train ppl: 345.7754366105285	val ppl: 279.08599585263664	best val: 279.08599585263664	time (s) spent in epoch: 295.7668800354004

EPOCH 4 ------------------
step: 10	loss: 2313.5142993927	speed (wps):3147.369000387499
step: 92	loss: 18923.460566997528	speed (wps):3196.511728481015
step: 174	loss: 35414.33638572693	speed (wps):3199.6904322344208
step: 256	loss: 52115.69757938385	speed (wps):3200.795222040117
step: 338	loss: 68708.78965616226	speed (wps):3201.2756788537495
step: 420	loss: 85208.9854645729	speed (wps):3201.5720461031256
step: 502	loss: 101739.2049908638	speed (wps):3201.750249320611
step: 584	loss: 118034.47501420975	speed (wps):3201.9013606612816
step: 666	loss: 134477.75448322296	speed (wps):3201.972542864483
step: 748	loss: 150863.5009598732	speed (wps):3201.9916321391756
epoch: 4	train ppl: 316.31683500735517	val ppl: 259.45484317040246	best val: 259.45484317040246	time (s) spent in epoch: 295.56580805778503

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=32_seq_len=35_hidden_size=1800_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3729.4715070724487	speed (wps):5310.656062584434
step: 92	loss: 24054.5548081398	speed (wps):5606.864829097086
step: 174	loss: 42952.89579153061	speed (wps):5626.704537610658
step: 256	loss: 61674.86770153046	speed (wps):5633.688069430883
step: 338	loss: 80045.88403224945	speed (wps):5637.512600804899
step: 420	loss: 98163.79013061523	speed (wps):5640.1538030286265
step: 502	loss: 116159.13998603821	speed (wps):5641.6214572283125
step: 584	loss: 133784.71539020538	speed (wps):5642.713882515453
step: 666	loss: 151494.9298644066	speed (wps):5643.4515044918335
step: 748	loss: 169048.34829330444	speed (wps):5643.980085430965
epoch: 0	train ppl: 612.3089019941061	val ppl: 353.060709918806	best val: 353.060709918806	time (s) spent in epoch: 167.7135305404663

EPOCH 1 ------------------
step: 10	loss: 2395.4984188079834	speed (wps):5461.475905597762
step: 92	loss: 19659.16784286499	speed (wps):5627.039139774532
step: 174	loss: 36757.221426963806	speed (wps):5637.217724537194
step: 256	loss: 54005.30259370804	speed (wps):5641.355749288241
step: 338	loss: 71078.64162683487	speed (wps):5643.267691242433
step: 420	loss: 88015.99734067917	speed (wps):5644.392173976574
step: 502	loss: 104973.81308555603	speed (wps):5644.643683762493
step: 584	loss: 121649.18396472931	speed (wps):5645.062670921283
step: 666	loss: 138469.49983358383	speed (wps):5645.357228408028
step: 748	loss: 155176.41425848007	speed (wps):5645.723691087923
epoch: 1	train ppl: 370.5547374444483	val ppl: 285.7357259453933	best val: 285.7357259453933	time (s) spent in epoch: 167.67238116264343

EPOCH 2 ------------------
step: 10	loss: 2319.6355843544006	speed (wps):5459.827832156587
step: 92	loss: 18961.256976127625	speed (wps):5624.607724547693
step: 174	loss: 35448.32536458969	speed (wps):5635.767967214134
step: 256	loss: 52126.409606933594	speed (wps):5639.469222266414
step: 338	loss: 68676.15823507309	speed (wps):5641.758316082175
step: 420	loss: 85111.96825742722	speed (wps):5642.6999736207745
step: 502	loss: 101569.17808532715	speed (wps):5643.603497318512
step: 584	loss: 117767.97166824341	speed (wps):5644.130006004888
step: 666	loss: 134133.7465929985	speed (wps):5644.469682722641
step: 748	loss: 150404.30548667908	speed (wps):5644.792608284905
epoch: 2	train ppl: 310.10466436608544	val ppl: 249.19482025338786	best val: 249.19482025338786	time (s) spent in epoch: 167.69882607460022

EPOCH 3 ------------------
step: 10	loss: 2270.1754236221313	speed (wps):5459.0831785633545
step: 92	loss: 18524.750373363495	speed (wps):5625.475597527961
step: 174	loss: 34641.116886138916	speed (wps):5635.968764770204
step: 256	loss: 50958.50707530975	speed (wps):5639.527309203668
step: 338	loss: 67182.71866321564	speed (wps):5641.387238685201
step: 420	loss: 83277.29485750198	speed (wps):5642.469276417614
step: 502	loss: 99402.9054737091	speed (wps):5642.999393172382
step: 584	loss: 115288.64384412766	speed (wps):5643.780365086982
step: 666	loss: 131338.25665950775	speed (wps):5644.085870812335
step: 748	loss: 147309.61661577225	speed (wps):5644.2850562776175
epoch: 3	train ppl: 276.1242706415008	val ppl: 227.49004924180673	best val: 227.49004924180673	time (s) spent in epoch: 167.71346426010132

EPOCH 4 ------------------
step: 10	loss: 2234.0200066566467	speed (wps):5459.617278203486
step: 92	loss: 18223.04987668991	speed (wps):5625.13342819229
step: 174	loss: 34060.23419857025	speed (wps):5635.669640745451
step: 256	loss: 50105.669531822205	speed (wps):5639.305610572747
step: 338	loss: 66076.52119874954	speed (wps):5641.348029286459
step: 420	loss: 81935.40202856064	speed (wps):5642.245648538591
step: 502	loss: 97819.8511338234	speed (wps):5643.110750914787
step: 584	loss: 113442.49509811401	speed (wps):5643.659802103357
step: 666	loss: 129250.18748998642	speed (wps):5643.803350113912
step: 748	loss: 144994.91806030273	speed (wps):5643.854173700229
epoch: 4	train ppl: 253.13530691318638	val ppl: 215.14551037049347	best val: 215.14551037049347	time (s) spent in epoch: 167.72867846488953

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3498.6992359161377	speed (wps):4329.435812466109
step: 92	loss: 23657.168159484863	speed (wps):4508.446873320182
step: 174	loss: 42449.36472415924	speed (wps):4520.2620075575605
step: 256	loss: 61076.05004072189	speed (wps):4524.752656865291
step: 338	loss: 79347.79542446136	speed (wps):4527.207933286626
step: 420	loss: 97383.24521303177	speed (wps):4528.558360074299
step: 502	loss: 115317.29533433914	speed (wps):4529.6556089475325
step: 584	loss: 132901.72638893127	speed (wps):4530.153206778102
step: 666	loss: 150552.79019355774	speed (wps):4530.783362721174
step: 748	loss: 168035.55211544037	speed (wps):4531.087987604665
epoch: 0	train ppl: 590.4635959516762	val ppl: 342.2763199430002	best val: 342.2763199430002	time (s) spent in epoch: 208.93831086158752

EPOCH 1 ------------------
step: 10	loss: 2389.8350381851196	speed (wps):4418.941160856364
step: 92	loss: 19607.7619433403	speed (wps):4520.185740785141
step: 174	loss: 36652.41672515869	speed (wps):4526.286442020326
step: 256	loss: 53851.88309431076	speed (wps):4528.7490832177655
step: 338	loss: 70902.64736175537	speed (wps):4530.308541501905
step: 420	loss: 87796.58375740051	speed (wps):4531.036845789213
step: 502	loss: 104725.31098604202	speed (wps):4531.476749798856
step: 584	loss: 121384.26591873169	speed (wps):4531.529235993365
step: 666	loss: 138184.2414855957	speed (wps):4531.79150735604
step: 748	loss: 154872.63360738754	speed (wps):4532.004825831128
epoch: 1	train ppl: 366.69683235843223	val ppl: 279.2131394110357	best val: 279.2131394110357	time (s) spent in epoch: 208.9062819480896

EPOCH 2 ------------------
step: 10	loss: 2314.676809310913	speed (wps):4414.423569961213
step: 92	loss: 18936.93577528	speed (wps):4520.361690019748
step: 174	loss: 35389.10930633545	speed (wps):4526.252848558394
step: 256	loss: 52029.00263786316	speed (wps):4528.821470778706
step: 338	loss: 68562.57527828217	speed (wps):4529.868608485477
step: 420	loss: 84958.40744972229	speed (wps):4530.529224567326
step: 502	loss: 101406.4407992363	speed (wps):4531.016359653271
step: 584	loss: 117579.22938108444	speed (wps):4529.73594521557
step: 666	loss: 133923.19429397583	speed (wps):4528.881308539151
step: 748	loss: 150170.55248975754	speed (wps):4528.098769684381
epoch: 2	train ppl: 307.5037697922364	val ppl: 247.4814899359291	best val: 247.4814899359291	time (s) spent in epoch: 209.13249731063843

EPOCH 3 ------------------
step: 10	loss: 2266.06760263443	speed (wps):4397.3094304880415
step: 92	loss: 18497.657396793365	speed (wps):4506.944454055739
step: 174	loss: 34575.42743206024	speed (wps):4514.989763274157
step: 256	loss: 50846.65917158127	speed (wps):4517.515064427123
step: 338	loss: 67057.97433614731	speed (wps):4518.884161641784
step: 420	loss: 83129.58997488022	speed (wps):4519.747543865689
step: 502	loss: 99232.48413324356	speed (wps):4520.47864905494
step: 584	loss: 115086.01814508438	speed (wps):4520.948244663493
step: 666	loss: 131110.38387060165	speed (wps):4521.309807786912
step: 748	loss: 147066.56172037125	speed (wps):4521.393680108863
epoch: 3	train ppl: 273.7148354175549	val ppl: 226.16968449608027	best val: 226.16968449608027	time (s) spent in epoch: 209.3963177204132

EPOCH 4 ------------------
step: 10	loss: 2229.880976676941	speed (wps):4403.810644535669
step: 92	loss: 18169.64030981064	speed (wps):4509.93786398374
step: 174	loss: 33972.62009382248	speed (wps):4516.42737538374
step: 256	loss: 49980.967128276825	speed (wps):4518.898095489897
step: 338	loss: 65913.59821081161	speed (wps):4520.225856349512
step: 420	loss: 81722.85824537277	speed (wps):4521.017322669377
step: 502	loss: 97578.58060836792	speed (wps):4521.388115189977
step: 584	loss: 113184.38363313675	speed (wps):4521.671780918565
step: 666	loss: 128957.38986730576	speed (wps):4521.731229673029
step: 748	loss: 144676.1417222023	speed (wps):4522.052910780274
epoch: 4	train ppl: 250.0060744484305	val ppl: 212.39880054398978	best val: 212.39880054398978	time (s) spent in epoch: 209.37010622024536

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1800_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3278.749041557312	speed (wps):3844.92056254457
step: 92	loss: 23422.91855573654	speed (wps):4013.5930148942793
step: 174	loss: 42374.91817712784	speed (wps):4025.428465128612
step: 256	loss: 61210.98914384842	speed (wps):4029.3536911323463
step: 338	loss: 79698.29331636429	speed (wps):4031.21063327898
step: 420	loss: 97959.31934595108	speed (wps):4032.4779127231245
step: 502	loss: 116115.24492263794	speed (wps):4033.3305392568996
step: 584	loss: 133947.1287035942	speed (wps):4033.963363939128
step: 666	loss: 151862.30531215668	speed (wps):4034.4194817700322
step: 748	loss: 169622.24387168884	speed (wps):4034.6768544811025
epoch: 0	train ppl: 628.8402616898876	val ppl: 374.2758406919904	best val: 374.2758406919904	time (s) spent in epoch: 234.49948573112488

EPOCH 1 ------------------
step: 10	loss: 2423.4910321235657	speed (wps):3940.5098948891323
step: 92	loss: 19914.177901744843	speed (wps):4026.6438066761725
step: 174	loss: 37228.11004161835	speed (wps):4032.1755493909654
step: 256	loss: 54702.23116159439	speed (wps):4033.977984835092
step: 338	loss: 72016.95400714874	speed (wps):4034.9871530286105
step: 420	loss: 89208.08074951172	speed (wps):4035.450994609892
step: 502	loss: 106389.59814310074	speed (wps):4035.6823077824197
step: 584	loss: 123305.11003255844	speed (wps):4035.9208406674893
step: 666	loss: 140375.71046829224	speed (wps):4036.2504273434897
step: 748	loss: 157339.56834077835	speed (wps):4036.454845643418
epoch: 1	train ppl: 402.35378285932876	val ppl: 301.0922233786627	best val: 301.0922233786627	time (s) spent in epoch: 234.40850400924683

EPOCH 2 ------------------
step: 10	loss: 2345.072114467621	speed (wps):3939.1791608378066
step: 92	loss: 19227.48403072357	speed (wps):4026.5029679326353
step: 174	loss: 35955.06726980209	speed (wps):4031.5381349438912
step: 256	loss: 52875.21786689758	speed (wps):4033.758613408204
step: 338	loss: 69676.90393686295	speed (wps):4034.76859309519
step: 420	loss: 86366.23119354248	speed (wps):4035.4405947769355
step: 502	loss: 103088.07903766632	speed (wps):4035.564666973076
step: 584	loss: 119560.02022743225	speed (wps):4036.0346940359145
step: 666	loss: 136176.06142759323	speed (wps):4036.215976475258
step: 748	loss: 152705.88007688522	speed (wps):4036.2946228073874
epoch: 2	train ppl: 338.45647180135194	val ppl: 271.17106589686836	best val: 271.17106589686836	time (s) spent in epoch: 234.4136037826538

EPOCH 3 ------------------
step: 10	loss: 2302.3460698127747	speed (wps):3937.6320740777824
step: 92	loss: 18820.98862886429	speed (wps):4026.190634259152
step: 174	loss: 35207.58415699005	speed (wps):4031.9131835099865
step: 256	loss: 51781.51404619217	speed (wps):4033.827390171568
step: 338	loss: 68255.67356109619	speed (wps):4034.834090095706
step: 420	loss: 84618.68410348892	speed (wps):4035.41463248252
step: 502	loss: 101016.50931119919	speed (wps):4035.6382020139627
step: 584	loss: 117157.36817359924	speed (wps):4035.965218296392
step: 666	loss: 133474.1471004486	speed (wps):4036.225517216739
step: 748	loss: 149707.565741539	speed (wps):4036.31615829107
epoch: 3	train ppl: 302.5720662159675	val ppl: 245.85327616077177	best val: 245.85327616077177	time (s) spent in epoch: 234.41118097305298

EPOCH 4 ------------------
step: 10	loss: 2268.444776535034	speed (wps):3937.5720641599187
step: 92	loss: 18518.39809179306	speed (wps):4025.9304735927603
step: 174	loss: 34633.437638282776	speed (wps):4031.4705592759283
step: 256	loss: 50946.3375043869	speed (wps):4032.804619879212
step: 338	loss: 67164.69642162323	speed (wps):4033.6735227152276
step: 420	loss: 83273.32615852356	speed (wps):4034.0466625248623
step: 502	loss: 99421.01633548737	speed (wps):4034.311392514874
step: 584	loss: 115321.78500652313	speed (wps):4034.2975879926544
step: 666	loss: 131386.89455509186	speed (wps):4034.450961743404
step: 748	loss: 147398.45668315887	speed (wps):4034.5164257980773
epoch: 4	train ppl: 277.24458593452357	val ppl: 228.86336996792244	best val: 228.86336996792244	time (s) spent in epoch: 234.53231954574585

DONE

Saving learning curves to ./results/RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=32_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:04:00.0.
All done.
