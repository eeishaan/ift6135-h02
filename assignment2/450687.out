
########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3496.0690021514893	speed (wps):8073.621426901871
step: 51	loss: 14279.210479259491	speed (wps):8458.535280488286
step: 92	loss: 24253.735218048096	speed (wps):8506.233298158633
step: 133	loss: 34003.97665262222	speed (wps):8526.159760517581
step: 174	loss: 43528.811559677124	speed (wps):8533.697134982085
step: 215	loss: 53084.86520528793	speed (wps):8537.381937948365
step: 256	loss: 62545.91637611389	speed (wps):8542.118692801512
step: 297	loss: 71901.33662462234	speed (wps):8545.97886182743
step: 338	loss: 81181.92934274673	speed (wps):8547.73087214336
step: 379	loss: 90438.9033293724	speed (wps):8548.907303417824
epoch: 0	train ppl: 868.077886142747	val ppl: 481.50394468852147	best val: 481.50394468852147	time (s) spent in epoch: 110.92899298667908

EPOCH 1 ------------------
step: 10	loss: 2477.1965670585632	speed (wps):8348.087926144843
step: 51	loss: 11584.435918331146	speed (wps):8516.596179494436
step: 92	loss: 20664.356186389923	speed (wps):8539.292573180017
step: 133	loss: 29684.625055789948	speed (wps):8548.052880178537
step: 174	loss: 38548.946981430054	speed (wps):8552.193172903046
step: 215	loss: 47539.889245033264	speed (wps):8554.225759572515
step: 256	loss: 56508.190569877625	speed (wps):8556.615164890874
step: 297	loss: 65411.28284215927	speed (wps):8556.896459073761
step: 338	loss: 74274.37938928604	speed (wps):8558.224880774764
step: 379	loss: 83139.69150781631	speed (wps):8558.54148089645
epoch: 1	train ppl: 513.8858317031501	val ppl: 382.1668075009447	best val: 382.1668075009447	time (s) spent in epoch: 110.82092785835266

EPOCH 2 ------------------
step: 10	loss: 2389.4813752174377	speed (wps):8323.498245703335
step: 51	loss: 11164.405961036682	speed (wps):8513.342797317135
step: 92	loss: 19918.02795648575	speed (wps):8539.444631091692
step: 133	loss: 28641.336975097656	speed (wps):8545.671613521088
step: 174	loss: 37214.32264328003	speed (wps):8552.58154050862
step: 215	loss: 45923.59908103943	speed (wps):8554.938249770423
step: 256	loss: 54623.51045131683	speed (wps):8558.181631663578
step: 297	loss: 63268.85174751282	speed (wps):8559.637460586027
step: 338	loss: 71864.46700572968	speed (wps):8561.008487682711
step: 379	loss: 80499.95615720749	speed (wps):8561.23020070557
epoch: 2	train ppl: 422.4552008573275	val ppl: 325.85781300113433	best val: 325.85781300113433	time (s) spent in epoch: 110.78637075424194

EPOCH 3 ------------------
step: 10	loss: 2332.1202158927917	speed (wps):8351.037144819325
step: 51	loss: 10894.307956695557	speed (wps):8518.429212228533
step: 92	loss: 19446.388590335846	speed (wps):8537.84921075472
step: 133	loss: 27964.80022907257	speed (wps):8545.197375401453
step: 174	loss: 36323.85626792908	speed (wps):8548.48708280495
step: 215	loss: 44835.45523405075	speed (wps):8551.032119284522
step: 256	loss: 53360.00109195709	speed (wps):8552.867356770097
step: 297	loss: 61825.80226421356	speed (wps):8554.869672368577
step: 338	loss: 70256.83586835861	speed (wps):8554.788024453237
step: 379	loss: 78729.58292722702	speed (wps):8555.441542427972
epoch: 3	train ppl: 370.53762549915757	val ppl: 297.11084876252363	best val: 297.11084876252363	time (s) spent in epoch: 110.85685968399048

EPOCH 4 ------------------
step: 10	loss: 2293.9256501197815	speed (wps):8334.699676645012
step: 51	loss: 10715.42183637619	speed (wps):8506.802602073876
step: 92	loss: 19123.250002861023	speed (wps):8536.038396968346
step: 133	loss: 27492.66842842102	speed (wps):8539.992859429576
step: 174	loss: 35715.718150138855	speed (wps):8544.542322630505
step: 215	loss: 44101.788992881775	speed (wps):8547.685220126334
step: 256	loss: 52495.60295343399	speed (wps):8549.885865829288
step: 297	loss: 60832.28980541229	speed (wps):8549.812908735428
step: 338	loss: 69137.49973535538	speed (wps):8552.16068501434
step: 379	loss: 77485.3765130043	speed (wps):8553.248384888366
epoch: 4	train ppl: 337.76519354672405	val ppl: 276.18572578703754	best val: 276.18572578703754	time (s) spent in epoch: 110.87968182563782

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3426.492943763733	speed (wps):9397.264789116856
step: 51	loss: 14449.392628669739	speed (wps):9911.944528618722
step: 92	loss: 24583.776400089264	speed (wps):9977.07911008155
step: 133	loss: 34484.54130649567	speed (wps):10004.091220878412
step: 174	loss: 44145.69178342819	speed (wps):10017.950769032605
step: 215	loss: 53848.64542007446	speed (wps):10025.987483132663
step: 256	loss: 63447.293598651886	speed (wps):10030.923076122252
step: 297	loss: 72954.9054980278	speed (wps):10035.764754725757
step: 338	loss: 82378.96536827087	speed (wps):10039.46413359179
step: 379	loss: 91783.60266447067	speed (wps):10041.235071906896
epoch: 0	train ppl: 960.9016783092367	val ppl: 525.3684554571053	best val: 525.3684554571053	time (s) spent in epoch: 94.46394181251526

EPOCH 1 ------------------
step: 10	loss: 2522.0179533958435	speed (wps):9679.062756966929
step: 51	loss: 11820.307338237762	speed (wps):9979.736060788258
step: 92	loss: 21062.21609354019	speed (wps):10016.013555864436
step: 133	loss: 30258.221027851105	speed (wps):10031.258853926776
step: 174	loss: 39306.21623277664	speed (wps):10038.012530537273
step: 215	loss: 48495.47740459442	speed (wps):10044.074402285698
step: 256	loss: 57634.75377082825	speed (wps):10047.448815642429
step: 297	loss: 66727.97302484512	speed (wps):10051.254248480216
step: 338	loss: 75767.54999637604	speed (wps):10051.7454830849
step: 379	loss: 84825.40492296219	speed (wps):10053.923747884992
epoch: 1	train ppl: 583.1934746422169	val ppl: 422.204963128285	best val: 422.204963128285	time (s) spent in epoch: 94.34823536872864

EPOCH 2 ------------------
step: 10	loss: 2437.804193496704	speed (wps):9673.259223968887
step: 51	loss: 11408.65796327591	speed (wps):9976.570749274872
step: 92	loss: 20358.75024318695	speed (wps):10011.850025809443
step: 133	loss: 29258.14283132553	speed (wps):10026.774828463744
step: 174	loss: 38026.0705947876	speed (wps):10035.26578293122
step: 215	loss: 46933.5085272789	speed (wps):10041.95633740393
step: 256	loss: 55828.73760223389	speed (wps):10045.381907594234
step: 297	loss: 64674.21643972397	speed (wps):10046.511856010611
step: 338	loss: 73480.82547903061	speed (wps):10048.311935029722
step: 379	loss: 82311.03949785233	speed (wps):10049.744819706302
epoch: 2	train ppl: 484.00118143897953	val ppl: 365.95914433369126	best val: 365.95914433369126	time (s) spent in epoch: 94.39310216903687

EPOCH 3 ------------------
step: 10	loss: 2380.935468673706	speed (wps):9683.833277237349
step: 51	loss: 11148.927502632141	speed (wps):9981.117183082017
step: 92	loss: 19898.893172740936	speed (wps):10015.466032675944
step: 133	loss: 28609.88583803177	speed (wps):10028.35359874186
step: 174	loss: 37181.15417718887	speed (wps):10036.325656251582
step: 215	loss: 45901.33560419083	speed (wps):10041.385473578603
step: 256	loss: 54623.80655288696	speed (wps):10045.857772427036
step: 297	loss: 63289.648501873016	speed (wps):10046.753217195968
step: 338	loss: 71917.6762509346	speed (wps):10048.937759862753
step: 379	loss: 80575.42459011078	speed (wps):10050.068711616797
epoch: 3	train ppl: 425.2940212914093	val ppl: 327.70139047166106	best val: 327.70139047166106	time (s) spent in epoch: 94.38718056678772

EPOCH 4 ------------------
step: 10	loss: 2339.1023182868958	speed (wps):9673.031066157922
step: 51	loss: 10951.015496253967	speed (wps):9982.452386625377
step: 92	loss: 19539.45684194565	speed (wps):10017.459286381169
step: 133	loss: 28101.23991727829	speed (wps):10027.049542053559
step: 174	loss: 36526.9992518425	speed (wps):10036.375280127713
step: 215	loss: 45092.98634529114	speed (wps):10041.49180068046
step: 256	loss: 53679.767656326294	speed (wps):10045.384331523583
step: 297	loss: 62211.64974689484	speed (wps):10048.26271765964
step: 338	loss: 70702.76082754135	speed (wps):10051.267312125881
step: 379	loss: 79231.64341688156	speed (wps):10052.701981460608
epoch: 4	train ppl: 385.07109535042133	val ppl: 302.6284956920181	best val: 302.6284956920181	time (s) spent in epoch: 94.3638687133789

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1024_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3499.8252964019775	speed (wps):6198.432595176292
step: 51	loss: 14272.426896095276	speed (wps):6388.304573013844
step: 92	loss: 24256.01480960846	speed (wps):6409.6396853530705
step: 133	loss: 34028.592586517334	speed (wps):6418.498116282616
step: 174	loss: 43585.14303922653	speed (wps):6423.488487171096
step: 215	loss: 53159.886598587036	speed (wps):6425.32453721496
step: 256	loss: 62641.19217157364	speed (wps):6426.402800897216
step: 297	loss: 72033.11106920242	speed (wps):6427.986168136511
step: 338	loss: 81334.70680475235	speed (wps):6429.355853083748
step: 379	loss: 90625.83943367004	speed (wps):6430.821512136229
epoch: 0	train ppl: 880.9524750937394	val ppl: 489.18231620512125	best val: 489.18231620512125	time (s) spent in epoch: 147.40629076957703

EPOCH 1 ------------------
step: 10	loss: 2485.61368227005	speed (wps):6315.992117022624
step: 51	loss: 11633.492364883423	speed (wps):6413.263070255656
step: 92	loss: 20752.924201488495	speed (wps):6425.384223356772
step: 133	loss: 29816.672337055206	speed (wps):6430.69475644904
step: 174	loss: 38735.613753795624	speed (wps):6432.667844476378
step: 215	loss: 47776.94759130478	speed (wps):6434.704764693725
step: 256	loss: 56797.48392343521	speed (wps):6434.985570940703
step: 297	loss: 65756.11728668213	speed (wps):6435.1060139465135
step: 338	loss: 74662.03091859818	speed (wps):6435.107651476814
step: 379	loss: 83599.1500878334	speed (wps):6435.1805254271885
epoch: 1	train ppl: 532.0327602903832	val ppl: 387.39100204435033	best val: 387.39100204435033	time (s) spent in epoch: 147.32253742218018

EPOCH 2 ------------------
step: 10	loss: 2401.4068484306335	speed (wps):6317.407494638945
step: 51	loss: 11243.766422271729	speed (wps):6413.767813145193
step: 92	loss: 20073.995044231415	speed (wps):6426.256020707292
step: 133	loss: 28862.3969745636	speed (wps):6431.38029233333
step: 174	loss: 37504.82640504837	speed (wps):6434.460984376271
step: 215	loss: 46284.16973352432	speed (wps):6436.113313436312
step: 256	loss: 55056.49107217789	speed (wps):6437.204499957663
step: 297	loss: 63769.68033313751	speed (wps):6437.86001282094
step: 338	loss: 72442.16263771057	speed (wps):6438.594234193533
step: 379	loss: 81152.10779905319	speed (wps):6439.486041944536
epoch: 2	train ppl: 443.76193813657045	val ppl: 335.1557075422236	best val: 335.1557075422236	time (s) spent in epoch: 147.22955799102783

EPOCH 3 ------------------
step: 10	loss: 2349.884407520294	speed (wps):6313.610263677265
step: 51	loss: 10990.733761787415	speed (wps):6413.720240157429
step: 92	loss: 19621.322083473206	speed (wps):6426.689833691034
step: 133	loss: 28217.97256231308	speed (wps):6430.882781360533
step: 174	loss: 36667.801756858826	speed (wps):6432.385152843147
step: 215	loss: 45260.48201799393	speed (wps):6433.035390024484
step: 256	loss: 53870.79636096954	speed (wps):6433.926997125172
step: 297	loss: 62426.626496315	speed (wps):6435.195942347919
step: 338	loss: 70935.30339956284	speed (wps):6435.978175787652
step: 379	loss: 79495.78717947006	speed (wps):6436.815437552876
epoch: 3	train ppl: 392.8048528006083	val ppl: 308.16218238469844	best val: 308.16218238469844	time (s) spent in epoch: 147.2906699180603

EPOCH 4 ------------------
step: 10	loss: 2313.9112877845764	speed (wps):6311.987635429873
step: 51	loss: 10815.997703075409	speed (wps):6413.566664213538
step: 92	loss: 19305.501370429993	speed (wps):6423.2453338909145
step: 133	loss: 27763.00517320633	speed (wps):6429.36345786488
step: 174	loss: 36077.36434459686	speed (wps):6432.6212098444985
step: 215	loss: 44533.74839544296	speed (wps):6433.99057320211
step: 256	loss: 53011.85695886612	speed (wps):6435.290608265104
step: 297	loss: 61444.38322067261	speed (wps):6435.914927455748
step: 338	loss: 69834.84179735184	speed (wps):6436.264995196081
step: 379	loss: 78283.79924297333	speed (wps):6437.029780124466
epoch: 4	train ppl: 358.7266439110007	val ppl: 283.70523459385913	best val: 283.70523459385913	time (s) spent in epoch: 147.27504110336304

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3457.5050354003906	speed (wps):16064.983060656366
step: 30	loss: 9078.795108795166	speed (wps):16621.192217592932
step: 50	loss: 14250.041053295135	speed (wps):16744.06029824766
step: 70	loss: 19219.632065296173	speed (wps):16800.665732020407
step: 90	loss: 24083.455436229706	speed (wps):16831.982534695577
step: 110	loss: 28871.4048743248	speed (wps):16851.97181871289
step: 130	loss: 33612.897424697876	speed (wps):16866.025995702876
step: 150	loss: 38307.83956050873	speed (wps):16876.249307571918
step: 170	loss: 42951.18258476257	speed (wps):16884.378732709614
step: 190	loss: 47607.88457632065	speed (wps):16890.154141146024
epoch: 0	train ppl: 1193.0828315806568	val ppl: 587.2847165348542	best val: 587.2847165348542	time (s) spent in epoch: 56.27603816986084

EPOCH 1 ------------------
step: 10	loss: 2562.111761569977	speed (wps):16480.939015471544
step: 30	loss: 7178.555188179016	speed (wps):16777.243060069453
step: 50	loss: 11783.63121509552	speed (wps):16839.912835139567
step: 70	loss: 16375.522828102112	speed (wps):16869.803259127148
step: 90	loss: 20948.166387081146	speed (wps):16886.090193143264
step: 110	loss: 25501.294813156128	speed (wps):16896.341459150808
step: 130	loss: 30043.799164295197	speed (wps):16903.0375247505
step: 150	loss: 34556.981399059296	speed (wps):16909.114189466316
step: 170	loss: 39039.71320390701	speed (wps):16912.755237285543
step: 190	loss: 43540.38687467575	speed (wps):16916.38665116303
epoch: 1	train ppl: 669.006703701895	val ppl: 492.31600148800857	best val: 492.31600148800857	time (s) spent in epoch: 56.194993019104004

EPOCH 2 ------------------
step: 10	loss: 2479.2538738250732	speed (wps):16481.66181030013
step: 30	loss: 6947.08455324173	speed (wps):16781.29387374461
step: 50	loss: 11409.760091304779	speed (wps):16844.49222960084
step: 70	loss: 15860.660910606384	speed (wps):16872.82798463476
step: 90	loss: 20292.920515537262	speed (wps):16888.103332879575
step: 110	loss: 24698.163664340973	speed (wps):16899.24425744347
step: 130	loss: 29098.235116004944	speed (wps):16905.92398413129
step: 150	loss: 33474.490485191345	speed (wps):16910.807076205332
step: 170	loss: 37824.250655174255	speed (wps):16915.122203937008
step: 190	loss: 42195.23215532303	speed (wps):16918.768324098284
epoch: 2	train ppl: 547.9863996835544	val ppl: 417.7120186262783	best val: 417.7120186262783	time (s) spent in epoch: 56.189401388168335

EPOCH 3 ------------------
step: 10	loss: 2415.21906375885	speed (wps):16481.318803395563
step: 30	loss: 6773.859326839447	speed (wps):16777.31264350601
step: 50	loss: 11127.269496917725	speed (wps):16845.345868906566
step: 70	loss: 15469.738578796387	speed (wps):16874.362844301395
step: 90	loss: 19794.613316059113	speed (wps):16889.98615540459
step: 110	loss: 24097.52767086029	speed (wps):16899.46799117749
step: 130	loss: 28392.50122308731	speed (wps):16906.336764294378
step: 150	loss: 32672.230784893036	speed (wps):16911.331296850305
step: 170	loss: 36920.90441226959	speed (wps):16915.31214199028
step: 190	loss: 41196.41850709915	speed (wps):16918.5125488026
epoch: 3	train ppl: 472.6370386804613	val ppl: 369.5265288463244	best val: 369.5265288463244	time (s) spent in epoch: 56.18926525115967

EPOCH 4 ------------------
step: 10	loss: 2367.7095651626587	speed (wps):16480.81680370295
step: 30	loss: 6643.107652664185	speed (wps):16775.5302331514
step: 50	loss: 10909.645073413849	speed (wps):16843.328423070132
step: 70	loss: 15165.194091796875	speed (wps):16872.58344018308
step: 90	loss: 19406.564881801605	speed (wps):16889.19490844364
step: 110	loss: 23631.0591340065	speed (wps):16899.267397316326
step: 130	loss: 27853.323211669922	speed (wps):16906.62078733055
step: 150	loss: 32056.546666622162	speed (wps):16911.557484761724
step: 170	loss: 36233.316695690155	speed (wps):16915.330931218494
step: 190	loss: 40436.76120042801	speed (wps):16918.180538960587
epoch: 4	train ppl: 422.2702866188039	val ppl: 332.9678747541152	best val: 332.9678747541152	time (s) spent in epoch: 56.19050574302673

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3460.472927093506	speed (wps):11922.549377031191
step: 51	loss: 14000.67521572113	speed (wps):12749.2555253599
step: 92	loss: 23843.035881519318	speed (wps):12845.713204341757
step: 133	loss: 33462.4050784111	speed (wps):12886.20442224589
step: 174	loss: 42818.39993953705	speed (wps):12903.785361233546
step: 215	loss: 52216.13248825073	speed (wps):12915.503841801165
step: 256	loss: 61500.01795530319	speed (wps):12923.934232614241
step: 297	loss: 70682.40705728531	speed (wps):12932.13029565759
step: 338	loss: 79771.54008626938	speed (wps):12936.82330333168
step: 379	loss: 88837.08792924881	speed (wps):12942.37858257854
epoch: 0	train ppl: 768.2741157324964	val ppl: 417.92003986294753	best val: 417.92003986294753	time (s) spent in epoch: 73.30415868759155

EPOCH 1 ------------------
step: 10	loss: 2422.661123275757	speed (wps):12349.149712736833
step: 51	loss: 11326.584005355835	speed (wps):12840.999442232242
step: 92	loss: 20194.53610420227	speed (wps):12894.119103462346
step: 133	loss: 28993.125417232513	speed (wps):12923.69656550945
step: 174	loss: 37637.41503715515	speed (wps):12935.427647271175
step: 215	loss: 46405.55787563324	speed (wps):12945.508649813139
step: 256	loss: 55148.11947107315	speed (wps):12951.695580515363
step: 297	loss: 63816.377012729645	speed (wps):12958.706350015193
step: 338	loss: 72421.96411132812	speed (wps):12962.06942985499
step: 379	loss: 81047.77645587921	speed (wps):12966.220061614642
epoch: 1	train ppl: 438.4562324495264	val ppl: 321.91869882267747	best val: 321.91869882267747	time (s) spent in epoch: 73.17928576469421

EPOCH 2 ------------------
step: 10	loss: 2326.6285014152527	speed (wps):12365.954736824724
step: 51	loss: 10868.230874538422	speed (wps):12857.564840096593
step: 92	loss: 19384.87092733383	speed (wps):12920.611839276198
step: 133	loss: 27865.078604221344	speed (wps):12946.861093141675
step: 174	loss: 36196.06647729874	speed (wps):12961.95123060307
step: 215	loss: 44666.91175699234	speed (wps):12972.72260752938
step: 256	loss: 53133.48514318466	speed (wps):12978.549421518337
step: 297	loss: 61533.31319332123	speed (wps):12981.08470116371
step: 338	loss: 69894.74013090134	speed (wps):12985.151803908499
step: 379	loss: 78293.24253320694	speed (wps):12986.5667315828
epoch: 2	train ppl: 357.85349697189474	val ppl: 277.16652132191314	best val: 277.16652132191314	time (s) spent in epoch: 73.07315731048584

EPOCH 3 ------------------
step: 10	loss: 2268.526737689972	speed (wps):12368.985772769804
step: 51	loss: 10599.941840171814	speed (wps):12884.74138093594
step: 92	loss: 18924.108712673187	speed (wps):12938.917252449737
step: 133	loss: 27199.664492607117	speed (wps):12958.081128807627
step: 174	loss: 35319.248411655426	speed (wps):12970.804811278555
step: 215	loss: 43594.21889781952	speed (wps):12976.514017197283
step: 256	loss: 51892.43921518326	speed (wps):12980.486478833553
step: 297	loss: 60122.23975896835	speed (wps):12985.755937035474
step: 338	loss: 68315.94985723495	speed (wps):12988.071853543557
step: 379	loss: 76571.1132311821	speed (wps):12990.128177123446
epoch: 3	train ppl: 315.15283527830877	val ppl: 253.42992679799627	best val: 253.42992679799627	time (s) spent in epoch: 73.0558340549469

EPOCH 4 ------------------
step: 10	loss: 2233.5577964782715	speed (wps):12358.459011614106
step: 51	loss: 10419.310085773468	speed (wps):12863.485174683012
step: 92	loss: 18593.290293216705	speed (wps):12927.266147627462
step: 133	loss: 26737.022495269775	speed (wps):12954.923487000204
step: 174	loss: 34730.68480730057	speed (wps):12967.903687044558
step: 215	loss: 42871.39678001404	speed (wps):12974.958313903768
step: 256	loss: 51038.88913869858	speed (wps):12981.015514665976
step: 297	loss: 59148.19682121277	speed (wps):12985.719197057497
step: 338	loss: 67221.00263357162	speed (wps):12988.134669291247
step: 379	loss: 75355.5156326294	speed (wps):12989.872433781002
epoch: 4	train ppl: 287.8644478529807	val ppl: 234.85475366874994	best val: 234.85475366874994	time (s) spent in epoch: 73.05267333984375

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3482.6787090301514	speed (wps):6205.489197309681
step: 51	loss: 13801.77179813385	speed (wps):6393.222791282189
step: 92	loss: 23540.556228160858	speed (wps):6415.276471438286
step: 133	loss: 33037.76496887207	speed (wps):6424.435118398155
step: 174	loss: 42305.33112049103	speed (wps):6428.979380355384
step: 215	loss: 51614.1806435585	speed (wps):6431.515040834662
step: 256	loss: 60834.036140441895	speed (wps):6433.137014616293
step: 297	loss: 69968.25109243393	speed (wps):6435.2526517801025
step: 338	loss: 79023.76100540161	speed (wps):6437.210328876793
step: 379	loss: 88059.95131731033	speed (wps):6438.01388032364
epoch: 0	train ppl: 727.2600203055068	val ppl: 401.37683863197066	best val: 401.37683863197066	time (s) spent in epoch: 147.22390604019165

EPOCH 1 ------------------
step: 10	loss: 2418.737518787384	speed (wps):6322.278519003024
step: 51	loss: 11311.51713848114	speed (wps):6422.399281841316
step: 92	loss: 20167.664260864258	speed (wps):6431.708650860754
step: 133	loss: 28966.85483932495	speed (wps):6435.710545784794
step: 174	loss: 37594.744617938995	speed (wps):6437.602576868485
step: 215	loss: 46356.3814663887	speed (wps):6439.179983661189
step: 256	loss: 55103.242309093475	speed (wps):6439.737580079566
step: 297	loss: 63779.995255470276	speed (wps):6440.294079015814
step: 338	loss: 72410.59769630432	speed (wps):6440.31302713222
step: 379	loss: 81067.83819675446	speed (wps):6441.193320860412
epoch: 1	train ppl: 439.54600086364513	val ppl: 320.5548455521526	best val: 320.5548455521526	time (s) spent in epoch: 147.17645859718323

EPOCH 2 ------------------
step: 10	loss: 2332.454435825348	speed (wps):6336.543956258888
step: 51	loss: 10903.556983470917	speed (wps):6418.246115703609
step: 92	loss: 19461.67886018753	speed (wps):6428.966823059999
step: 133	loss: 27969.248361587524	speed (wps):6432.111586571987
step: 174	loss: 36314.90093946457	speed (wps):6434.69150237404
step: 215	loss: 44794.73457574844	speed (wps):6435.891789559119
step: 256	loss: 53292.37098932266	speed (wps):6436.203783578248
step: 297	loss: 61723.074395656586	speed (wps):6437.901788036402
step: 338	loss: 70121.75879478455	speed (wps):6438.22429126346
step: 379	loss: 78551.83990716934	speed (wps):6438.812559829726
epoch: 2	train ppl: 364.91688444964126	val ppl: 280.8031617762979	best val: 280.8031617762979	time (s) spent in epoch: 147.2237629890442

EPOCH 3 ------------------
step: 10	loss: 2278.2318663597107	speed (wps):6324.736202321742
step: 51	loss: 10659.17263507843	speed (wps):6419.016202141032
step: 92	loss: 19018.16477060318	speed (wps):6429.513979664762
step: 133	loss: 27333.31434726715	speed (wps):6433.264330324978
step: 174	loss: 35491.96125984192	speed (wps):6434.860007024712
step: 215	loss: 43803.773159980774	speed (wps):6436.436370059452
step: 256	loss: 52137.33063459396	speed (wps):6437.510108115211
step: 297	loss: 60416.02567195892	speed (wps):6438.113781185293
step: 338	loss: 68658.87120723724	speed (wps):6438.900760418355
step: 379	loss: 76947.91474580765	speed (wps):6439.231154127958
epoch: 3	train ppl: 324.32336051354685	val ppl: 255.05200307894776	best val: 255.05200307894776	time (s) spent in epoch: 147.210599899292

EPOCH 4 ------------------
step: 10	loss: 2241.3483786582947	speed (wps):6322.824677698386
step: 51	loss: 10471.384375095367	speed (wps):6422.297632878628
step: 92	loss: 18691.383459568024	speed (wps):6433.986403347639
step: 133	loss: 26864.461660385132	speed (wps):6436.618056014202
step: 174	loss: 34890.52826881409	speed (wps):6437.81809459967
step: 215	loss: 43062.27202653885	speed (wps):6439.629813730047
step: 256	loss: 51267.92198419571	speed (wps):6440.042037505385
step: 297	loss: 59425.12191772461	speed (wps):6440.656343722879
step: 338	loss: 67544.7177362442	speed (wps):6441.395174086561
step: 379	loss: 75722.46186494827	speed (wps):6441.741015705927
epoch: 4	train ppl: 296.0818412749682	val ppl: 240.65643821049835	best val: 240.65643821049835	time (s) spent in epoch: 147.1617603302002

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3324.272346496582	speed (wps):7748.463528348888
step: 51	loss: 13797.6034116745	speed (wps):8090.232876946431
step: 92	loss: 23652.505168914795	speed (wps):8131.786871652035
step: 133	loss: 33370.144221782684	speed (wps):8148.566530297605
step: 174	loss: 42838.76232147217	speed (wps):8155.31275442925
step: 215	loss: 52348.23062181473	speed (wps):8162.044594995007
step: 256	loss: 61765.43113946915	speed (wps):8165.278167403248
step: 297	loss: 71080.99239826202	speed (wps):8167.887863243778
step: 338	loss: 80317.77635335922	speed (wps):8169.948457465086
step: 379	loss: 89539.287712574	speed (wps):8171.717428723004
epoch: 0	train ppl: 813.4622564976748	val ppl: 453.1112378875703	best val: 453.1112378875703	time (s) spent in epoch: 116.03166770935059

EPOCH 1 ------------------
step: 10	loss: 2459.585816860199	speed (wps):7969.030768020752
step: 51	loss: 11526.615085601807	speed (wps):8138.976072966962
step: 92	loss: 20560.053532123566	speed (wps):8161.462418542641
step: 133	loss: 29541.513736248016	speed (wps):8171.254853190896
step: 174	loss: 38370.96972703934	speed (wps):8175.388243599711
step: 215	loss: 47318.46313238144	speed (wps):8178.19674306707
step: 256	loss: 56242.81622171402	speed (wps):8179.547778339779
step: 297	loss: 65097.15709447861	speed (wps):8181.357205277205
step: 338	loss: 73901.7045712471	speed (wps):8182.53868961318
step: 379	loss: 82723.66681575775	speed (wps):8182.922063325534
epoch: 1	train ppl: 497.7143604407814	val ppl: 355.38808353870706	best val: 355.38808353870706	time (s) spent in epoch: 115.89794325828552

EPOCH 2 ------------------
step: 10	loss: 2374.133777618408	speed (wps):7982.855399819608
step: 51	loss: 11108.779637813568	speed (wps):8143.876675257852
step: 92	loss: 19821.05801343918	speed (wps):8164.13420513743
step: 133	loss: 28505.182790756226	speed (wps):8171.756862868399
step: 174	loss: 37026.57455444336	speed (wps):8176.234602884782
step: 215	loss: 45692.47451543808	speed (wps):8178.052688646204
step: 256	loss: 54354.32007789612	speed (wps):8180.0755816935925
step: 297	loss: 62951.88808441162	speed (wps):8181.69909079774
step: 338	loss: 71518.47058534622	speed (wps):8183.142162959913
step: 379	loss: 80109.55212831497	speed (wps):8183.633028102339
epoch: 2	train ppl: 410.30556249531253	val ppl: 310.6621424233112	best val: 310.6621424233112	time (s) spent in epoch: 115.89540529251099

EPOCH 3 ------------------
step: 10	loss: 2321.2411618232727	speed (wps):7976.95811256944
step: 51	loss: 10862.982306480408	speed (wps):8143.91169981752
step: 92	loss: 19394.199328422546	speed (wps):8166.377550036665
step: 133	loss: 27883.022389411926	speed (wps):8173.068637688149
step: 174	loss: 36221.92810058594	speed (wps):8177.508252935271
step: 215	loss: 44705.449504852295	speed (wps):8180.028037559253
step: 256	loss: 53213.95502567291	speed (wps):8181.311823101664
step: 297	loss: 61652.01656103134	speed (wps):8182.314724134497
step: 338	loss: 70059.435338974	speed (wps):8182.8651070809065
step: 379	loss: 78503.40655565262	speed (wps):8183.622354464246
epoch: 3	train ppl: 364.4425556690146	val ppl: 282.94902884030296	best val: 282.94902884030296	time (s) spent in epoch: 115.88570928573608

EPOCH 4 ------------------
step: 10	loss: 2281.1422300338745	speed (wps):7976.156542750316
step: 51	loss: 10677.74400472641	speed (wps):8146.281036786517
step: 92	loss: 19062.314128875732	speed (wps):8167.382495146863
step: 133	loss: 27420.986714363098	speed (wps):8173.826069444244
step: 174	loss: 35624.30689573288	speed (wps):8176.824694344543
step: 215	loss: 43981.95449113846	speed (wps):8178.659788319312
step: 256	loss: 52360.3604221344	speed (wps):8179.07307121465
step: 297	loss: 60679.688115119934	speed (wps):8180.46272408187
step: 338	loss: 68969.96567726135	speed (wps):8180.814661207212
step: 379	loss: 77303.32653522491	speed (wps):8182.084216084913
epoch: 4	train ppl: 333.37789181593456	val ppl: 267.50052673058394	best val: 267.50052673058394	time (s) spent in epoch: 115.90947961807251

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1500_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3453.9615964889526	speed (wps):4996.654585623841
step: 51	loss: 13807.96452999115	speed (wps):5131.893819308504
step: 92	loss: 23674.35672044754	speed (wps):5146.092873411188
step: 133	loss: 33299.33941364288	speed (wps):5150.852105908161
step: 174	loss: 42647.731647491455	speed (wps):5153.880681480513
step: 215	loss: 52070.84476709366	speed (wps):5156.375615219234
step: 256	loss: 61394.384446144104	speed (wps):5157.602250137304
step: 297	loss: 70633.14015865326	speed (wps):5159.037125853916
step: 338	loss: 79821.34454727173	speed (wps):5159.475845863882
step: 379	loss: 88976.40295028687	speed (wps):5160.288385555736
epoch: 0	train ppl: 779.9240568035584	val ppl: 430.3478250015449	best val: 430.3478250015449	time (s) spent in epoch: 183.64695811271667

EPOCH 1 ------------------
step: 10	loss: 2448.4774351119995	speed (wps):5093.858537942124
step: 51	loss: 11460.64590215683	speed (wps):5153.563801665014
step: 92	loss: 20453.45780134201	speed (wps):5158.9246430119365
step: 133	loss: 29364.08737897873	speed (wps):5158.981136843615
step: 174	loss: 38119.325299263	speed (wps):5160.647484925081
step: 215	loss: 47025.08083343506	speed (wps):5162.400831512594
step: 256	loss: 55896.92002058029	speed (wps):5162.159684818171
step: 297	loss: 64697.69150972366	speed (wps):5162.603124289862
step: 338	loss: 73466.80361270905	speed (wps):5163.052740950122
step: 379	loss: 82232.86933422089	speed (wps):5163.0983330265235
epoch: 1	train ppl: 479.8724327993318	val ppl: 349.04511554477676	best val: 349.04511554477676	time (s) spent in epoch: 183.56807684898376

EPOCH 2 ------------------
step: 10	loss: 2367.7010703086853	speed (wps):5090.849998179863
step: 51	loss: 11055.81360578537	speed (wps):5150.100755815407
step: 92	loss: 19728.56442451477	speed (wps):5157.770382330721
step: 133	loss: 28343.3877825737	speed (wps):5160.663538560488
step: 174	loss: 36808.59605073929	speed (wps):5161.673008291482
step: 215	loss: 45438.00180196762	speed (wps):5161.974131738345
step: 256	loss: 54048.56161117554	speed (wps):5162.408156041424
step: 297	loss: 62601.16844177246	speed (wps):5162.5511579636195
step: 338	loss: 71126.99603796005	speed (wps):5162.46728579026
step: 379	loss: 79679.71021413803	speed (wps):5162.6193954897935
epoch: 2	train ppl: 397.82063282037024	val ppl: 301.93067372763045	best val: 301.93067372763045	time (s) spent in epoch: 183.56898760795593

EPOCH 3 ------------------
step: 10	loss: 2308.9844727516174	speed (wps):5087.233920665011
step: 51	loss: 10806.683304309845	speed (wps):5146.61579470563
step: 92	loss: 19292.34205722809	speed (wps):5154.763959593034
step: 133	loss: 27724.9125289917	speed (wps):5158.462654887532
step: 174	loss: 36015.375175476074	speed (wps):5159.640309339699
step: 215	loss: 44453.29431772232	speed (wps):5161.184027224163
step: 256	loss: 52904.59188699722	speed (wps):5161.934234961221
step: 297	loss: 61301.213555336	speed (wps):5162.0136786692165
step: 338	loss: 69674.74178791046	speed (wps):5162.866891960639
step: 379	loss: 78077.54916191101	speed (wps):5162.889019769538
epoch: 3	train ppl: 352.870278337036	val ppl: 276.7980515177837	best val: 276.7980515177837	time (s) spent in epoch: 183.5770046710968

EPOCH 4 ------------------
step: 10	loss: 2277.7174019813538	speed (wps):5093.034662646421
step: 51	loss: 10631.296563148499	speed (wps):5151.18629682518
step: 92	loss: 18972.431647777557	speed (wps):5157.7759235455915
step: 133	loss: 27283.001029491425	speed (wps):5160.877015289583
step: 174	loss: 35442.90599822998	speed (wps):5162.293114664587
step: 215	loss: 43742.08172798157	speed (wps):5162.6525248776425
step: 256	loss: 52072.135701179504	speed (wps):5163.475564123061
step: 297	loss: 60351.70155286789	speed (wps):5163.55004845272
step: 338	loss: 68594.43722248077	speed (wps):5163.980930741169
step: 379	loss: 76885.92924833298	speed (wps):5164.204972295527
epoch: 4	train ppl: 323.1622073127529	val ppl: 258.41360110383346	best val: 258.41360110383346	time (s) spent in epoch: 183.5268678665161

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=64_seq_len=35_hidden_size=1500_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3361.1942195892334	speed (wps):15894.63173869462
step: 30	loss: 8626.420769691467	speed (wps):16550.724671809727
step: 50	loss: 13533.588056564331	speed (wps):16699.05145881761
step: 70	loss: 18312.56868124008	speed (wps):16764.20329315556
step: 90	loss: 23043.351924419403	speed (wps):16800.752218620237
step: 110	loss: 27718.788390159607	speed (wps):16824.63828382943
step: 130	loss: 32363.53515148163	speed (wps):16841.96677330598
step: 150	loss: 36960.23386716843	speed (wps):16854.054423402664
step: 170	loss: 41508.051290512085	speed (wps):16863.65499936325
step: 190	loss: 46049.803721904755	speed (wps):16871.778643977483
epoch: 0	train ppl: 948.3101547868732	val ppl: 500.8265368742871	best val: 500.8265368742871	time (s) spent in epoch: 56.3346962928772

EPOCH 1 ------------------
step: 10	loss: 2481.3417553901672	speed (wps):16321.907642435177
step: 30	loss: 6949.168879985809	speed (wps):16713.713995916725
step: 50	loss: 11389.79546546936	speed (wps):16801.351176261654
step: 70	loss: 15813.073990345001	speed (wps):16838.669520610103
step: 90	loss: 20212.18403816223	speed (wps):16860.221490747343
step: 110	loss: 24583.650028705597	speed (wps):16873.930550851597
step: 130	loss: 28934.1837310791	speed (wps):16883.43515419669
step: 150	loss: 33253.09381246567	speed (wps):16890.61653529715
step: 170	loss: 37542.18670606613	speed (wps):16895.625735939724
step: 190	loss: 41837.25533723831	speed (wps):16900.413074814518
epoch: 1	train ppl: 518.0038716092513	val ppl: 373.97499680499067	best val: 373.97499680499067	time (s) spent in epoch: 56.24681329727173

EPOCH 2 ------------------
step: 10	loss: 2374.6644139289856	speed (wps):16328.246218803315
step: 30	loss: 6647.811381816864	speed (wps):16717.898277517168
step: 50	loss: 10908.165299892426	speed (wps):16801.282543026442
step: 70	loss: 15160.674846172333	speed (wps):16840.35738676984
step: 90	loss: 19390.006892681122	speed (wps):16861.531100251053
step: 110	loss: 23604.668259620667	speed (wps):16875.051117790044
step: 130	loss: 27804.17365550995	speed (wps):16885.105401837798
step: 150	loss: 31982.20736503601	speed (wps):16891.294360782802
step: 170	loss: 36134.48008775711	speed (wps):16896.194960837478
step: 190	loss: 40298.306646347046	speed (wps):16900.581316131655
epoch: 2	train ppl: 413.0059999013072	val ppl: 314.88279312769635	best val: 314.88279312769635	time (s) spent in epoch: 56.24752235412598

EPOCH 3 ------------------
step: 10	loss: 2309.8619627952576	speed (wps):16315.294521046419
step: 30	loss: 6469.998300075531	speed (wps):16710.675073704926
step: 50	loss: 10623.985247612	speed (wps):16795.56291778393
step: 70	loss: 14772.684407234192	speed (wps):16836.138886828736
step: 90	loss: 18908.950572013855	speed (wps):16857.913335892285
step: 110	loss: 23032.570452690125	speed (wps):16873.506008443066
step: 130	loss: 27136.34390115738	speed (wps):16882.987479293923
step: 150	loss: 31224.47106361389	speed (wps):16890.439471023594
step: 170	loss: 35285.35905122757	speed (wps):16896.425700193195
step: 190	loss: 39365.557827949524	speed (wps):16900.63209160533
epoch: 3	train ppl: 359.92692093269824	val ppl: 282.08496164238693	best val: 282.08496164238693	time (s) spent in epoch: 56.24798583984375

EPOCH 4 ------------------
step: 10	loss: 2267.1156239509583	speed (wps):16319.916571511047
step: 30	loss: 6356.985332965851	speed (wps):16712.24953664426
step: 50	loss: 10435.355112552643	speed (wps):16801.700831533908
step: 70	loss: 14502.416772842407	speed (wps):16839.67123186693
step: 90	loss: 18564.15259361267	speed (wps):16859.736902009205
step: 110	loss: 22616.75420999527	speed (wps):16874.078940969714
step: 130	loss: 26656.740534305573	speed (wps):16883.700226910936
step: 150	loss: 30674.852108955383	speed (wps):16891.038045654466
step: 170	loss: 34671.622841358185	speed (wps):16896.123527982534
step: 190	loss: 38690.61429262161	speed (wps):16900.541284899627
epoch: 4	train ppl: 325.4754471997639	val ppl: 259.7472730908491	best val: 259.7472730908491	time (s) spent in epoch: 56.25053834915161

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3378.2365822792053	speed (wps):9924.52823064302
step: 30	loss: 8541.997809410095	speed (wps):10105.11432100791
step: 50	loss: 13420.706117153168	speed (wps):10147.80281247959
step: 70	loss: 18188.24812889099	speed (wps):10162.499587958151
step: 90	loss: 22885.34632205963	speed (wps):10167.499109164783
step: 110	loss: 27488.120727539062	speed (wps):10171.032426576523
step: 130	loss: 32065.51905155182	speed (wps):10172.553830988692
step: 150	loss: 36584.97668027878	speed (wps):10174.041683596433
step: 170	loss: 41042.02663898468	speed (wps):10175.087095861281
step: 190	loss: 45496.56213760376	speed (wps):10176.976368333006
epoch: 0	train ppl: 870.7177897732075	val ppl: 454.6081128549726	best val: 454.6081128549726	time (s) spent in epoch: 93.49979901313782

EPOCH 1 ------------------
step: 10	loss: 2438.094971179962	speed (wps):10021.462082618677
step: 30	loss: 6829.221308231354	speed (wps):10127.370458643074
step: 50	loss: 11207.757470607758	speed (wps):10151.567363327109
step: 70	loss: 15562.65742778778	speed (wps):10160.791173796762
step: 90	loss: 19889.02839422226	speed (wps):10165.428141979095
step: 110	loss: 24184.088397026062	speed (wps):10169.48637651228
step: 130	loss: 28467.493226528168	speed (wps):10172.813133196423
step: 150	loss: 32727.461454868317	speed (wps):10174.625858205309
step: 170	loss: 36946.48109912872	speed (wps):10176.386970033622
step: 190	loss: 41179.24783706665	speed (wps):10177.281666285407
epoch: 1	train ppl: 469.15698494282077	val ppl: 342.5792742123001	best val: 342.5792742123001	time (s) spent in epoch: 93.49051833152771

EPOCH 2 ------------------
step: 10	loss: 2341.298279762268	speed (wps):10022.294955039264
step: 30	loss: 6563.578329086304	speed (wps):10127.778965436939
step: 50	loss: 10768.687608242035	speed (wps):10150.139794733042
step: 70	loss: 14963.159160614014	speed (wps):10160.164081577212
step: 90	loss: 19131.63166999817	speed (wps):10166.814475987167
step: 110	loss: 23283.292512893677	speed (wps):10171.148637360517
step: 130	loss: 27424.06630754471	speed (wps):10173.80077189438
step: 150	loss: 31551.159064769745	speed (wps):10175.774555987875
step: 170	loss: 35646.83376312256	speed (wps):10176.744246279199
step: 190	loss: 39757.66109466553	speed (wps):10177.921411329766
epoch: 2	train ppl: 381.0064594375068	val ppl: 296.9927254116985	best val: 296.9927254116985	time (s) spent in epoch: 93.48471593856812

EPOCH 3 ------------------
step: 10	loss: 2282.2663044929504	speed (wps):10031.19550060322
step: 30	loss: 6399.351055622101	speed (wps):10131.319387665813
step: 50	loss: 10504.75845336914	speed (wps):10155.072022292963
step: 70	loss: 14599.597268104553	speed (wps):10163.658723348944
step: 90	loss: 18675.807218551636	speed (wps):10168.595197821824
step: 110	loss: 22738.2980799675	speed (wps):10171.759692850377
step: 130	loss: 26790.711908340454	speed (wps):10174.313671258997
step: 150	loss: 30836.37715101242	speed (wps):10175.836705451922
step: 170	loss: 34850.15800237656	speed (wps):10176.022918936356
step: 190	loss: 38880.95621585846	speed (wps):10176.8466821285
epoch: 3	train ppl: 334.55474152770734	val ppl: 267.92867379844984	best val: 267.92867379844984	time (s) spent in epoch: 93.50232720375061

EPOCH 4 ------------------
step: 10	loss: 2242.6418828964233	speed (wps):10024.823089375392
step: 30	loss: 6286.678249835968	speed (wps):10127.324855834624
step: 50	loss: 10317.630138397217	speed (wps):10150.543176293379
step: 70	loss: 14344.516696929932	speed (wps):10160.705973234364
step: 90	loss: 18353.35307598114	speed (wps):10164.770978791532
step: 110	loss: 22348.51262331009	speed (wps):10168.813672146829
step: 130	loss: 26340.325815677643	speed (wps):10171.3170303939
step: 150	loss: 30321.34917497635	speed (wps):10173.47107267096
step: 170	loss: 34273.63874912262	speed (wps):10174.809904090225
step: 190	loss: 38242.30810403824	speed (wps):10175.387435187291
epoch: 4	train ppl: 304.2840039207207	val ppl: 246.7081305092297	best val: 246.7081305092297	time (s) spent in epoch: 93.51490187644958

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3358.8008403778076	speed (wps):10797.943941150024
step: 30	loss: 8647.714648246765	speed (wps):11114.359117333895
step: 50	loss: 13601.619119644165	speed (wps):11183.702477660652
step: 70	loss: 18440.114340782166	speed (wps):11214.67076178671
step: 90	loss: 23220.397465229034	speed (wps):11232.96376955113
step: 110	loss: 27929.28312778473	speed (wps):11244.470275571275
step: 130	loss: 32583.128628730774	speed (wps):11252.491529762974
step: 150	loss: 37187.49926567078	speed (wps):11258.61227255789
step: 170	loss: 41743.09490442276	speed (wps):11263.229766931798
step: 190	loss: 46301.990485191345	speed (wps):11266.894989662078
epoch: 0	train ppl: 984.6420858820388	val ppl: 506.29830548668883	best val: 506.29830548668883	time (s) spent in epoch: 84.28156352043152

EPOCH 1 ------------------
step: 10	loss: 2501.5706396102905	speed (wps):11086.484705801078
step: 30	loss: 7008.807578086853	speed (wps):11223.790061991405
step: 50	loss: 11500.436182022095	speed (wps):11252.84321862117
step: 70	loss: 15991.833221912384	speed (wps):11265.814974241684
step: 90	loss: 20463.117117881775	speed (wps):11273.001180645328
step: 110	loss: 24899.719228744507	speed (wps):11278.12456783276
step: 130	loss: 29322.18555212021	speed (wps):11281.017943356113
step: 150	loss: 33722.82524108887	speed (wps):11282.493766523316
step: 170	loss: 38082.638511657715	speed (wps):11284.047675616172
step: 190	loss: 42463.2280087471	speed (wps):11285.168692734047
epoch: 1	train ppl: 569.1660161402131	val ppl: 411.3172947069786	best val: 411.3172947069786	time (s) spent in epoch: 84.16606187820435

EPOCH 2 ------------------
step: 10	loss: 2419.3083930015564	speed (wps):11086.189770169534
step: 30	loss: 6780.443289279938	speed (wps):11222.796644032709
step: 50	loss: 11134.41043138504	speed (wps):11252.815206162988
step: 70	loss: 15480.418746471405	speed (wps):11266.583221655492
step: 90	loss: 19807.616283893585	speed (wps):11274.13198316666
step: 110	loss: 24108.882870674133	speed (wps):11278.243852966172
step: 130	loss: 28402.341084480286	speed (wps):11280.51317083647
step: 150	loss: 32676.077201366425	speed (wps):11282.495291890202
step: 170	loss: 36912.92981386185	speed (wps):11284.229489147672
step: 190	loss: 41175.186779499054	speed (wps):11285.42756175621
epoch: 2	train ppl: 470.49859847061134	val ppl: 351.35418879439385	best val: 351.35418879439385	time (s) spent in epoch: 84.15803408622742

EPOCH 3 ------------------
step: 10	loss: 2358.7600445747375	speed (wps):11085.921011848475
step: 30	loss: 6621.2436509132385	speed (wps):11219.51554947145
step: 50	loss: 10871.327607631683	speed (wps):11249.593250806893
step: 70	loss: 15111.35811328888	speed (wps):11262.3314429714
step: 90	loss: 19333.859128952026	speed (wps):11269.971786808072
step: 110	loss: 23537.81973361969	speed (wps):11274.40847790162
step: 130	loss: 27736.660277843475	speed (wps):11278.091470142243
step: 150	loss: 31915.04177570343	speed (wps):11280.552353724852
step: 170	loss: 36066.00942850113	speed (wps):11281.90443749531
step: 190	loss: 40244.587359428406	speed (wps):11283.21707135075
epoch: 3	train ppl: 409.95934103278324	val ppl: 313.8431910380172	best val: 313.8431910380172	time (s) spent in epoch: 84.17546725273132

EPOCH 4 ------------------
step: 10	loss: 2315.4748916625977	speed (wps):11094.403286809136
step: 30	loss: 6500.000138282776	speed (wps):11224.174804606115
step: 50	loss: 10670.65701007843	speed (wps):11254.207805603904
step: 70	loss: 14832.458922863007	speed (wps):11266.291513147125
step: 90	loss: 18982.87212371826	speed (wps):11271.869341772383
step: 110	loss: 23123.639192581177	speed (wps):11277.072638312882
step: 130	loss: 27254.533004760742	speed (wps):11280.178869209733
step: 150	loss: 31370.76131105423	speed (wps):11282.160752915457
step: 170	loss: 35465.24604558945	speed (wps):11283.74429624689
step: 190	loss: 39580.84179401398	speed (wps):11285.388349167146
epoch: 4	train ppl: 371.85105307774904	val ppl: 290.6572941521334	best val: 290.6572941521334	time (s) spent in epoch: 84.1567485332489

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1500_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3201.7145347595215	speed (wps):6075.672475537441
step: 30	loss: 8313.693451881409	speed (wps):6186.134433236054
step: 50	loss: 13182.394070625305	speed (wps):6211.563193912706
step: 70	loss: 17968.001832962036	speed (wps):6222.240547714384
step: 90	loss: 22688.980078697205	speed (wps):6228.639555892914
step: 110	loss: 27337.65291452408	speed (wps):6233.335188382063
step: 130	loss: 31941.60406589508	speed (wps):6236.6617438851545
step: 150	loss: 36496.156322956085	speed (wps):6238.2541485291695
step: 170	loss: 41012.92644023895	speed (wps):6239.974645273077
step: 190	loss: 45530.92305421829	speed (wps):6240.775101398105
epoch: 0	train ppl: 881.228996145133	val ppl: 488.01827655698975	best val: 488.01827655698975	time (s) spent in epoch: 152.41178727149963

EPOCH 1 ------------------
step: 10	loss: 2479.7297859191895	speed (wps):6183.193032786027
step: 30	loss: 6945.094771385193	speed (wps):6224.203188752464
step: 50	loss: 11399.472873210907	speed (wps):6233.814497304467
step: 70	loss: 15837.821502685547	speed (wps):6238.980491743485
step: 90	loss: 20259.580466747284	speed (wps):6240.865509408746
step: 110	loss: 24647.463755607605	speed (wps):6242.6496349752815
step: 130	loss: 29018.547010421753	speed (wps):6243.626151210782
step: 150	loss: 33365.38940668106	speed (wps):6244.567639975876
step: 170	loss: 37689.74996328354	speed (wps):6245.48886201514
step: 190	loss: 42023.253185749054	speed (wps):6246.208403692216
epoch: 1	train ppl: 532.6424075681161	val ppl: 378.60947242855093	best val: 378.60947242855093	time (s) spent in epoch: 152.3243989944458

EPOCH 2 ------------------
step: 10	loss: 2389.8456859588623	speed (wps):6190.552568345539
step: 30	loss: 6704.6564412117	speed (wps):6227.60435639094
step: 50	loss: 11005.058121681213	speed (wps):6235.6829902213385
step: 70	loss: 15292.245481014252	speed (wps):6239.702106379214
step: 90	loss: 19561.720719337463	speed (wps):6241.945952790548
step: 110	loss: 23811.17891073227	speed (wps):6243.195600839836
step: 130	loss: 28048.086864948273	speed (wps):6243.989735401762
step: 150	loss: 32270.520055294037	speed (wps):6245.201626120937
step: 170	loss: 36468.0176115036	speed (wps):6245.821621943045
step: 190	loss: 40682.745735645294	speed (wps):6245.552908669993
epoch: 2	train ppl: 437.488138892061	val ppl: 325.93465316448714	best val: 325.93465316448714	time (s) spent in epoch: 152.29867720603943

EPOCH 3 ------------------
step: 10	loss: 2334.089469909668	speed (wps):6197.5548072545125
step: 30	loss: 6546.716911792755	speed (wps):6231.458514146618
step: 50	loss: 10749.560618400574	speed (wps):6238.797184970973
step: 70	loss: 14942.235934734344	speed (wps):6240.349513101369
step: 90	loss: 19118.94356250763	speed (wps):6243.088245022608
step: 110	loss: 23281.910922527313	speed (wps):6244.4029582937455
step: 130	loss: 27438.768763542175	speed (wps):6245.386912168563
step: 150	loss: 31578.467633724213	speed (wps):6246.077592579096
step: 170	loss: 35689.62451219559	speed (wps):6246.611161152709
step: 190	loss: 39825.78192949295	speed (wps):6247.000126830924
epoch: 3	train ppl: 385.29591872492983	val ppl: 296.9392608929193	best val: 296.9392608929193	time (s) spent in epoch: 152.30997371673584

EPOCH 4 ------------------
step: 10	loss: 2294.013936519623	speed (wps):6193.249944785337
step: 30	loss: 6438.381655216217	speed (wps):6227.976359892767
step: 50	loss: 10574.646117687225	speed (wps):6237.1547947608
step: 70	loss: 14702.42333650589	speed (wps):6241.056642186289
step: 90	loss: 18811.26702785492	speed (wps):6242.805840727837
step: 110	loss: 22919.262001514435	speed (wps):6243.5175670127965
step: 130	loss: 27012.388994693756	speed (wps):6243.365363401726
step: 150	loss: 31094.87869501114	speed (wps):6243.990668617072
step: 170	loss: 35147.830452919006	speed (wps):6245.083722167087
step: 190	loss: 39223.98409366608	speed (wps):6245.946004384658
epoch: 4	train ppl: 352.463805090206	val ppl: 277.53986539966473	best val: 277.53986539966473	time (s) spent in epoch: 152.30437564849854

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0002_batch_size=128_seq_len=35_hidden_size=1500_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3398.8427114486694	speed (wps):11755.7120648425
step: 51	loss: 13648.216798305511	speed (wps):12622.5901498369
step: 92	loss: 23287.78133392334	speed (wps):12736.663001599974
step: 133	loss: 32687.572691440582	speed (wps):12772.570285988546
step: 174	loss: 41841.52037143707	speed (wps):12797.168097179472
step: 215	loss: 51085.235743522644	speed (wps):12810.314219798036
step: 256	loss: 60210.039331912994	speed (wps):12819.834808590302
step: 297	loss: 69219.56140995026	speed (wps):12828.153031778966
step: 338	loss: 78143.20837497711	speed (wps):12834.21937073644
step: 379	loss: 87071.07521295547	speed (wps):12838.805040806834
epoch: 0	train ppl: 674.0702742141117	val ppl: 372.1563703074462	best val: 372.1563703074462	time (s) spent in epoch: 73.89753842353821

EPOCH 1 ------------------
step: 10	loss: 2380.720109939575	speed (wps):12247.611757405737
step: 51	loss: 11137.070639133453	speed (wps):12746.545892323906
step: 92	loss: 19833.010790348053	speed (wps):12804.371759834492
step: 133	loss: 28456.88046693802	speed (wps):12827.373377551025
step: 174	loss: 36917.652752399445	speed (wps):12841.215673937513
step: 215	loss: 45508.11764240265	speed (wps):12852.502483707167
step: 256	loss: 54080.97243309021	speed (wps):12860.287774724038
step: 297	loss: 62585.906727313995	speed (wps):12865.089108046934
step: 338	loss: 71031.3411283493	speed (wps):12868.82321909702
step: 379	loss: 79511.70738697052	speed (wps):12870.604222148568
epoch: 1	train ppl: 390.7183744824632	val ppl: 286.89420991668794	best val: 286.89420991668794	time (s) spent in epoch: 73.7219660282135

EPOCH 2 ------------------
step: 10	loss: 2281.890227794647	speed (wps):12279.412661211798
step: 51	loss: 10663.532631397247	speed (wps):12784.207178498611
step: 92	loss: 19014.429938793182	speed (wps):12842.064831308287
step: 133	loss: 27329.88489627838	speed (wps):12866.347922968922
step: 174	loss: 35490.110466480255	speed (wps):12879.419498668025
step: 215	loss: 43787.417261600494	speed (wps):12890.285771612153
step: 256	loss: 52091.71931028366	speed (wps):12892.566072055759
step: 297	loss: 60334.77147579193	speed (wps):12900.034016262598
step: 338	loss: 68540.38004398346	speed (wps):12903.240554203021
step: 379	loss: 76795.96307039261	speed (wps):12906.767077131653
epoch: 2	train ppl: 319.86944311391653	val ppl: 252.37084906196267	best val: 252.37084906196267	time (s) spent in epoch: 73.52542757987976

EPOCH 3 ------------------
step: 10	loss: 2225.331289768219	speed (wps):12277.602312114557
step: 51	loss: 10400.35159111023	speed (wps):12794.651732849894
step: 92	loss: 18558.330647945404	speed (wps):12858.764156606661
step: 133	loss: 26670.295751094818	speed (wps):12883.776642572873
step: 174	loss: 34630.091149806976	speed (wps):12893.600405526808
step: 215	loss: 42736.64035797119	speed (wps):12900.029322385384
step: 256	loss: 50875.22653102875	speed (wps):12905.853935669642
step: 297	loss: 58951.00298881531	speed (wps):12910.111178645477
step: 338	loss: 66996.270570755	speed (wps):12914.123921896755
step: 379	loss: 75104.92540121078	speed (wps):12917.192678629906
epoch: 3	train ppl: 282.3255794989746	val ppl: 228.60020529857667	best val: 228.60020529857667	time (s) spent in epoch: 73.46333837509155

EPOCH 4 ------------------
step: 10	loss: 2190.264482498169	speed (wps):12286.590746054946
step: 51	loss: 10222.78037071228	speed (wps):12792.062775489085
step: 92	loss: 18235.305914878845	speed (wps):12861.816906306105
step: 133	loss: 26216.783134937286	speed (wps):12887.353221957334
step: 174	loss: 34044.88317966461	speed (wps):12901.852980692802
step: 215	loss: 42015.252969264984	speed (wps):12905.500541779027
step: 256	loss: 50020.44414997101	speed (wps):12910.260251326341
step: 297	loss: 57980.28526067734	speed (wps):12914.035694990082
step: 338	loss: 65899.94652986526	speed (wps):12918.1906444531
step: 379	loss: 73893.95018815994	speed (wps):12919.009066602393
epoch: 4	train ppl: 258.1031607892441	val ppl: 216.11419693333588	best val: 216.11419693333588	time (s) spent in epoch: 73.4588451385498

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3258.5147500038147	speed (wps):8062.41587299928
step: 51	loss: 13343.367881774902	speed (wps):8427.772251256336
step: 92	loss: 22892.171444892883	speed (wps):8472.160535041563
step: 133	loss: 32207.93118953705	speed (wps):8487.871244939073
step: 174	loss: 41279.86277341843	speed (wps):8496.363740015875
step: 215	loss: 50405.46541213989	speed (wps):8502.134907185899
step: 256	loss: 59448.498804569244	speed (wps):8506.216340795017
step: 297	loss: 68380.53711652756	speed (wps):8509.303816439964
step: 338	loss: 77217.27284669876	speed (wps):8512.197971728507
step: 379	loss: 86034.77173805237	speed (wps):8514.013375593833
epoch: 0	train ppl: 624.4860058200718	val ppl: 356.32622869550227	best val: 356.32622869550227	time (s) spent in epoch: 111.3980360031128

EPOCH 1 ------------------
step: 10	loss: 2364.98361825943	speed (wps):8264.18770264113
step: 51	loss: 11036.546242237091	speed (wps):8473.63019971942
step: 92	loss: 19661.73100233078	speed (wps):8500.321368085719
step: 133	loss: 28210.21066904068	speed (wps):8511.24921453656
step: 174	loss: 36590.12182474136	speed (wps):8516.593851581232
step: 215	loss: 45097.011053562164	speed (wps):8520.943371866199
step: 256	loss: 53603.37650537491	speed (wps):8523.054777161551
step: 297	loss: 62036.5865778923	speed (wps):8524.334244819076
step: 338	loss: 70437.03518629074	speed (wps):8524.482453261773
step: 379	loss: 78857.83716440201	speed (wps):8525.829523417398
epoch: 1	train ppl: 372.5363572085788	val ppl: 280.93050613143873	best val: 280.93050613143873	time (s) spent in epoch: 111.24592876434326

EPOCH 2 ------------------
step: 10	loss: 2272.8087949752808	speed (wps):8264.342343310584
step: 51	loss: 10598.453738689423	speed (wps):8467.689590856808
step: 92	loss: 18910.304408073425	speed (wps):8490.906302178546
step: 133	loss: 27163.691806793213	speed (wps):8504.018834230263
step: 174	loss: 35263.806788921356	speed (wps):8510.040018793889
step: 215	loss: 43501.57635688782	speed (wps):8516.074899687992
step: 256	loss: 51755.8583855629	speed (wps):8517.350186329486
step: 297	loss: 59945.948123931885	speed (wps):8518.196941007309
step: 338	loss: 68111.37675046921	speed (wps):8521.679200882867
step: 379	loss: 76318.70777130127	speed (wps):8522.496113574323
epoch: 2	train ppl: 308.99462881197866	val ppl: 244.49736853357646	best val: 244.49736853357646	time (s) spent in epoch: 111.28498315811157

EPOCH 3 ------------------
step: 10	loss: 2219.8983550071716	speed (wps):8269.229059891702
step: 51	loss: 10347.50637292862	speed (wps):8482.288939069163
step: 92	loss: 18456.61864042282	speed (wps):8500.68209945483
step: 133	loss: 26526.050209999084	speed (wps):8510.152920875393
step: 174	loss: 34429.93778705597	speed (wps):8513.540294543722
step: 215	loss: 42473.96869182587	speed (wps):8517.375529531804
step: 256	loss: 50555.66240787506	speed (wps):8518.233261745412
step: 297	loss: 58575.72882890701	speed (wps):8519.38616775647
step: 338	loss: 66574.0487575531	speed (wps):8520.527775949193
step: 379	loss: 74623.77723932266	speed (wps):8521.761382589186
epoch: 3	train ppl: 272.3657704455723	val ppl: 224.80125466244422	best val: 224.80125466244422	time (s) spent in epoch: 111.28236556053162

EPOCH 4 ------------------
step: 10	loss: 2178.1372356414795	speed (wps):8248.40651369961
step: 51	loss: 10162.118618488312	speed (wps):8465.020654248883
step: 92	loss: 18135.631654262543	speed (wps):8489.260423943706
step: 133	loss: 26053.630719184875	speed (wps):8500.869664610997
step: 174	loss: 33814.170389175415	speed (wps):8509.324489918123
step: 215	loss: 41733.54728937149	speed (wps):8511.626868223291
step: 256	loss: 49684.11854505539	speed (wps):8514.788894463712
step: 297	loss: 57584.57248210907	speed (wps):8517.045148009565
step: 338	loss: 65461.53442621231	speed (wps):8518.976704264656
step: 379	loss: 73395.19728422165	speed (wps):8520.03571674613
epoch: 4	train ppl: 248.55665556916335	val ppl: 215.62642362945712	best val: 215.62642362945712	time (s) spent in epoch: 111.321608543396

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3422.759847640991	speed (wps):6167.7164392507
step: 51	loss: 13559.562549591064	speed (wps):6370.894845348779
step: 92	loss: 23168.26177597046	speed (wps):6393.686526680158
step: 133	loss: 32535.520696640015	speed (wps):6404.882193874306
step: 174	loss: 41660.516946315765	speed (wps):6410.933001257893
step: 215	loss: 50870.107271671295	speed (wps):6413.7614762764515
step: 256	loss: 59993.791942596436	speed (wps):6417.423764690399
step: 297	loss: 69023.1850528717	speed (wps):6418.816421297038
step: 338	loss: 77974.4877409935	speed (wps):6420.374440221642
step: 379	loss: 86902.05605745316	speed (wps):6421.095665965289
epoch: 0	train ppl: 667.2293243525564	val ppl: 372.6422288678057	best val: 372.6422288678057	time (s) spent in epoch: 147.63973331451416

EPOCH 1 ------------------
step: 10	loss: 2391.8997383117676	speed (wps):6298.71370820806
step: 51	loss: 11168.626117706299	speed (wps):6401.45820046986
step: 92	loss: 19904.583957195282	speed (wps):6412.024028429738
step: 133	loss: 28566.76480770111	speed (wps):6415.154023187924
step: 174	loss: 37073.990976810455	speed (wps):6417.407385198566
step: 215	loss: 45725.697605609894	speed (wps):6419.72966612302
step: 256	loss: 54351.47291660309	speed (wps):6421.718179483956
step: 297	loss: 62917.136170864105	speed (wps):6422.41583418445
step: 338	loss: 71434.42864179611	speed (wps):6423.187358493288
step: 379	loss: 79980.22458553314	speed (wps):6424.252264222731
epoch: 1	train ppl: 405.3639554061488	val ppl: 294.6885443071258	best val: 294.6885443071258	time (s) spent in epoch: 147.57604694366455

EPOCH 2 ------------------
step: 10	loss: 2303.8485407829285	speed (wps):6300.268833431309
step: 51	loss: 10765.870687961578	speed (wps):6404.3927486001185
step: 92	loss: 19209.914603233337	speed (wps):6411.075930180346
step: 133	loss: 27594.212741851807	speed (wps):6416.929918899707
step: 174	loss: 35831.11790418625	speed (wps):6418.002781562208
step: 215	loss: 44202.59303569794	speed (wps):6419.642382590814
step: 256	loss: 52586.00727558136	speed (wps):6420.278797035656
step: 297	loss: 60909.2875123024	speed (wps):6420.591262327564
step: 338	loss: 69208.94027233124	speed (wps):6420.572710182423
step: 379	loss: 77527.85074949265	speed (wps):6420.8187556720595
epoch: 2	train ppl: 338.0942653890097	val ppl: 258.7114827988264	best val: 258.7114827988264	time (s) spent in epoch: 147.66551327705383

EPOCH 3 ------------------
step: 10	loss: 2249.240298271179	speed (wps):6312.657716802423
step: 51	loss: 10517.962076663971	speed (wps):6399.902814727002
step: 92	loss: 18764.53102827072	speed (wps):6413.037738507934
step: 133	loss: 26958.1813788414	speed (wps):6414.721478997213
step: 174	loss: 35010.58393716812	speed (wps):6417.77498581773
step: 215	loss: 43207.72395372391	speed (wps):6419.156050852462
step: 256	loss: 51429.08974170685	speed (wps):6420.538923053086
step: 297	loss: 59598.886692523956	speed (wps):6421.317370235521
step: 338	loss: 67732.64218091965	speed (wps):6421.752371241471
step: 379	loss: 75917.21755743027	speed (wps):6421.941499424749
epoch: 3	train ppl: 300.2020381657697	val ppl: 239.40884033254304	best val: 239.40884033254304	time (s) spent in epoch: 147.64261984825134

EPOCH 4 ------------------
step: 10	loss: 2211.604857444763	speed (wps):6300.923367507043
step: 51	loss: 10325.995817184448	speed (wps):6399.946661754411
step: 92	loss: 18435.910255908966	speed (wps):6413.350199010716
step: 133	loss: 26493.647966384888	speed (wps):6418.444287274801
step: 174	loss: 34402.530183792114	speed (wps):6420.892553519718
step: 215	loss: 42458.080327510834	speed (wps):6422.121971744553
step: 256	loss: 50552.17951774597	speed (wps):6423.290768219483
step: 297	loss: 58592.33875513077	speed (wps):6423.435476134639
step: 338	loss: 66606.63314819336	speed (wps):6423.794905236911
step: 379	loss: 74681.52280330658	speed (wps):6424.777368845144
epoch: 4	train ppl: 273.7898039862351	val ppl: 224.17736091899914	best val: 224.17736091899914	time (s) spent in epoch: 147.56004118919373

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3329.6524262428284	speed (wps):7702.783884411045
step: 51	loss: 13556.39522075653	speed (wps):8049.245639742674
step: 92	loss: 23302.423808574677	speed (wps):8093.597685329426
step: 133	loss: 32835.12595176697	speed (wps):8111.171141560089
step: 174	loss: 42124.2586183548	speed (wps):8120.081623611776
step: 215	loss: 51484.146943092346	speed (wps):8124.630356995706
step: 256	loss: 60752.89256095886	speed (wps):8128.646245630726
step: 297	loss: 69946.47210597992	speed (wps):8131.309879472108
step: 338	loss: 79051.8221783638	speed (wps):8133.246010693825
step: 379	loss: 88136.20126008987	speed (wps):8134.969531992584
epoch: 0	train ppl: 733.0785415140605	val ppl: 408.8467529512387	best val: 408.8467529512387	time (s) spent in epoch: 116.5684130191803

EPOCH 1 ------------------
step: 10	loss: 2427.1229910850525	speed (wps):7926.318957570643
step: 51	loss: 11370.6498503685	speed (wps):8104.877045645525
step: 92	loss: 20269.53439950943	speed (wps):8127.896360307246
step: 133	loss: 29088.271420001984	speed (wps):8133.39340880289
step: 174	loss: 37756.470432281494	speed (wps):8137.284360566063
step: 215	loss: 46553.62558364868	speed (wps):8141.202829421066
step: 256	loss: 55318.02130699158	speed (wps):8142.240503243155
step: 297	loss: 64021.771466732025	speed (wps):8145.080726426064
step: 338	loss: 72676.02907180786	speed (wps):8145.473218960795
step: 379	loss: 81347.72279024124	speed (wps):8145.708838225628
epoch: 1	train ppl: 448.94357947924806	val ppl: 328.7146359643001	best val: 328.7146359643001	time (s) spent in epoch: 116.43087697029114

EPOCH 2 ------------------
step: 10	loss: 2341.9402503967285	speed (wps):7940.840626433235
step: 51	loss: 10941.033709049225	speed (wps):8107.151611048045
step: 92	loss: 19513.920426368713	speed (wps):8125.838011195622
step: 133	loss: 28041.952996253967	speed (wps):8136.487064523978
step: 174	loss: 36431.04914188385	speed (wps):8139.624124038507
step: 215	loss: 44954.37142372131	speed (wps):8141.3712275909875
step: 256	loss: 53478.239612579346	speed (wps):8143.001891126221
step: 297	loss: 61952.35010385513	speed (wps):8144.4518953628185
step: 338	loss: 70388.65942955017	speed (wps):8145.270510589759
step: 379	loss: 78849.9755191803	speed (wps):8147.125682498816
epoch: 2	train ppl: 373.49091604619235	val ppl: 285.92486608378204	best val: 285.92486608378204	time (s) spent in epoch: 116.40077090263367

EPOCH 3 ------------------
step: 10	loss: 2291.408886909485	speed (wps):7934.9205449552455
step: 51	loss: 10691.582372188568	speed (wps):8107.427679937373
step: 92	loss: 19085.738079547882	speed (wps):8127.49180418584
step: 133	loss: 27427.743678092957	speed (wps):8135.874128376372
step: 174	loss: 35636.37576341629	speed (wps):8139.496145721613
step: 215	loss: 43980.63605308533	speed (wps):8140.227230429116
step: 256	loss: 52353.16437959671	speed (wps):8142.847037328518
step: 297	loss: 60665.23242712021	speed (wps):8144.431923156323
step: 338	loss: 68951.6405081749	speed (wps):8145.788581667163
step: 379	loss: 77266.78365945816	speed (wps):8146.205742721259
epoch: 3	train ppl: 332.29731793950214	val ppl: 264.0330428152124	best val: 264.0330428152124	time (s) spent in epoch: 116.42938995361328

EPOCH 4 ------------------
step: 10	loss: 2251.088321208954	speed (wps):7944.89710451694
step: 51	loss: 10520.10042667389	speed (wps):8107.220222905169
step: 92	loss: 18778.112030029297	speed (wps):8130.041687150554
step: 133	loss: 26995.725445747375	speed (wps):8138.399501343581
step: 174	loss: 35075.16921043396	speed (wps):8142.190301742614
step: 215	loss: 43300.43735742569	speed (wps):8145.438937335911
step: 256	loss: 51552.76370048523	speed (wps):8146.6597619022305
step: 297	loss: 59750.78728199005	speed (wps):8147.129294391164
step: 338	loss: 67922.8539276123	speed (wps):8147.517927434679
step: 379	loss: 76135.5148267746	speed (wps):8148.577854118831
epoch: 4	train ppl: 305.3520296414353	val ppl: 249.35148048763637	best val: 249.35148048763637	time (s) spent in epoch: 116.38978672027588

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3640.2903270721436	speed (wps):4962.831011066189
step: 51	loss: 13893.139226436615	speed (wps):5108.133197718004
step: 92	loss: 23602.906620502472	speed (wps):5125.907435761015
step: 133	loss: 33082.338552474976	speed (wps):5132.684557683509
step: 174	loss: 42339.94581460953	speed (wps):5135.413048485567
step: 215	loss: 51669.13115024567	speed (wps):5137.78027690126
step: 256	loss: 60909.03673887253	speed (wps):5138.924261212029
step: 297	loss: 70065.7182097435	speed (wps):5139.726036466415
step: 338	loss: 79140.01571893692	speed (wps):5140.343936024287
step: 379	loss: 88190.67441940308	speed (wps):5140.726140816609
epoch: 0	train ppl: 735.0920542050545	val ppl: 399.0887876772549	best val: 399.0887876772549	time (s) spent in epoch: 184.34722018241882

EPOCH 1 ------------------
step: 10	loss: 2428.113284111023	speed (wps):5072.801662836905
step: 51	loss: 11338.205482959747	speed (wps):5129.629490738733
step: 92	loss: 20213.429577350616	speed (wps):5138.064604297464
step: 133	loss: 29021.778042316437	speed (wps):5141.068902955159
step: 174	loss: 37664.78744268417	speed (wps):5142.975213705178
step: 215	loss: 46440.77019691467	speed (wps):5143.743914652195
step: 256	loss: 55192.84644603729	speed (wps):5144.481547113147
step: 297	loss: 63890.31523704529	speed (wps):5144.956983006728
step: 338	loss: 72536.49174928665	speed (wps):5145.462444376773
step: 379	loss: 81198.0889248848	speed (wps):5146.041194357443
epoch: 1	train ppl: 444.1982784136347	val ppl: 318.88299268220527	best val: 318.88299268220527	time (s) spent in epoch: 184.18794798851013

EPOCH 2 ------------------
step: 10	loss: 2342.0732641220093	speed (wps):5076.717930157117
step: 51	loss: 10930.321564674377	speed (wps):5135.134912712776
step: 92	loss: 19493.960723876953	speed (wps):5140.934935708322
step: 133	loss: 28001.16237640381	speed (wps):5142.442252963184
step: 174	loss: 36367.15263605118	speed (wps):5143.779333904817
step: 215	loss: 44872.45224237442	speed (wps):5144.845839253712
step: 256	loss: 53386.19890451431	speed (wps):5145.260517443121
step: 297	loss: 61839.06448364258	speed (wps):5145.568327929042
step: 338	loss: 70262.20429897308	speed (wps):5145.543502622842
step: 379	loss: 78720.1549744606	speed (wps):5145.698426863801
epoch: 2	train ppl: 370.34666594298875	val ppl: 282.2679820979338	best val: 282.2679820979338	time (s) spent in epoch: 184.18088603019714

EPOCH 3 ------------------
step: 10	loss: 2285.443046092987	speed (wps):5076.774540548828
step: 51	loss: 10687.385597229004	speed (wps):5131.587107591467
step: 92	loss: 19067.26823091507	speed (wps):5138.293666856093
step: 133	loss: 27399.27618741989	speed (wps):5141.652307005159
step: 174	loss: 35582.95035839081	speed (wps):5143.62599474933
step: 215	loss: 43915.152394771576	speed (wps):5144.5265232134025
step: 256	loss: 52265.41360616684	speed (wps):5145.260495514885
step: 297	loss: 60564.862089157104	speed (wps):5145.970177175813
step: 338	loss: 68829.79653120041	speed (wps):5146.386456898281
step: 379	loss: 77137.23868846893	speed (wps):5146.612898970441
epoch: 3	train ppl: 328.8499831687372	val ppl: 264.4774526659119	best val: 264.4774526659119	time (s) spent in epoch: 184.16450762748718

EPOCH 4 ------------------
step: 10	loss: 2256.5736603736877	speed (wps):5074.336450246412
step: 51	loss: 10513.285284042358	speed (wps):5134.175255107971
step: 92	loss: 18751.44641637802	speed (wps):5141.830516393235
step: 133	loss: 26946.40524148941	speed (wps):5144.352499957453
step: 174	loss: 34996.93253993988	speed (wps):5145.02724204257
step: 215	loss: 43194.08637523651	speed (wps):5145.977100091556
step: 256	loss: 51420.10769367218	speed (wps):5145.802519909039
step: 297	loss: 59606.102311611176	speed (wps):5146.027031286292
step: 338	loss: 67758.74876976013	speed (wps):5145.84233708443
step: 379	loss: 75949.70361471176	speed (wps):5146.251369148334
epoch: 4	train ppl: 301.3610327827072	val ppl: 243.28885672365453	best val: 243.28885672365453	time (s) spent in epoch: 184.16334056854248

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=64_seq_len=35_hidden_size=1500_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=128_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3658.686993122101	speed (wps):7553.429251518906
step: 30	loss: 8725.038180351257	speed (wps):7675.534487010628
step: 50	loss: 13600.123708248138	speed (wps):7702.974369685156
step: 70	loss: 18325.329320430756	speed (wps):7713.862917193314
step: 90	loss: 22956.77278995514	speed (wps):7719.821119498176
step: 110	loss: 27510.087552070618	speed (wps):7724.275823578967
step: 130	loss: 32022.73638010025	speed (wps):7727.154393684911
step: 150	loss: 36494.03331041336	speed (wps):7729.015414900048
step: 170	loss: 40910.26572942734	speed (wps):7730.037315913614
step: 190	loss: 45332.7556848526	speed (wps):7730.666818905633
epoch: 0	train ppl: 848.413763936098	val ppl: 414.002084416278	best val: 414.002084416278	time (s) spent in epoch: 123.05939602851868

EPOCH 1 ------------------
step: 10	loss: 2425.579481124878	speed (wps):7644.303228850308
step: 30	loss: 6794.728062152863	speed (wps):7709.870178402236
step: 50	loss: 11145.594565868378	speed (wps):7723.818271725919
step: 70	loss: 15473.673431873322	speed (wps):7729.263659261542
step: 90	loss: 19768.07891368866	speed (wps):7733.39930971088
step: 110	loss: 24030.866315364838	speed (wps):7735.223696691682
step: 130	loss: 28276.28808259964	speed (wps):7735.2947922264375
step: 150	loss: 32503.200714588165	speed (wps):7736.908738604601
step: 170	loss: 36688.033928871155	speed (wps):7737.369837645219
step: 190	loss: 40898.890635967255	speed (wps):7738.463079122824
epoch: 1	train ppl: 449.7356042803464	val ppl: 317.03382358682694	best val: 317.03382358682694	time (s) spent in epoch: 122.93977546691895

EPOCH 2 ------------------
step: 10	loss: 2326.7544388771057	speed (wps):7657.6896073342405
step: 30	loss: 6516.293835639954	speed (wps):7714.465173908462
step: 50	loss: 10693.501224517822	speed (wps):7723.891667986068
step: 70	loss: 14851.96724653244	speed (wps):7729.322276171036
step: 90	loss: 18997.059597969055	speed (wps):7732.512752299888
step: 110	loss: 23124.846563339233	speed (wps):7734.813982233202
step: 130	loss: 27234.3469786644	speed (wps):7736.5142605206565
step: 150	loss: 31337.399699687958	speed (wps):7737.437127201282
step: 170	loss: 35398.35247516632	speed (wps):7738.649432762506
step: 190	loss: 39488.13383102417	speed (wps):7738.925894676491
epoch: 2	train ppl: 365.63602351849073	val ppl: 275.51948824404974	best val: 275.51948824404974	time (s) spent in epoch: 122.93899059295654

EPOCH 3 ------------------
step: 10	loss: 2267.1689462661743	speed (wps):7659.082278051832
step: 30	loss: 6362.334387302399	speed (wps):7713.568964071573
step: 50	loss: 10445.711591243744	speed (wps):7723.929642849734
step: 70	loss: 14518.538103103638	speed (wps):7729.298900948524
step: 90	loss: 18575.812056064606	speed (wps):7732.718155619122
step: 110	loss: 22612.793405056	speed (wps):7735.53192190555
step: 130	loss: 26644.54766511917	speed (wps):7736.979296211715
step: 150	loss: 30658.890562057495	speed (wps):7738.614361507554
step: 170	loss: 34647.8343462944	speed (wps):7739.715498121378
step: 190	loss: 38653.96111726761	speed (wps):7740.174017149232
epoch: 3	train ppl: 323.13323265563815	val ppl: 251.12466972479538	best val: 251.12466972479538	time (s) spent in epoch: 122.91662287712097

EPOCH 4 ------------------
step: 10	loss: 2221.961452960968	speed (wps):7648.924697344375
step: 30	loss: 6244.347224235535	speed (wps):7706.76811659933
step: 50	loss: 10250.502717494965	speed (wps):7719.859866898204
step: 70	loss: 14251.40990972519	speed (wps):7726.916131503505
step: 90	loss: 18238.86888027191	speed (wps):7731.6030104517995
step: 110	loss: 22209.208734035492	speed (wps):7735.429014870135
step: 130	loss: 26174.29142475128	speed (wps):7736.425753471453
step: 150	loss: 30125.91065645218	speed (wps):7737.131128156037
step: 170	loss: 34057.80178308487	speed (wps):7737.996715002902
step: 190	loss: 38009.32361602783	speed (wps):7738.866154051154
epoch: 4	train ppl: 293.83921679955955	val ppl: 235.55090012405446	best val: 235.55090012405446	time (s) spent in epoch: 122.94613075256348

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=128_seq_len=35_hidden_size=1500_num_layers=3_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy

########## Setting Up Experiment ######################

Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=128_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0
Using the GPU
Loading data from data
  vocabulary size: 10000

########## Running Main Loop ##########################

EPOCH 0 ------------------
step: 10	loss: 3251.2238121032715	speed (wps):10801.38883412008
step: 30	loss: 8359.421617984772	speed (wps):11116.517086752945
step: 50	loss: 13195.872566699982	speed (wps):11186.02575001932
step: 70	loss: 17945.315482616425	speed (wps):11217.598537280626
step: 90	loss: 22633.717415332794	speed (wps):11235.047747613326
step: 110	loss: 27253.263149261475	speed (wps):11245.917161930243
step: 130	loss: 31843.396544456482	speed (wps):11253.340431691862
step: 150	loss: 36386.13975286484	speed (wps):11259.27397809111
step: 170	loss: 40894.29418563843	speed (wps):11263.743363590595
step: 190	loss: 45402.00207948685	speed (wps):11266.99342571883
epoch: 0	train ppl: 864.0660637240134	val ppl: 467.0257884270076	best val: 467.0257884270076	time (s) spent in epoch: 84.282541513443

EPOCH 1 ------------------
step: 10	loss: 2468.4346175193787	speed (wps):11086.312856317476
step: 30	loss: 6907.814211845398	speed (wps):11220.462782281335
step: 50	loss: 11338.562400341034	speed (wps):11250.77582469879
step: 70	loss: 15742.01218366623	speed (wps):11263.464169731185
step: 90	loss: 20128.135635852814	speed (wps):11271.560616832923
step: 110	loss: 24483.980424404144	speed (wps):11276.525195514985
step: 130	loss: 28824.423525333405	speed (wps):11279.877254855615
step: 150	loss: 33136.99397563934	speed (wps):11281.974941392524
step: 170	loss: 37415.28171539307	speed (wps):11283.819822553281
step: 190	loss: 41705.810322761536	speed (wps):11284.67227940377
epoch: 1	train ppl: 507.66958762295604	val ppl: 360.7163415570602	best val: 360.7163415570602	time (s) spent in epoch: 84.16682076454163

EPOCH 2 ------------------
step: 10	loss: 2365.837275981903	speed (wps):11090.525595551155
step: 30	loss: 6632.908821105957	speed (wps):11223.345445709083
step: 50	loss: 10898.127937316895	speed (wps):11252.349321293343
step: 70	loss: 15143.038477897644	speed (wps):11264.18901379792
step: 90	loss: 19363.784112930298	speed (wps):11271.344853774515
step: 110	loss: 23572.93438911438	speed (wps):11275.836257685021
step: 130	loss: 27773.423686027527	speed (wps):11279.908837025076
step: 150	loss: 31953.04152727127	speed (wps):11282.459984238863
step: 170	loss: 36098.258731365204	speed (wps):11283.846688922851
step: 190	loss: 40263.035328388214	speed (wps):11285.311947502567
epoch: 2	train ppl: 410.5584575804022	val ppl: 309.1623405160014	best val: 309.1623405160014	time (s) spent in epoch: 84.15517497062683

EPOCH 3 ------------------
step: 10	loss: 2311.374864578247	speed (wps):11090.009092189955
step: 30	loss: 6481.736569404602	speed (wps):11224.628138352919
step: 50	loss: 10644.771955013275	speed (wps):11251.950852536298
step: 70	loss: 14799.666967391968	speed (wps):11263.625639643027
step: 90	loss: 18926.795105934143	speed (wps):11270.583285133796
step: 110	loss: 23052.89288043976	speed (wps):11274.795786472287
step: 130	loss: 27163.092076778412	speed (wps):11277.895220255816
step: 150	loss: 31258.340780735016	speed (wps):11280.538047139265
step: 170	loss: 35325.96119880676	speed (wps):11282.009054729182
step: 190	loss: 39417.18242406845	speed (wps):11283.495645569097
epoch: 3	train ppl: 362.449833462233	val ppl: 280.1743859266773	best val: 280.1743859266773	time (s) spent in epoch: 84.1748399734497

EPOCH 4 ------------------
step: 10	loss: 2270.066809654236	speed (wps):11089.038695830732
step: 30	loss: 6368.049004077911	speed (wps):11223.899923971007
step: 50	loss: 10464.242372512817	speed (wps):11252.703157724907
step: 70	loss: 14543.116919994354	speed (wps):11264.428778739655
step: 90	loss: 18606.620070934296	speed (wps):11271.849279710197
step: 110	loss: 22668.590576648712	speed (wps):11276.872043346448
step: 130	loss: 26715.936381816864	speed (wps):11279.352065683626
step: 150	loss: 30749.854142665863	speed (wps):11281.239919878688
step: 170	loss: 34759.253952503204	speed (wps):11282.519532265955
step: 190	loss: 38790.07866859436	speed (wps):11283.733258643804
epoch: 4	train ppl: 330.2178894514976	val ppl: 267.2898782320712	best val: 267.2898782320712	time (s) spent in epoch: 84.17123866081238

DONE

Saving learning curves to RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0003_batch_size=128_seq_len=35_hidden_size=1024_num_layers=4_dp_keep_prob=0.35_num_epochs=5_0/learning_curves.npy
Set compute mode to DEFAULT for GPU 00000000:83:00.0.
All done.
